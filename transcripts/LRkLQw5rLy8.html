<p class="speaker"><span class="line" data-starttime="0"><span class="speakerName">Garrick Evans</span>: My name's </span><span class="line" data-startTime="0">Garrick Evans. </span> <span class="line" data-startTime="1">I'm a software architect on </span><span class="line" data-startTime="1">the Google Cloud Platform </span><span class="line" data-startTime="4">solutions team. </span> <span class="line" data-startTime="5">And I want to tell you one of </span><span class="line" data-startTime="5">the coolest parts about my job </span><span class="line" data-startTime="8">is the opportunity that comes </span><span class="line" data-startTime="8">up once in a while to get </span><span class="line" data-startTime="10">involved with some projects </span><span class="line" data-startTime="10">that partners are doing </span><span class="line" data-startTime="13">working on some very, very hard </span><span class="line" data-startTime="13">problems and asking some </span><span class="line" data-startTime="16">of the most intriguing and </span><span class="line" data-startTime="16">important questions of the </span><span class="line" data-startTime="18">world, in this particular </span><span class="line" data-startTime="18">case, in the universe. </span> <span class="line" data-startTime="22">So today, I'm actually pleased </span><span class="line" data-startTime="22">to share with you </span><span class="line" data-startTime="24">one of these projects. </span></p>

<p><span class="line" data-startTime="24">It's the ATLAS experiment on </span><span class="line" data-startTime="24">Google Compute Engine. </span> <span class="line" data-startTime="27">And with this project, what we </span><span class="line" data-startTime="27">wanted to demonstrate was </span><span class="line" data-startTime="30">actually tangible acceleration </span><span class="line" data-startTime="30">of scientific research by </span><span class="line" data-startTime="33">leveraging and taking advantage </span><span class="line" data-startTime="33">of Google's Cloud in </span><span class="line" data-startTime="35">a particular Compute Engine. </span> <span class="line" data-startTime="38">The computational and data </span><span class="line" data-startTime="38">demands of the ATLAS </span><span class="line" data-startTime="40">experiment are pretty </span><span class="line" data-startTime="40">substantial. </span> <span class="line" data-startTime="41">There's thousands of </span><span class="line" data-startTime="41">collaborators running as a </span><span class="line" data-startTime="44">baseline hundreds of thousands </span><span class="line" data-startTime="44">of jobs per day, peaking at </span><span class="line" data-startTime="49">over 10 times that amount. </span> <span class="line" data-startTime="51">The experiment generates tens </span><span class="line" data-startTime="51">of petabytes of data a year, </span><span class="line" data-startTime="53">and there's currently well over </span><span class="line" data-startTime="53">100 petabytes of data </span><span class="line" data-startTime="55">under management coordination. </span></p>

<p><span class="line" data-startTime="58">So with me today are Dr. </span> <span class="line" data-startTime="58">Sergey Panitkin of the </span><span class="line" data-startTime="60">Brookhaven National Lab in New </span><span class="line" data-startTime="60">York and Andrew Hanushevsy of </span><span class="line" data-startTime="63">the SLAC National Accelerator </span><span class="line" data-startTime="63">Lab at Stanford. </span> <span class="line" data-startTime="66">Sergey leads research and </span><span class="line" data-startTime="66">development of cloud computing </span><span class="line" data-startTime="69">at the ATLAS experiment at </span><span class="line" data-startTime="69">a Large Hadron Collider. </span> <span class="line" data-startTime="72">And we'll talk about the project </span><span class="line" data-startTime="72">itself, the compute </span><span class="line" data-startTime="74">clusters this team has </span><span class="line" data-startTime="74">assembled, and share his </span><span class="line" data-startTime="76">results with you. </span> <span class="line" data-startTime="78">Andy's an information systems </span><span class="line" data-startTime="78">specialist who's designed and </span><span class="line" data-startTime="81">co-developed a data clustering </span><span class="line" data-startTime="81">technology, XRootD, which was </span><span class="line" data-startTime="85">used to federate data between </span><span class="line" data-startTime="85">ATLAS and the Cloud and will </span><span class="line" data-startTime="88">provide us an overview </span><span class="line" data-startTime="88">of the technology. </span></p>

<p><span class="line" data-startTime="90">So with that, I'd like to </span><span class="line" data-startTime="90">introduce you to Dr. Sergey </span><span class="line" data-startTime="93">Panitkin of Brookhaven </span><span class="line" data-startTime="93">National Lab. </span> <span class="line" data-startTime="95">[APPLAUSE] </span><span class="line" data-startTime="103">DR. SERGEY PANITKIN: Today </span><span class="line" data-startTime="103">I will talk &mdash; oh. </span> <span class="line" data-startTime="105">[LAUGHTER] </span><span class="line" data-startTime="106">DR. SERGEY PANITKIN: Today I </span><span class="line" data-startTime="106">will talk a little bit about </span><span class="line" data-startTime="108">ATLAS experiment, Large Hadron </span><span class="line" data-startTime="108">Collider, ATLAS experiment </span><span class="line" data-startTime="111">computing the challenges of </span><span class="line" data-startTime="111">a big data experiment, our </span><span class="line" data-startTime="115">interesting Clouds, and our </span><span class="line" data-startTime="115">recent project Google Compute </span><span class="line" data-startTime="119">Engine, some conditional </span><span class="line" data-startTime="119">clusters that we run on the </span><span class="line" data-startTime="123">grid and now we're running in </span><span class="line" data-startTime="123">the Cloud, and we'll also </span><span class="line" data-startTime="128">describe the XRoot technology. </span></p>

<p><span class="line" data-startTime="130">Andy will go into more </span><span class="line" data-startTime="130">details about that. </span> <span class="line" data-startTime="132">We're saying that there are </span><span class="line" data-startTime="132">products developed in the high </span><span class="line" data-startTime="135">energy community. </span> <span class="line" data-startTime="136">Remember, worldwide web. </span> <span class="line" data-startTime="139">There is a new generation of </span><span class="line" data-startTime="139">technology developed that </span><span class="line" data-startTime="141">community, they're open source, </span><span class="line" data-startTime="141">and we're saying </span><span class="line" data-startTime="144">they're ready to be shared with </span><span class="line" data-startTime="144">people who are interested </span><span class="line" data-startTime="146">in Clouds and large </span><span class="line" data-startTime="146">computational clustering, </span><span class="line" data-startTime="149">bridging virtual and real </span><span class="line" data-startTime="151">infrastructure, things like that. </span> <span class="line" data-startTime="157">ATLAS is a multipurpose detector </span><span class="line" data-startTime="157">and Large Hadron </span><span class="line" data-startTime="159">Collider at CERN. </span> <span class="line" data-startTime="161">The ATLAS experiment itself </span><span class="line" data-startTime="161">is a large international </span><span class="line" data-startTime="164">collaboration of about 3,000 </span><span class="line" data-startTime="164">scientists and engineers from </span><span class="line" data-startTime="168">many universities and labs </span><span class="line" data-startTime="168">around the globe. </span> <span class="line" data-startTime="171">It took almost 20 years to </span><span class="line" data-startTime="171">design and build the </span><span class="line" data-startTime="174">apparatus, but nevertheless, </span><span class="line" data-startTime="174">it's very young collaboration </span><span class="line" data-startTime="178">with more than 1,200 graduate </span><span class="line" data-startTime="178">students working ATLAS and </span><span class="line" data-startTime="181">driving the analyses. </span></p>

<p><span class="line" data-startTime="185">This photo shows the outline of </span><span class="line" data-startTime="185">the LHC tunnel on an aerial </span><span class="line" data-startTime="191">view of countryside near </span><span class="line" data-startTime="191">Geneva, Switzerland. </span> <span class="line" data-startTime="194">The LHC is one of the largest </span><span class="line" data-startTime="194">scientific instruments ever </span><span class="line" data-startTime="197">built and certainly only one </span><span class="line" data-startTime="197">of the most complex. </span> <span class="line" data-startTime="201">And everything about the LHC </span><span class="line" data-startTime="201">is extreme, its size, its </span><span class="line" data-startTime="205">energy, its detectors. </span> <span class="line" data-startTime="207">It's the coldest and emptiest </span><span class="line" data-startTime="207">place in the solar system. </span> <span class="line" data-startTime="212">It's the hottest place in the </span><span class="line" data-startTime="212">universe, with temperatures of </span><span class="line" data-startTime="214">particle created under collision </span><span class="line" data-startTime="214">exceeding trillions </span><span class="line" data-startTime="217">degrees of Celsius. </span></p>

<p><span class="line" data-startTime="222">LHC tunnel is about 27 </span><span class="line" data-startTime="222">kilometers long. </span> <span class="line" data-startTime="226">Thousands of superconducting </span><span class="line" data-startTime="226">magnets working at near </span><span class="line" data-startTime="230">absolute zero temperature are </span><span class="line" data-startTime="230">needed in order to accelerate </span><span class="line" data-startTime="233">and collide protons and heavy </span><span class="line" data-startTime="233">ions at the highest </span><span class="line" data-startTime="236">temperatures ever achieved </span><span class="line" data-startTime="236">in the lab. </span> <span class="line" data-startTime="239">Such energies are needed in </span><span class="line" data-startTime="239">order to explore the high </span><span class="line" data-startTime="242">energy frontier of modern </span><span class="line" data-startTime="242">particle physics to discover </span><span class="line" data-startTime="245">things like Higgs boson, the </span><span class="line" data-startTime="245">missing piece of the standard </span><span class="line" data-startTime="248">model, a particle that is </span><span class="line" data-startTime="248">responsible for electric </span><span class="line" data-startTime="251">symmetry breaking </span><span class="line" data-startTime="251">that generates </span><span class="line" data-startTime="253">massive elementary particles. </span></p>

<p><span class="line" data-startTime="255">LHC's aim to explore physics </span><span class="line" data-startTime="255">beyond the standard model, </span><span class="line" data-startTime="258">things like supersymmetry, </span><span class="line" data-startTime="258">possible existence of </span><span class="line" data-startTime="261">extradimensional, possible </span><span class="line" data-startTime="261">candidates of dark matter, </span><span class="line" data-startTime="264">dark energy, and whatever </span><span class="line" data-startTime="264">can be at that frontier. </span> <span class="line" data-startTime="272">The ATLAS detector is a </span><span class="line" data-startTime="272">multipurpose apparatus </span><span class="line" data-startTime="275">designed to detect particles </span><span class="line" data-startTime="275">created in the collision of </span><span class="line" data-startTime="280">the LHC beams. </span> <span class="line" data-startTime="281">It's the largest detector of its </span><span class="line" data-startTime="281">kind, and also one of the </span><span class="line" data-startTime="284">most complex. </span> <span class="line" data-startTime="287">It's built like a Russian doll </span><span class="line" data-startTime="287">with detectors inside other </span><span class="line" data-startTime="290">detectors with several giant </span><span class="line" data-startTime="290">superconducting magnets. </span> <span class="line" data-startTime="294">All of this is needed to </span><span class="line" data-startTime="294">accurately register particle </span><span class="line" data-startTime="299">and identify them. </span> <span class="line" data-startTime="301">It has very high granularity of </span><span class="line" data-startTime="301">sensing elements with about </span><span class="line" data-startTime="304">150 millions of sensors </span><span class="line" data-startTime="304">of various kinds. </span> <span class="line" data-startTime="308">And it's capable of taking </span><span class="line" data-startTime="308">snapshots of the collision </span><span class="line" data-startTime="310">events at the rate of 40 </span><span class="line" data-startTime="310">megahertz, or every 25 </span><span class="line" data-startTime="313">nanoseconds. </span></p>

<p><span class="line" data-startTime="316">And it weighs about 7,000 </span><span class="line" data-startTime="316">tons, and it's </span><span class="line" data-startTime="319">quite large in size. </span> <span class="line" data-startTime="322">This slide gives you a </span><span class="line" data-startTime="322">feeling of the size </span><span class="line" data-startTime="324">of the ATLAS detector. </span> <span class="line" data-startTime="326">Here, outlines of ATLAS and our </span><span class="line" data-startTime="326">sister detector, CMS, are </span><span class="line" data-startTime="329">superimposed with the image of </span><span class="line" data-startTime="329">the ATLAS and CMS six-story </span><span class="line" data-startTime="333">office building at CERN. </span> <span class="line" data-startTime="335">And ATLAS is taller than </span><span class="line" data-startTime="335">that building. </span> <span class="line" data-startTime="337">It's a huge apparatus. </span> <span class="line" data-startTime="341">The detector itself sits in </span><span class="line" data-startTime="341">the LHC tunnel about 100 </span><span class="line" data-startTime="345">meters below the surface, and </span><span class="line" data-startTime="345">it was assembled there piece </span><span class="line" data-startTime="348">by piece carefully like </span><span class="line" data-startTime="348">a ship in the bottle. </span></p>

<p><span class="line" data-startTime="351">Quite extraordinary engineering </span><span class="line" data-startTime="351">achievement. </span> <span class="line" data-startTime="354">Especially taking into account </span><span class="line" data-startTime="354">that the detector should be </span><span class="line" data-startTime="357">able to measure particle </span><span class="line" data-startTime="357">positions with accuracy </span><span class="line" data-startTime="359">exceeding 100 microns. </span> <span class="line" data-startTime="362">So it should be assembled with </span><span class="line" data-startTime="362">a very high precision. </span> <span class="line" data-startTime="366">And that's how it looks fully </span><span class="line" data-startTime="366">assembled in the cover. </span> <span class="line" data-startTime="375">As I mentioned before, LHC and </span><span class="line" data-startTime="375">ATLAS were built to explore </span><span class="line" data-startTime="378">high energy frontier of modern </span><span class="line" data-startTime="378">particle physics to search for </span><span class="line" data-startTime="382">new phenomena that may occur at </span><span class="line" data-startTime="382">these energies to probe the </span><span class="line" data-startTime="385">very fundamental </span><span class="line" data-startTime="385">laws of nature. </span> <span class="line" data-startTime="387">In particular, to search for </span><span class="line" data-startTime="387">Higgs boson, this missing </span><span class="line" data-startTime="392">piece of standard model </span><span class="line" data-startTime="392">elementary particles </span><span class="line" data-startTime="394">responsible for electroweak </span><span class="line" data-startTime="394">symmetry break and again </span><span class="line" data-startTime="397">generation of masses of other </span><span class="line" data-startTime="397">elementary particles. </span></p>

<p><span class="line" data-startTime="400">And Higgs' mechanism was </span><span class="line" data-startTime="400">suggested about 40 years ago, </span><span class="line" data-startTime="403">and since then physicists </span><span class="line" data-startTime="403">around the world were </span><span class="line" data-startTime="405">searching for proof </span><span class="line" data-startTime="405">of its existence. </span> <span class="line" data-startTime="408">And recent discovery of a new </span><span class="line" data-startTime="408">product at the LHC which looks </span><span class="line" data-startTime="412">like Higgs boson culminated </span><span class="line" data-startTime="412">this search. </span> <span class="line" data-startTime="422">Less than a year ago, two </span><span class="line" data-startTime="422">experiments at LHC ATLAS and </span><span class="line" data-startTime="425">CMS announced discovery </span><span class="line" data-startTime="425">of the new particle. </span> <span class="line" data-startTime="428">It was called a giant leap for </span><span class="line" data-startTime="428">science, the most important </span><span class="line" data-startTime="431">discovery of the last decade </span><span class="line" data-startTime="431">in the particle physics. </span> <span class="line" data-startTime="435">It generated a lot of public </span><span class="line" data-startTime="435">interest and a lot of media </span><span class="line" data-startTime="437">attention, with thousands </span><span class="line" data-startTime="437">and thousands of print </span><span class="line" data-startTime="440">stories and TV spots. </span> <span class="line" data-startTime="442">And most likely you have heard </span><span class="line" data-startTime="442">about this discovery already. </span></p>

<p><span class="line" data-startTime="446">But you probably haven't </span><span class="line" data-startTime="446">seen my next slide. </span> <span class="line" data-startTime="450">I must note here that typically </span><span class="line" data-startTime="450">we do not detect </span><span class="line" data-startTime="456">elementary particles like </span><span class="line" data-startTime="456">Higgs directly. </span> <span class="line" data-startTime="460">Like many other subatomic </span><span class="line" data-startTime="460">particles, Higgs is too heavy, </span><span class="line" data-startTime="463">too unstable, and </span><span class="line" data-startTime="463">too short lived. </span> <span class="line" data-startTime="467">When produced, it immediately </span><span class="line" data-startTime="467">decays into wider, more </span><span class="line" data-startTime="469">stable, more long-lived </span><span class="line" data-startTime="469">particles that are actually </span><span class="line" data-startTime="472">registered by our detectors, </span><span class="line" data-startTime="472">like ATLAS. </span> <span class="line" data-startTime="475">And by the way, such decays </span><span class="line" data-startTime="475">may be multi-staged, and </span><span class="line" data-startTime="478">[INAUDIBLE] stable object </span><span class="line" data-startTime="478">decay [INAUDIBLE]. </span></p>

<p><span class="line" data-startTime="481">After several of such </span><span class="line" data-startTime="481">decays, you're </span><span class="line" data-startTime="482">getting final state particles. </span> <span class="line" data-startTime="485">By using collected information </span><span class="line" data-startTime="485">about decayed products of </span><span class="line" data-startTime="489">Higgs, what we call the final </span><span class="line" data-startTime="489">state particles, we can </span><span class="line" data-startTime="493">nevertheless prove the existence </span><span class="line" data-startTime="493">of parent particles, </span><span class="line" data-startTime="495">like Higgs. </span> <span class="line" data-startTime="497">Indirectly, yes, but without </span><span class="line" data-startTime="497">any doubt, just by using </span><span class="line" data-startTime="501">simple laws of conservation of </span><span class="line" data-startTime="501">energy and momentum familiar </span><span class="line" data-startTime="504">from freshman physics 101. </span> <span class="line" data-startTime="507">Whatever energy existed before </span><span class="line" data-startTime="507">the particle decayed should be </span><span class="line" data-startTime="510">conserved after the decay and </span><span class="line" data-startTime="510">mass equivalent to energy, as </span><span class="line" data-startTime="512">was pointed out by Einstein, </span><span class="line" data-startTime="512">and mass of the decaying </span><span class="line" data-startTime="517">parent particle doesn't </span><span class="line" data-startTime="517">disappear. </span></p>

<p><span class="line" data-startTime="518">It transformed to masses </span><span class="line" data-startTime="518">[INAUDIBLE] </span><span class="line" data-startTime="520">the decay product to </span><span class="line" data-startTime="520">other particles. </span> <span class="line" data-startTime="522">So again, by using the </span><span class="line" data-startTime="522">information collected about </span><span class="line" data-startTime="525">final state particles, </span><span class="line" data-startTime="525">we can reconstruct </span><span class="line" data-startTime="527">the mass of the particle. </span> <span class="line" data-startTime="529">And we measure many events. </span> <span class="line" data-startTime="531">For each event, take all final </span><span class="line" data-startTime="531">state particles that belong to </span><span class="line" data-startTime="533">a particular decay channel. </span> <span class="line" data-startTime="535">We calculate effective mass of </span><span class="line" data-startTime="535">such combination, and plug in </span><span class="line" data-startTime="538">a histogram of effective mass. </span> <span class="line" data-startTime="542">And eventually we expect to see </span><span class="line" data-startTime="542">a peak in that histogram </span><span class="line" data-startTime="545">that corresponds to the mass </span><span class="line" data-startTime="545">of the parent particle. </span> <span class="line" data-startTime="548">Of course, there may be other </span><span class="line" data-startTime="548">particles that decay into the </span><span class="line" data-startTime="553">same final state, and you </span><span class="line" data-startTime="553">will see a spectrum in </span><span class="line" data-startTime="555">distribution. </span></p>

<p><span class="line" data-startTime="557">But they should be at peak </span><span class="line" data-startTime="557">corresponding to the mass of </span><span class="line" data-startTime="559">the particle you search for. </span> <span class="line" data-startTime="563">There will be noise, but if </span><span class="line" data-startTime="563">particle exists, there will be </span><span class="line" data-startTime="565">a signal simply because </span><span class="line" data-startTime="565">of conservation </span><span class="line" data-startTime="568">of energy and momentum. </span> <span class="line" data-startTime="573">And on this slide, you can see </span><span class="line" data-startTime="573">how the discovery of the Higgs </span><span class="line" data-startTime="575">boson unfolded. </span> <span class="line" data-startTime="578">Here, looking at the spectrum </span><span class="line" data-startTime="578">of effective mass of </span><span class="line" data-startTime="580">[INAUDIBLE] </span><span class="line" data-startTime="582">detected by ATLAS. </span> <span class="line" data-startTime="583">Decaying to [INAUDIBLE] </span><span class="line" data-startTime="584">is one of the predicted decay </span><span class="line" data-startTime="584">channels for the Higgs boson. </span> <span class="line" data-startTime="587">And as we were collecting </span><span class="line" data-startTime="587">more and more data, </span><span class="line" data-startTime="590">the peak at 125 GV &mdash; </span><span class="line" data-startTime="593">and GV is a unit of mass used in </span><span class="line" data-startTime="593">high energy physics &mdash; just </span><span class="line" data-startTime="596">kept growing, indicating that </span><span class="line" data-startTime="596">there is a new particle, very </span><span class="line" data-startTime="600">heavy, never seen before, clear </span><span class="line" data-startTime="600">and beautiful peak. </span></p>

<p><span class="line" data-startTime="607">And of course, you </span><span class="line" data-startTime="607">do search in many </span><span class="line" data-startTime="609">different decay channels. </span> <span class="line" data-startTime="611">Higgs can decay into anything </span><span class="line" data-startTime="611">that is not forbidden by </span><span class="line" data-startTime="614">conservation laws. </span> <span class="line" data-startTime="615">In one event, it can decay into </span><span class="line" data-startTime="615">gamma [INAUDIBLE], in </span><span class="line" data-startTime="617">another into [INAUDIBLE], and so </span><span class="line" data-startTime="617">forth, and some events may </span><span class="line" data-startTime="621">have no Higgs in there. </span> <span class="line" data-startTime="624">But it's the same Higgs, so in </span><span class="line" data-startTime="624">every decay channel, you </span><span class="line" data-startTime="627">should expect a peak at the same </span><span class="line" data-startTime="627">place, at the same mass </span><span class="line" data-startTime="632">in these various </span><span class="line" data-startTime="632">decay channels. </span> <span class="line" data-startTime="635">And you see the clear indication </span><span class="line" data-startTime="635">that there is </span><span class="line" data-startTime="637">something at 125 GV. </span> <span class="line" data-startTime="641">Now, let's talk about data </span><span class="line" data-startTime="641">challenges of such analysis. </span></p>

<p><span class="line" data-startTime="646">Particles that we want to </span><span class="line" data-startTime="646">discover and study are rare. </span> <span class="line" data-startTime="649">That's why LHC runs at such </span><span class="line" data-startTime="649">high energy and with such </span><span class="line" data-startTime="651">intense beams, almost a billion </span><span class="line" data-startTime="653">interactions per second. </span> <span class="line" data-startTime="655">Even in this condition, the </span><span class="line" data-startTime="655">probability to create Higgs </span><span class="line" data-startTime="658">boson is tiny. </span> <span class="line" data-startTime="659">You would need to search for </span><span class="line" data-startTime="659">one Higgs in more than a </span><span class="line" data-startTime="661">trillion events. </span> <span class="line" data-startTime="663">And at high beam densities, </span><span class="line" data-startTime="663">multiple collisions can occur </span><span class="line" data-startTime="666">in one beam crossing. </span> <span class="line" data-startTime="670">This plot shows how such </span><span class="line" data-startTime="670">event looks like. </span> <span class="line" data-startTime="674">It's quite messy. </span> <span class="line" data-startTime="676">That explains why you need such </span><span class="line" data-startTime="676">a big detector, such high </span><span class="line" data-startTime="679">granularity, such strong </span><span class="line" data-startTime="679">magnetic fields, so many </span><span class="line" data-startTime="682">channels of sensors, so many </span><span class="line" data-startTime="682">channels of electronics, </span><span class="line" data-startTime="686">because you are searching </span><span class="line" data-startTime="686">for something very rare. </span> <span class="line" data-startTime="690">And that's probably for Google </span><span class="line" data-startTime="690">Developers, at a familiar </span><span class="line" data-startTime="696">problem, your selectivity should </span><span class="line" data-startTime="696">be very, very high, </span><span class="line" data-startTime="699">like one in several trillions. </span> <span class="line" data-startTime="702">It's like looking for one </span><span class="line" data-startTime="702">person's cells in the world </span><span class="line" data-startTime="704">population, of one needle </span><span class="line" data-startTime="704">in 20 million haystacks. </span></p>

<p><span class="line" data-startTime="710">ATLAS is the quintessential </span><span class="line" data-startTime="710">big data experiment. </span> <span class="line" data-startTime="714">ATLAS detector generates </span><span class="line" data-startTime="714">about one petabyte per </span><span class="line" data-startTime="716">second of raw data. </span> <span class="line" data-startTime="719">No one can store it, </span><span class="line" data-startTime="719">even Google. </span> <span class="line" data-startTime="721">Most is filtered out in real </span><span class="line" data-startTime="721">time by the trigger system. </span> <span class="line" data-startTime="724">Interesting events are </span><span class="line" data-startTime="724">recorded for further </span><span class="line" data-startTime="727">reconstruction analysis. </span> <span class="line" data-startTime="730">And as of this year, we're </span><span class="line" data-startTime="730">managing about 140 petabytes </span><span class="line" data-startTime="733">of data worldwide, and that's </span><span class="line" data-startTime="733">distributed over </span><span class="line" data-startTime="737">100 computing centers. </span> <span class="line" data-startTime="739">And that's actually not only </span><span class="line" data-startTime="739">raw data, but the derived </span><span class="line" data-startTime="742">formats, the simulation, </span><span class="line" data-startTime="742">about 50% of our data. </span></p>

<p><span class="line" data-startTime="745">The Monte Carlo simulation of </span><span class="line" data-startTime="745">various properties, simulation </span><span class="line" data-startTime="749">of how the particle propagates </span><span class="line" data-startTime="749">a detector </span><span class="line" data-startTime="751">iteration, things like that. </span> <span class="line" data-startTime="754">And we expect that these data </span><span class="line" data-startTime="754">rates will be only growing </span><span class="line" data-startTime="758">after LHC large shut down, </span><span class="line" data-startTime="758">what's happening now in 2013, </span><span class="line" data-startTime="763">the data rate will be higher, </span><span class="line" data-startTime="763">the energy of </span><span class="line" data-startTime="765">collision will be higher. </span> <span class="line" data-startTime="766">We expect the influx of already </span><span class="line" data-startTime="766">filtered data on the </span><span class="line" data-startTime="769">level of 40 petabytes </span><span class="line" data-startTime="769">per year. </span> <span class="line" data-startTime="773">And we have to deliver this </span><span class="line" data-startTime="773">data to thousands of </span><span class="line" data-startTime="775">physicists worldwide. </span> <span class="line" data-startTime="779">Now, a little bit about </span><span class="line" data-startTime="779">ATLAS computing. </span> <span class="line" data-startTime="781">ATLAS uses grid computing </span><span class="line" data-startTime="781">paradigm for organization of </span><span class="line" data-startTime="784">distributed resources. </span> <span class="line" data-startTime="786">Job distribution is </span><span class="line" data-startTime="786">managed by PanDA </span><span class="line" data-startTime="788">Workload Management System. </span> <span class="line" data-startTime="789">PanDA stands for production </span><span class="line" data-startTime="789">analysis. </span> <span class="line" data-startTime="791">Think of it as grid </span><span class="line" data-startTime="791">metascheduler. </span> <span class="line" data-startTime="794">PanDA was developed by ATLAS, </span><span class="line" data-startTime="794">and now it manages </span><span class="line" data-startTime="796">distribution of computing jobs </span><span class="line" data-startTime="796">for hundreds of computing </span><span class="line" data-startTime="799">sites, about 100,000 cores, 100 </span><span class="line" data-startTime="799">million jobs per year, and </span><span class="line" data-startTime="804">serving thousands of users. </span></p>

<p><span class="line" data-startTime="807">Organizationally, it's a grid </span><span class="line" data-startTime="807">set up in a tiered system, </span><span class="line" data-startTime="810">highly hierarchical </span><span class="line" data-startTime="810">with the tier zero </span><span class="line" data-startTime="812">central located at CERN. </span> <span class="line" data-startTime="814">Tier zero center receives the </span><span class="line" data-startTime="814">raw data from the ATLAS </span><span class="line" data-startTime="819">detectors, performs first pass </span><span class="line" data-startTime="819">analysis, and then distributes </span><span class="line" data-startTime="822">among other tiers. </span> <span class="line" data-startTime="824">Tier one is typically national </span><span class="line" data-startTime="824">based large centers. </span> <span class="line" data-startTime="828">One of them is the </span><span class="line" data-startTime="828">Brookhaven Lab. </span> <span class="line" data-startTime="830">And each tier one facility then </span><span class="line" data-startTime="830">distributors derived data </span><span class="line" data-startTime="834">to tier two computing facilities </span><span class="line" data-startTime="834">that provides data </span><span class="line" data-startTime="836">storage and processing </span><span class="line" data-startTime="836">capabilities for more in depth </span><span class="line" data-startTime="839">end user analysis. </span> <span class="line" data-startTime="844">This plot shows the distribution </span><span class="line" data-startTime="844">of running jobs, </span><span class="line" data-startTime="846">both Monte Carlo simulation &mdash; </span><span class="line" data-startTime="847">we call them production jobs &mdash; </span><span class="line" data-startTime="849">on the ATLAS grid and in </span><span class="line" data-startTime="849">analysis jobs on the ATLAS </span><span class="line" data-startTime="852">grid for the past year. </span></p>

<p><span class="line" data-startTime="853">We run about 100,000 cores </span><span class="line" data-startTime="853">worldwide, processing 150,000 </span><span class="line" data-startTime="858">jobs per day. </span> <span class="line" data-startTime="859">It's clear that all available </span><span class="line" data-startTime="859">computational resources are </span><span class="line" data-startTime="863">fully stressed and utilized. </span> <span class="line" data-startTime="866">And on this plot, you see the </span><span class="line" data-startTime="866">distribution of pending jobs, </span><span class="line" data-startTime="869">submitted but waiting for </span><span class="line" data-startTime="869">execution on ATLAS grid for </span><span class="line" data-startTime="872">the past year. </span> <span class="line" data-startTime="873">You can see that the job </span><span class="line" data-startTime="873">submission pattern is very </span><span class="line" data-startTime="875">uneven in time. </span> <span class="line" data-startTime="877">Spikes in demand usually happen </span><span class="line" data-startTime="877">before major physics </span><span class="line" data-startTime="879">conference or during </span><span class="line" data-startTime="879">data reprocessing. </span> <span class="line" data-startTime="882">And demand can exceed available </span><span class="line" data-startTime="882">computational </span><span class="line" data-startTime="884">resources by more than an </span><span class="line" data-startTime="884">order of magnitude. </span></p>

<p><span class="line" data-startTime="887">Lack of resources slows down </span><span class="line" data-startTime="887">the pace of scientific </span><span class="line" data-startTime="890">discovery, and that's why ATLAS </span><span class="line" data-startTime="890">is interested in cloud </span><span class="line" data-startTime="893">computing, and, in particular, </span><span class="line" data-startTime="893">in public cloud resources. </span> <span class="line" data-startTime="898">A couple of years ago, ATLAS </span><span class="line" data-startTime="898">set up cloud computing R&D </span><span class="line" data-startTime="901">project to explore </span><span class="line" data-startTime="901">virtualization and cloud </span><span class="line" data-startTime="904">computing primarily as a </span><span class="line" data-startTime="904">tool to cope with peak </span><span class="line" data-startTime="906">loads on the grid. </span> <span class="line" data-startTime="907">We wanted to learn how to use </span><span class="line" data-startTime="907">public and private clouds in </span><span class="line" data-startTime="910">typical ATLAS computational </span><span class="line" data-startTime="910">scenarios. </span> <span class="line" data-startTime="913">Since then, we gained experience </span><span class="line" data-startTime="913">with many cloud </span><span class="line" data-startTime="916">platforms, like Amazon, </span><span class="line" data-startTime="916">[INAUDIBLE] </span><span class="line" data-startTime="919">consortium of European cloud </span><span class="line" data-startTime="919">providers, future grid </span><span class="line" data-startTime="922">[INAUDIBLE] academic clouds in </span><span class="line" data-startTime="922">US and Canada and many others. </span> <span class="line" data-startTime="925">We also explored private and </span><span class="line" data-startTime="925">hybrid cloud configuration </span><span class="line" data-startTime="929">based on OpenStack, cloud stack </span><span class="line" data-startTime="929">open nebula, and others. </span></p>

<p><span class="line" data-startTime="933">And now our latest project was </span><span class="line" data-startTime="933">on Google Compute Engine, and </span><span class="line" data-startTime="935">I'll talk about it </span><span class="line" data-startTime="935">now in detail. </span> <span class="line" data-startTime="938">We were invited to participate </span><span class="line" data-startTime="938">in Google Compute Engine trial </span><span class="line" data-startTime="941">period in August 2012. </span> <span class="line" data-startTime="943">And we were immediately </span><span class="line" data-startTime="943">attracted by modern hardware, </span><span class="line" data-startTime="946">powerful API, and competitive </span><span class="line" data-startTime="946">pricing. </span> <span class="line" data-startTime="949">And this is Google, after all. </span> <span class="line" data-startTime="953">At the beginning, we were </span><span class="line" data-startTime="953">frustrated that none of the </span><span class="line" data-startTime="955">tools that we had used before </span><span class="line" data-startTime="955">supported Google Compute </span><span class="line" data-startTime="957">Engine, so initial a lot of </span><span class="line" data-startTime="957">manual labor and image </span><span class="line" data-startTime="960">building cluster management was </span><span class="line" data-startTime="960">needed since we couldn't </span><span class="line" data-startTime="963">reuse our standard tools. </span> <span class="line" data-startTime="967">We're glad to see here at Google </span><span class="line" data-startTime="967">I/O that this situation </span><span class="line" data-startTime="969">is changing and many tools are </span><span class="line" data-startTime="969">supporting Google Compute </span><span class="line" data-startTime="972">Engine now. </span></p>

<p><span class="line" data-startTime="974">But Google engineers were very </span><span class="line" data-startTime="974">helpful in helping us with </span><span class="line" data-startTime="978">initial setup and debugging the </span><span class="line" data-startTime="978">problems and explaining to </span><span class="line" data-startTime="981">us features of the Google Cloud </span><span class="line" data-startTime="981">platform implementation. </span> <span class="line" data-startTime="984">And also Google was very </span><span class="line" data-startTime="984">gracious in providing more </span><span class="line" data-startTime="987">resources than the initial </span><span class="line" data-startTime="987">trial quarter so we could </span><span class="line" data-startTime="990">start working on the larger </span><span class="line" data-startTime="990">scale cluster. </span> <span class="line" data-startTime="994">We wanted to try several ATLAS </span><span class="line" data-startTime="994">computational scenarios, high </span><span class="line" data-startTime="998">performance analysis clusters </span><span class="line" data-startTime="998">like PROOF. </span> <span class="line" data-startTime="1000">We wanted to learn about storage </span><span class="line" data-startTime="1000">and data management of </span><span class="line" data-startTime="1003">the cloud, in particular </span><span class="line" data-startTime="1003">utilizing XRootD technology </span><span class="line" data-startTime="1005">for storage aggregation, </span><span class="line" data-startTime="1005">Ephemeral Storage aggregation, </span><span class="line" data-startTime="1009">and federation. </span></p>

<p><span class="line" data-startTime="1011">We also wanted to try a life </span><span class="line" data-startTime="1011">scale Monte Carlo production </span><span class="line" data-startTime="1014">simulation of the cloud using </span><span class="line" data-startTime="1014">PanDA Workload Management </span><span class="line" data-startTime="1017">System, as well as some other </span><span class="line" data-startTime="1017">smaller projects. </span> <span class="line" data-startTime="1021">So let me talk about </span><span class="line" data-startTime="1021">PanDA [INAUDIBLE] </span><span class="line" data-startTime="1023">on Google Compute Engine. </span> <span class="line" data-startTime="1025">Google agreed to allocate </span><span class="line" data-startTime="1025">additional resources for ATLAS </span><span class="line" data-startTime="1028">at the tune of about five </span><span class="line" data-startTime="1028">million core hours. </span> <span class="line" data-startTime="1032">Resources were organized as </span><span class="line" data-startTime="1032">HTCondor PanDA queue, and that </span><span class="line" data-startTime="1040">allows for transparent inclusion </span><span class="line" data-startTime="1040">of the cloud </span><span class="line" data-startTime="1042">resource into ATLAS </span><span class="line" data-startTime="1042">computational grid. </span> <span class="line" data-startTime="1045">Google Compute Engine looks as a </span><span class="line" data-startTime="1045">part of the ATLAS grid, just </span><span class="line" data-startTime="1047">like another grid site. </span> <span class="line" data-startTime="1049">Very transparent. </span></p>

<p><span class="line" data-startTime="1050">It was intended to run CPU </span><span class="line" data-startTime="1050">intensive Monte Carlo </span><span class="line" data-startTime="1052">simulation, and the idea was </span><span class="line" data-startTime="1052">to try to have a production </span><span class="line" data-startTime="1056">type of run on Google </span><span class="line" data-startTime="1056">Compute Engine. </span> <span class="line" data-startTime="1058">And the system was delivered </span><span class="line" data-startTime="1058">to ATLAS as a production </span><span class="line" data-startTime="1060">resource, not as R&D platform. </span> <span class="line" data-startTime="1064">We ran for about eight weeks. </span> <span class="line" data-startTime="1066">Two weeks were planned </span><span class="line" data-startTime="1066">for start up. </span> <span class="line" data-startTime="1068">And we had very stable </span><span class="line" data-startTime="1068">running. </span> <span class="line" data-startTime="1073">The Google Compute Engine </span><span class="line" data-startTime="1073">was rock solid. </span> <span class="line" data-startTime="1075">We had a few problems, and most </span><span class="line" data-startTime="1075">of them were on the ATLAS </span><span class="line" data-startTime="1077">side, were on computational </span><span class="line" data-startTime="1077">intensive job, not much I/O, </span><span class="line" data-startTime="1080">for this particular workload. </span> <span class="line" data-startTime="1082">These were physics [INAUDIBLE] </span><span class="line" data-startTime="1082">generators [INAUDIBLE] </span><span class="line" data-startTime="1084">fast detector simulation, full </span><span class="line" data-startTime="1084">detector simulation. </span></p>

<p><span class="line" data-startTime="1087">Produced data was automatically </span><span class="line" data-startTime="1087">shipped to ATLAS </span><span class="line" data-startTime="1089">grid storage for further </span><span class="line" data-startTime="1089">processing and analysis. </span> <span class="line" data-startTime="1091">That was really usable data. </span> <span class="line" data-startTime="1094">We completed about 450,000 jobs </span><span class="line" data-startTime="1094">generated and processed </span><span class="line" data-startTime="1100">about more than 200 </span><span class="line" data-startTime="1100">million events. </span> <span class="line" data-startTime="1103">Very good performance, very </span><span class="line" data-startTime="1103">comparable to performance of </span><span class="line" data-startTime="1105">ATLAS grid. </span> <span class="line" data-startTime="1108">This plot shows the job </span><span class="line" data-startTime="1108">failure rate as </span><span class="line" data-startTime="1110">a function of time. </span> <span class="line" data-startTime="1111">Most failures occurred during </span><span class="line" data-startTime="1111">start up and scale up period </span><span class="line" data-startTime="1114">as we expected. </span> <span class="line" data-startTime="1115">Most problems were actually </span><span class="line" data-startTime="1115">on the ATLAS side. </span> <span class="line" data-startTime="1118">No failures were due to the </span><span class="line" data-startTime="1118">Google Compute Engine. </span> <span class="line" data-startTime="1120">Very stable performance </span><span class="line" data-startTime="1120">of the platform. </span></p>

<p><span class="line" data-startTime="1124">This plot shows distribution of </span><span class="line" data-startTime="1124">finished and failed jobs. </span> <span class="line" data-startTime="1126">Green histogram is for finished </span><span class="line" data-startTime="1126">jobs, the pink one </span><span class="line" data-startTime="1129">for failed ones. </span> <span class="line" data-startTime="1130">Again, very good performance. </span> <span class="line" data-startTime="1131">We reached high rates </span><span class="line" data-startTime="1131">of production, </span><span class="line" data-startTime="1133">50,000 jobs per day. </span> <span class="line" data-startTime="1135">Good number. </span> <span class="line" data-startTime="1138">We also tried PROOF clusters. </span> <span class="line" data-startTime="1141">And PROOF is implementation of </span><span class="line" data-startTime="1141">MapReduce paradigm based on </span><span class="line" data-startTime="1144">the ROOT framework. </span> <span class="line" data-startTime="1145">ROOT framework for data analysis </span><span class="line" data-startTime="1145">was developed by high </span><span class="line" data-startTime="1149">energy nuclear physics </span><span class="line" data-startTime="1149">community, developed and </span><span class="line" data-startTime="1152">supported by the ROOT </span><span class="line" data-startTime="1152">team at CERN. </span> <span class="line" data-startTime="1154">It's written in C++, free, </span><span class="line" data-startTime="1154">open source, very high </span><span class="line" data-startTime="1157">performance. </span></p>

<p><span class="line" data-startTime="1158">And we'll have a slide with the </span><span class="line" data-startTime="1158">pointers to the system. </span> <span class="line" data-startTime="1161">And PROOF allows for efficient </span><span class="line" data-startTime="1161">aggregation and use of </span><span class="line" data-startTime="1164">distributed computing resources </span><span class="line" data-startTime="1164">for data intensive </span><span class="line" data-startTime="1167">event based analyses. </span> <span class="line" data-startTime="1169">It uses XRootD for clustering, </span><span class="line" data-startTime="1169">storage </span><span class="line" data-startTime="1172">aggregation, data discovery. </span> <span class="line" data-startTime="1174">Xroot is well suited for </span><span class="line" data-startTime="1174">ephemeral storage aggregation </span><span class="line" data-startTime="1177">into one name space. </span> <span class="line" data-startTime="1180">And PROOF cluster also </span><span class="line" data-startTime="1180">can be federated. </span> <span class="line" data-startTime="1182">So on this slide, you can see </span><span class="line" data-startTime="1182">typical architecture of the </span><span class="line" data-startTime="1186">PROOF clusters with the super </span><span class="line" data-startTime="1186">master, which serves as users' </span><span class="line" data-startTime="1190">single point of entry. </span> <span class="line" data-startTime="1191">System complex is completely </span><span class="line" data-startTime="1191">hidden from users. </span> <span class="line" data-startTime="1195">And it allows, for example, </span><span class="line" data-startTime="1195">for interactive analysis, </span><span class="line" data-startTime="1199">where you can send the query and </span><span class="line" data-startTime="1199">in real time see how the </span><span class="line" data-startTime="1202">particular histogram just </span><span class="line" data-startTime="1202">grows and changes. </span> <span class="line" data-startTime="1205">One of the distinctive </span><span class="line" data-startTime="1205">feature. </span> <span class="line" data-startTime="1207">But it also allows batch </span><span class="line" data-startTime="1207">analysis, and you can connect </span><span class="line" data-startTime="1209">to the system, look at the </span><span class="line" data-startTime="1209">histogram, and disconnect, </span><span class="line" data-startTime="1212">then connect again, look again, </span><span class="line" data-startTime="1212">see how it's going. </span></p>

<p><span class="line" data-startTime="1217">And here's another view of </span><span class="line" data-startTime="1217">the typical structure </span><span class="line" data-startTime="1219">of the PROOF cluster. </span> <span class="line" data-startTime="1220">PROOF, as I mentioned, </span><span class="line" data-startTime="1220">works very well with </span><span class="line" data-startTime="1222">XRootD based storage. </span> <span class="line" data-startTime="1224">XRootD provides data discovery </span><span class="line" data-startTime="1224">and hails PROOF exploits this </span><span class="line" data-startTime="1228">data locality [INAUDIBLE]. </span> <span class="line" data-startTime="1232">Access to Google Compute Engine </span><span class="line" data-startTime="1232">allowed us to build and </span><span class="line" data-startTime="1234">test large PROOF clusters, up </span><span class="line" data-startTime="1234">to 1,000 workers, something </span><span class="line" data-startTime="1238">that is very difficult to </span><span class="line" data-startTime="1238">do in the real domain. </span> <span class="line" data-startTime="1240">We just don't have resources </span><span class="line" data-startTime="1240">to do this kind of test and </span><span class="line" data-startTime="1244">see how it all scales. </span> <span class="line" data-startTime="1245">And the figure here shows </span><span class="line" data-startTime="1245">scalability test for 500 </span><span class="line" data-startTime="1248">worker cluster. </span></p>

<p><span class="line" data-startTime="1250">And it shows very good </span><span class="line" data-startTime="1250">performance and pretty good </span><span class="line" data-startTime="1252">scalability. </span> <span class="line" data-startTime="1257">We also look at the storage </span><span class="line" data-startTime="1257">performance of </span><span class="line" data-startTime="1261">Google Compute Engine. </span> <span class="line" data-startTime="1262">And this plot shows the </span><span class="line" data-startTime="1262">performance in the typical </span><span class="line" data-startTime="1269">ATLAS analysis scenario of </span><span class="line" data-startTime="1269">ephemeral store, persistent </span><span class="line" data-startTime="1274">store, and here compare it to </span><span class="line" data-startTime="1274">what happened if you have all </span><span class="line" data-startTime="1277">the data in the memory. </span> <span class="line" data-startTime="1279">And know that ephemeral disk </span><span class="line" data-startTime="1279">has better single worker </span><span class="line" data-startTime="1282">performance, but the persistent </span><span class="line" data-startTime="1282">storage shows </span><span class="line" data-startTime="1287">better scaling and better </span><span class="line" data-startTime="1287">peak performance. </span> <span class="line" data-startTime="1289">And of course, in this situation </span><span class="line" data-startTime="1289">it's clear that the </span><span class="line" data-startTime="1292">RAID is needed for better </span><span class="line" data-startTime="1292">performance. </span></p>

<p><span class="line" data-startTime="1297">We also look at the data </span><span class="line" data-startTime="1297">transfer capabilities. </span> <span class="line" data-startTime="1303">And this plot shows the data </span><span class="line" data-startTime="1303">transfer from our own </span><span class="line" data-startTime="1307">Federated ATLAS Xroot to Google </span><span class="line" data-startTime="1307">Compute Engine in </span><span class="line" data-startTime="1311">extreme copy mode, which is sort </span><span class="line" data-startTime="1311">of similar to bitTorrent. </span> <span class="line" data-startTime="1314">If you have several copies, </span><span class="line" data-startTime="1314">you can copy them in </span><span class="line" data-startTime="1317">multisource, multistream mode. </span> <span class="line" data-startTime="1319">And Google Compute Engine Xroot </span><span class="line" data-startTime="1319">cluster using ephemeral </span><span class="line" data-startTime="1323">storage was used for this test, </span><span class="line" data-startTime="1323">and average transfer </span><span class="line" data-startTime="1327">rate was about 60 megabyte </span><span class="line" data-startTime="1327">per second. </span> <span class="line" data-startTime="1331">And this is very good taking </span><span class="line" data-startTime="1331">into account that this is over </span><span class="line" data-startTime="1334">completely unmanaged </span><span class="line" data-startTime="1334">public network. </span> <span class="line" data-startTime="1336">We have no control there. </span> <span class="line" data-startTime="1338">But still, this is single </span><span class="line" data-startTime="1338">client performance. </span></p>

<p><span class="line" data-startTime="1340">Many of the plots that I'm </span><span class="line" data-startTime="1340">showing, they're single client </span><span class="line" data-startTime="1344">because the system that we're </span><span class="line" data-startTime="1344">running, they scale very well </span><span class="line" data-startTime="1346">when you add resources. </span> <span class="line" data-startTime="1348">So you expect clustering and </span><span class="line" data-startTime="1348">scaling up, so what you are </span><span class="line" data-startTime="1351">really interested in is how the </span><span class="line" data-startTime="1352">building blocks are running. </span> <span class="line" data-startTime="1354">And here is single client, but </span><span class="line" data-startTime="1354">you can run on multiple VMs, </span><span class="line" data-startTime="1357">multiple clients simultaneously </span><span class="line" data-startTime="1358">and stream them over. </span> <span class="line" data-startTime="1361">And we're also thinking about </span><span class="line" data-startTime="1361">dedicated network peering </span><span class="line" data-startTime="1364">between ATLAS network </span><span class="line" data-startTime="1366">infrastructure and Google storage. </span> <span class="line" data-startTime="1368">So that will give us much </span><span class="line" data-startTime="1368">higher, 100 gigabyte per </span><span class="line" data-startTime="1372">second performance, than we </span><span class="line" data-startTime="1372">would be able to control it </span><span class="line" data-startTime="1375">and do manageable transfers. </span></p>

<p><span class="line" data-startTime="1377">But this is if you just want to </span><span class="line" data-startTime="1377">run and bring the data in. </span> <span class="line" data-startTime="1380">It's doable. </span> <span class="line" data-startTime="1384">And now we'll talk about Xroot, </span><span class="line" data-startTime="1384">this clustering and </span><span class="line" data-startTime="1388">storage clustering technology </span><span class="line" data-startTime="1388">that Andy was a creator and a </span><span class="line" data-startTime="1392">driving force behind this </span><span class="line" data-startTime="1392">project, and still is. </span> <span class="line" data-startTime="1395">And Andy, please. </span> <span class="line" data-startTime="1396">[APPLAUSE] </span><span class="line" data-startTime="1400">ANDREW HANUSHEVSKY: </span><span class="line" data-startTime="1400">[INAUDIBLE]. </span> <span class="line" data-startTime="1403">So I want to take a quick trip </span><span class="line" data-startTime="1403">through a bit of technology we </span><span class="line" data-startTime="1407">developed a while back but wound </span><span class="line" data-startTime="1407">up being absolutely an </span><span class="line" data-startTime="1412">ideal match with the Google </span><span class="line" data-startTime="1412">Compute Engine. </span> <span class="line" data-startTime="1414">And that's XRootD. </span> <span class="line" data-startTime="1415">Now you'll say, never </span><span class="line" data-startTime="1415">heard of it. </span></p>

<p><span class="line" data-startTime="1417">Well, it's a system for scalable </span><span class="line" data-startTime="1417">cluster data access. </span> <span class="line" data-startTime="1420">And you say, that's nice. </span> <span class="line" data-startTime="1421">What is it really? </span><span class="line" data-startTime="1423">Well, what we have is an </span><span class="line" data-startTime="1423">implementation of two </span><span class="line" data-startTime="1425">services, one an XRoot service </span><span class="line" data-startTime="1425">that provides access to data. </span> <span class="line" data-startTime="1429">So you would take one of these </span><span class="line" data-startTime="1429">demons and drop it on every </span><span class="line" data-startTime="1432">node where you have data that </span><span class="line" data-startTime="1432">you need to access. </span> <span class="line" data-startTime="1435">Now, there's a companion </span><span class="line" data-startTime="1435">service. </span> <span class="line" data-startTime="1437">It's called CMSD. </span> <span class="line" data-startTime="1438">Stands for Cluster Management </span><span class="line" data-startTime="1438">Services. </span> <span class="line" data-startTime="1441">And that's used for data </span><span class="line" data-startTime="1441">discovery as well as routing </span><span class="line" data-startTime="1444">clients to where the data is </span><span class="line" data-startTime="1444">and server clustering. </span> <span class="line" data-startTime="1448">So these are separate, but we </span><span class="line" data-startTime="1448">normally use them together. </span> <span class="line" data-startTime="1451">And so for the purposes of this </span><span class="line" data-startTime="1451">talk, we'll always talk </span><span class="line" data-startTime="1454">about this particular pair. </span></p>

<p><span class="line" data-startTime="1456">Now I want to emphasize </span><span class="line" data-startTime="1456">that this system </span><span class="line" data-startTime="1459">is not a file system. </span> <span class="line" data-startTime="1461">People are actually using the </span><span class="line" data-startTime="1461">system to cluster existing </span><span class="line" data-startTime="1464">file systems. </span> <span class="line" data-startTime="1465">So we have people basically </span><span class="line" data-startTime="1465">taking HDFS, GPFS, Lustre, and </span><span class="line" data-startTime="1470">building one big giant cluster </span><span class="line" data-startTime="1470">out of that and having uniform </span><span class="line" data-startTime="1474">data access. </span> <span class="line" data-startTime="1476">So while it's not a file system, </span><span class="line" data-startTime="1476">it's also not just for </span><span class="line" data-startTime="1480">file systems. </span> <span class="line" data-startTime="1481">We have an experiment that's </span><span class="line" data-startTime="1481">using this as a framework to </span><span class="line" data-startTime="1485">cluster MySQL tables across </span><span class="line" data-startTime="1485">hundreds of MySQL servers so </span><span class="line" data-startTime="1491">they could do massively </span><span class="line" data-startTime="1491">parallel queries. </span> <span class="line" data-startTime="1495">So the idea is that if we don't </span><span class="line" data-startTime="1495">have a plug-in for your </span><span class="line" data-startTime="1499">data, and you can write a </span><span class="line" data-startTime="1499">plug-in for your data, then </span><span class="line" data-startTime="1502">you can cluster it. </span></p>

<p><span class="line" data-startTime="1503">And so I'd like to show you what </span><span class="line" data-startTime="1503">that plug-in architecture </span><span class="line" data-startTime="1506">looks like. </span> <span class="line" data-startTime="1507">First, we start off with </span><span class="line" data-startTime="1507">a protocol driver. </span> <span class="line" data-startTime="1511">You can plug-in any number of </span><span class="line" data-startTime="1511">protocols into that driver. </span> <span class="line" data-startTime="1514">In our particular case, </span><span class="line" data-startTime="1514">we want to do XRoot. </span> <span class="line" data-startTime="1516">So let's take a look as we </span><span class="line" data-startTime="1516">plug stuff together. </span> <span class="line" data-startTime="1519">So plug in your protocol. </span> <span class="line" data-startTime="1521">Plug in your authentication </span><span class="line" data-startTime="1521">framework. </span> <span class="line" data-startTime="1524">Plug in your logical file </span><span class="line" data-startTime="1524">system, your authorization </span><span class="line" data-startTime="1527">framework, your storage </span><span class="line" data-startTime="1527">system, and then your </span><span class="line" data-startTime="1530">clustering. </span> <span class="line" data-startTime="1531">And all of a sudden, you've </span><span class="line" data-startTime="1531">built up a clustering system </span><span class="line" data-startTime="1534">for a particular kind </span><span class="line" data-startTime="1534">of application. </span></p>

<p><span class="line" data-startTime="1538">So let's get back to what the </span><span class="line" data-startTime="1538">data access problem is. </span> <span class="line" data-startTime="1542">It's the High Energy </span><span class="line" data-startTime="1542">Physics regime. </span> <span class="line" data-startTime="1544">Yeah, they do nasty things, </span><span class="line" data-startTime="1544">like start up thousands of </span><span class="line" data-startTime="1548">parallel jobs. </span> <span class="line" data-startTime="1550">And they all start up pretty </span><span class="line" data-startTime="1550">much at the same time. </span> <span class="line" data-startTime="1553">And if that weren't bad enough, </span><span class="line" data-startTime="1553">each one of those </span><span class="line" data-startTime="1555">opens 10 or more files. </span> <span class="line" data-startTime="1558">Pretty much a profile </span><span class="line" data-startTime="1558">[INAUDIBLE] denial service </span><span class="line" data-startTime="1560">attack, if you ask me. </span> <span class="line" data-startTime="1562">But basically, you have </span><span class="line" data-startTime="1562">to handle that. </span> <span class="line" data-startTime="1565">To make matters worse, the </span><span class="line" data-startTime="1565">particular framework that they </span><span class="line" data-startTime="1569">use is small block sparse random </span><span class="line" data-startTime="1569">I/O. What do we mean by </span><span class="line" data-startTime="1573">small block? </span><span class="line" data-startTime="1574">Average read size about 4K. </span> <span class="line" data-startTime="1576">What do we mean by sparse? </span><span class="line" data-startTime="1578">Well, they have like a </span><span class="line" data-startTime="1578">10-gigabyte file, and you'll </span><span class="line" data-startTime="1582">be lucky if they read </span><span class="line" data-startTime="1582">half of it randomly. </span></p>

<p><span class="line" data-startTime="1585">So pretty challenging. </span> <span class="line" data-startTime="1587">So we adopted a synergistic </span><span class="line" data-startTime="1587">solution to try </span><span class="line" data-startTime="1591">to attack this problem. </span> <span class="line" data-startTime="1593">And you'll see what we </span><span class="line" data-startTime="1593">mean by synergy here. </span> <span class="line" data-startTime="1597">First, we wanted to minimize </span><span class="line" data-startTime="1597">the latency. </span> <span class="line" data-startTime="1599">And the key elements there </span><span class="line" data-startTime="1599">were using a paralyzable </span><span class="line" data-startTime="1602">protocol, file sessions, a </span><span class="line" data-startTime="1602">sticky thread model, and </span><span class="line" data-startTime="1606">lockless I/O. Next, we wanted </span><span class="line" data-startTime="1606">to minimize hardware </span><span class="line" data-startTime="1610">requirements, so short code </span><span class="line" data-startTime="1610">paths, compact data </span><span class="line" data-startTime="1614">structures, members that are </span><span class="line" data-startTime="1614">cognizant of what the memory </span><span class="line" data-startTime="1618">cache is so it's friendly </span><span class="line" data-startTime="1618">to the memory cache. </span> <span class="line" data-startTime="1623">We don't actually move data </span><span class="line" data-startTime="1623">around in the server, and we </span><span class="line" data-startTime="1626">don't do crossthread </span><span class="line" data-startTime="1626">data sharing. </span></p>

<p><span class="line" data-startTime="1628">So in the end, we wind up with </span><span class="line" data-startTime="1628">less than seven microseconds </span><span class="line" data-startTime="1632">overhead on a two gigahertz CPU </span><span class="line" data-startTime="1632">per I/O request and less </span><span class="line" data-startTime="1636">than a 100-megabyte </span><span class="line" data-startTime="1636">memory footprint. </span> <span class="line" data-startTime="1638">So pretty compact. </span> <span class="line" data-startTime="1640">Now those two are synergistic </span><span class="line" data-startTime="1640">in the sense that if you </span><span class="line" data-startTime="1643">minimize latency, you'll see </span><span class="line" data-startTime="1643">opportunities to minimize </span><span class="line" data-startTime="1647">hardware requirements. </span> <span class="line" data-startTime="1648">And if you start minimizing </span><span class="line" data-startTime="1648">hardware requirements, you see </span><span class="line" data-startTime="1652">opportunities to minimize </span><span class="line" data-startTime="1652">latency. </span> <span class="line" data-startTime="1655">The next thing we wanted to do </span><span class="line" data-startTime="1655">was minimize human cost. </span> <span class="line" data-startTime="1659">So what does that mean? </span><span class="line" data-startTime="1660">Well for us that meant a single </span><span class="line" data-startTime="1660">configuration file, no </span><span class="line" data-startTime="1665">database requirement. </span> <span class="line" data-startTime="1667">You can add and delete </span><span class="line" data-startTime="1667">nodes at will. </span></p>

<p><span class="line" data-startTime="1669">We don't care. </span> <span class="line" data-startTime="1670">You don't have to restart </span><span class="line" data-startTime="1670">anything. </span> <span class="line" data-startTime="1671">You just add stuff </span><span class="line" data-startTime="1671">and delete stuff. </span> <span class="line" data-startTime="1674">And you use your natural file </span><span class="line" data-startTime="1674">system administration tools to </span><span class="line" data-startTime="1678">administer this thing because </span><span class="line" data-startTime="1678">that's what you know. </span> <span class="line" data-startTime="1682">Now, that together, we wanted </span><span class="line" data-startTime="1682">to maximize scaling. </span> <span class="line" data-startTime="1686">And those two are actually </span><span class="line" data-startTime="1686">synergistic. </span> <span class="line" data-startTime="1689">You can't maximize scaling </span><span class="line" data-startTime="1689">unless you </span><span class="line" data-startTime="1691">minimize the human cost. </span> <span class="line" data-startTime="1693">And you see immediately </span><span class="line" data-startTime="1693">opportunities between those </span><span class="line" data-startTime="1695">two as you attack both </span><span class="line" data-startTime="1695">of those problems. </span> <span class="line" data-startTime="1698">So let's talk about scaling. </span> <span class="line" data-startTime="1701">We used B64 trees for scaling. </span> <span class="line" data-startTime="1704">And we'll basically scale this </span><span class="line" data-startTime="1704">using this pair, XRootD CMS. </span></p>

<p><span class="line" data-startTime="1709">So let's start out with </span><span class="line" data-startTime="1709">a single node. </span> <span class="line" data-startTime="1712">And then what we'll do </span><span class="line" data-startTime="1712">is we'll add 64 data </span><span class="line" data-startTime="1715">servers to that node. </span> <span class="line" data-startTime="1719">Gee, looks like a really dinky </span><span class="line" data-startTime="1719">cluster, doesn't it? </span><span class="line" data-startTime="1722">So what we do, it's a B64 tree, </span><span class="line" data-startTime="1722">well, we'll add 64 data </span><span class="line" data-startTime="1726">servers to each one </span><span class="line" data-startTime="1726">of those 64 nodes. </span> <span class="line" data-startTime="1728">Well now we have a </span><span class="line" data-startTime="1728">cluster of 4,096. </span> <span class="line" data-startTime="1732">Decent, but not very big. </span> <span class="line" data-startTime="1734">We can just repeat this step. </span> <span class="line" data-startTime="1736">And now we've accomplished a </span><span class="line" data-startTime="1736">cluster of 262,000 servers. </span> <span class="line" data-startTime="1742">Well, that looks big, but what </span><span class="line" data-startTime="1742">if we iterate one more time? </span><span class="line" data-startTime="1746">Now we've constructed </span><span class="line" data-startTime="1746">a cluster of 16 </span><span class="line" data-startTime="1749">million data servers. </span> <span class="line" data-startTime="1752">And you look at that and </span><span class="line" data-startTime="1752">say, hm, that looks </span><span class="line" data-startTime="1755">like a house of cards. </span> <span class="line" data-startTime="1757">Well, not really because we can </span><span class="line" data-startTime="1757">replicate the head node, </span><span class="line" data-startTime="1761">geographically distribute it, </span><span class="line" data-startTime="1761">and now we have quite a bit of </span><span class="line" data-startTime="1764">redundancy in the system. </span></p>

<p><span class="line" data-startTime="1766">So let's add some names </span><span class="line" data-startTime="1766">to these things. </span> <span class="line" data-startTime="1769">The head node is the manager. </span> <span class="line" data-startTime="1770">Intermediate nodes </span><span class="line" data-startTime="1770">are supervisors. </span> <span class="line" data-startTime="1772">And data servers always </span><span class="line" data-startTime="1772">are at the leaf nodes. </span> <span class="line" data-startTime="1775">So remember that. </span> <span class="line" data-startTime="1778">Now, this is a B tree. </span> <span class="line" data-startTime="1779">We can split it up </span><span class="line" data-startTime="1779">any way we want. </span> <span class="line" data-startTime="1782">And this works great for </span><span class="line" data-startTime="1782">basically doing cloud </span><span class="line" data-startTime="1784">deployment because in fact </span><span class="line" data-startTime="1784">part of that three can be </span><span class="line" data-startTime="1788">inside the GCE, another part </span><span class="line" data-startTime="1788">can be in a private cloud, </span><span class="line" data-startTime="1792">another part in a private </span><span class="line" data-startTime="1792">cluster, and we can piece that </span><span class="line" data-startTime="1795">all together to make it look </span><span class="line" data-startTime="1795">like one big cluster. </span></p>

<p><span class="line" data-startTime="1800">Now, you look at that </span><span class="line" data-startTime="1800">and say great. </span> <span class="line" data-startTime="1802">But I have 16 million nodes. </span> <span class="line" data-startTime="1804">How do I get to the data? </span><span class="line" data-startTime="1805">I can't keep track of </span><span class="line" data-startTime="1805">16 million things. </span> <span class="line" data-startTime="1808">Well, in fact, let's take </span><span class="line" data-startTime="1808">a look at how we do it. </span> <span class="line" data-startTime="1813">So when you've got a big cluster </span><span class="line" data-startTime="1813">like this, you really </span><span class="line" data-startTime="1817">have to adopt a brand </span><span class="line" data-startTime="1817">new strategy. </span> <span class="line" data-startTime="1820">So we have a client. </span> <span class="line" data-startTime="1822">He gets an open or doesn't </span><span class="line" data-startTime="1822">open to the head node. </span> <span class="line" data-startTime="1825">And here we're going to assume </span><span class="line" data-startTime="1825">that the head node knows </span><span class="line" data-startTime="1829">nothing about the file </span><span class="line" data-startTime="1829">the client needs. </span></p>

<p><span class="line" data-startTime="1832">So what does it have to do? </span><span class="line" data-startTime="1834">It has to find the route </span><span class="line" data-startTime="1834">to the file. </span> <span class="line" data-startTime="1837">And it'll accomplish that </span><span class="line" data-startTime="1837">by just doing a directed </span><span class="line" data-startTime="1839">broadcast parallel query. </span> <span class="line" data-startTime="1841">And that'll set up the routing </span><span class="line" data-startTime="1841">tables in this tree. </span> <span class="line" data-startTime="1844">After that, the head node can </span><span class="line" data-startTime="1844">then redirect the client to </span><span class="line" data-startTime="1848">the next subtree. </span> <span class="line" data-startTime="1850">That subtree in turns </span><span class="line" data-startTime="1850">redirects the </span><span class="line" data-startTime="1852">client to the leaf node. </span> <span class="line" data-startTime="1854">Now, this is the only scalable </span><span class="line" data-startTime="1854">way of doing it because you </span><span class="line" data-startTime="1858">don't have to keep track </span><span class="line" data-startTime="1858">of anything but </span><span class="line" data-startTime="1859">the immediate route. </span> <span class="line" data-startTime="1861">Pretty much in how the </span><span class="line" data-startTime="1861">internet works. </span> <span class="line" data-startTime="1864">So the bottom line here, this </span><span class="line" data-startTime="1864">is a simple, flexible, and </span><span class="line" data-startTime="1868">effective system. </span> <span class="line" data-startTime="1869">I want to say it's simple, but </span><span class="line" data-startTime="1869">the devil's in the details. </span> <span class="line" data-startTime="1873">We have a paper we presented </span><span class="line" data-startTime="1873">in IPDS that will give you </span><span class="line" data-startTime="1876">some of the algorithms </span><span class="line" data-startTime="1876">we have to use. </span></p>

<p><span class="line" data-startTime="1880">And you can actually get the </span><span class="line" data-startTime="1880">paper at XRootD.org. </span> <span class="line" data-startTime="1884">It's LGPL open-source. </span> <span class="line" data-startTime="1886">You can download it, run it. </span> <span class="line" data-startTime="1888">It's managed by the XRootD </span><span class="line" data-startTime="1888">collaboration. </span> <span class="line" data-startTime="1890">That collaboration is </span><span class="line" data-startTime="1890">open to new members. </span> <span class="line" data-startTime="1893">And I do encourage you </span><span class="line" data-startTime="1893">to go to XRootD.org. </span> <span class="line" data-startTime="1899">So now Sergey, finish it up. </span> <span class="line" data-startTime="1905">DR. SERGEY PANITKIN: And </span><span class="line" data-startTime="1905">let me summarize. </span> <span class="line" data-startTime="1910">All in all, we had great </span><span class="line" data-startTime="1910">experience with </span><span class="line" data-startTime="1912">Google Compute Engine. </span> <span class="line" data-startTime="1914">We tested several computational </span><span class="line" data-startTime="1914">scenarios on </span><span class="line" data-startTime="1917">that platform, PROOF, XRootD </span><span class="line" data-startTime="1917">clusters, PanDA batch </span><span class="line" data-startTime="1921">clusters, and ran large scale </span><span class="line" data-startTime="1921">Monte Carlo production. </span></p>

<p><span class="line" data-startTime="1926">We think that Google Compute </span><span class="line" data-startTime="1926">Engine is modern cloud </span><span class="line" data-startTime="1930">infrastructure that can serve </span><span class="line" data-startTime="1930">as a stable high performance </span><span class="line" data-startTime="1933">platform for scientific </span><span class="line" data-startTime="1933">computing. </span> <span class="line" data-startTime="1936">And tools developed by the LHC </span><span class="line" data-startTime="1936">community may be of some </span><span class="line" data-startTime="1940">interest to the broader </span><span class="line" data-startTime="1940">community of developers </span><span class="line" data-startTime="1942">working on Google Compute </span><span class="line" data-startTime="1942">Engine and </span><span class="line" data-startTime="1944">other compute engine. </span> <span class="line" data-startTime="1947">And thank you very much. </span> <span class="line" data-startTime="1950">[APPLAUSE] </span></p>

<p class="speaker"><span class="line" data-starttime="1957"><span class="speakerName">Garrick Evans</span>: Thanks a lot. </span><span class="line" data-startTime="1959">So we're done, and we'd be happy </span><span class="line" data-startTime="1959">to take any questions </span><span class="line" data-startTime="1963">that you guys have </span><span class="line" data-startTime="1963">at the time. </span><span class="line" data-startTime="1965">Please come up to the mics. </span><span class="line" data-startTime="1972">No? </span><span class="line" data-startTime="1973">OK, well, thanks. </span><span class="line" data-startTime="1974">Hope you had a great I/O. </span></p>