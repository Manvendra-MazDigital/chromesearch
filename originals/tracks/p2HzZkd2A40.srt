1
00:00:00,610 --> 00:00:05,220
>>Justin Uberti: Hi, everyone. Thanks for
coming to the session on WebRTC for plugin-free

2
00:00:05,220 --> 00:00:11,309
realtime communication. I'm Justin Uberti,
tech lead for WebRTC at Google, and with me

3
00:00:11,309 --> 00:00:18,309
today is -- hey, has anyone seen Sam? -- Sam
Dutton, coming to you live from WebRTC on

4
00:00:21,820 --> 00:00:24,020
Chrome for Android.
[ Applause ]

5
00:00:24,020 --> 00:00:31,020
>>Sam Dutton: On a beautiful Nexus 7.
We got this slow res to cope with the WiFi

6
00:00:32,300 --> 00:00:37,440
here, but it seems to be working pretty well.
>>Justin Uberti: That was quite an entrance.

7
00:00:37,440 --> 00:00:42,060
Why don't you come up here and introduce yourself.
>>Sam Dutton: Yeah. Hey, I'm Sam Dutton. I'm

8
00:00:42,060 --> 00:00:47,890
a developer advocate for Chrome.
>>Justin Uberti: So we're here to talk to

9
00:00:47,890 --> 00:00:52,829
you today about the great things that WebRTC
has been working on and how you can use them.

10
00:00:52,829 --> 00:00:56,760
So what is WebRTC?
Well, in a nutshell, it's what we call realtime

11
00:00:56,760 --> 00:01:03,559
communication, RTC. The ability to communicate
live with somebody or something as if you

12
00:01:03,559 --> 00:01:10,559
were right there next to them. And this can
mean audio, video, or even just peer-to-peer

13
00:01:11,090 --> 00:01:16,869
data.
And we think WebRTC is really cool. But there's

14
00:01:16,869 --> 00:01:20,580
a lot of other people who are really excited
about WebRTC as well, and one of the reasons

15
00:01:20,580 --> 00:01:27,580
is that WebRTC fills a critical gap in the
web platform, where previously in a native

16
00:01:27,650 --> 00:01:32,619
proprietary app like Skype could do something
the web just couldn't. But now we've turned

17
00:01:32,619 --> 00:01:38,030
that around and changed that so we have a
web of connected WebRTC devices that can communicate

18
00:01:38,030 --> 00:01:45,030
in real time just by loading a Web page.
So here's what we're trying to do with WebRTC.

19
00:01:45,770 --> 00:01:50,470
To build the key APIs for realtime communication
into the web.

20
00:01:50,470 --> 00:01:55,740
To make an amazing media stack in Chrome so
that developers can build great experiences.

21
00:01:55,740 --> 00:02:02,740
And to use this network of connected WebRTC
devices to create a new communications ecosystem.

22
00:02:04,439 --> 00:02:10,080
And these kind of seem like lofty goals, but
take this quote from the current CTO of the

23
00:02:10,080 --> 00:02:15,770
FCC, who says he sees traditional telephony
fading away as voice just becomes another

24
00:02:15,770 --> 00:02:21,970
Webapp.
So we're trying to live up to that promise,

25
00:02:21,970 --> 00:02:28,900
and right now you can build a single app with
WebRTC that connects Chrome, Chrome for Android,

26
00:02:28,900 --> 00:02:34,129
Firefox, and very soon Opera.
I'm especially excited to announce that as

27
00:02:34,129 --> 00:02:39,790
of this week Firefox 22 has gone to beta,
which is the very first WebRTC-enabled version

28
00:02:39,790 --> 00:02:45,540
of Firefox.
So within a matter of weeks, we will have

29
00:02:45,540 --> 00:02:52,540
over 1 billion users using a WebRTC-enabled
browser.

30
00:02:53,670 --> 00:02:57,640
[ Cheers and applause ]
>>Justin Uberti: And I think that just kind

31
00:02:57,640 --> 00:03:01,950
of gives like a good idea of the size of the
opportunity here.

32
00:03:01,950 --> 00:03:07,769
And we expect that number to grow very significantly
as both Chrome and Firefox get increased adoption.

33
00:03:07,769 --> 00:03:12,930
For places where we don't have WebRTC-enabled
browsers, we're providing native supported

34
00:03:12,930 --> 00:03:19,930
official toolkits on both Android and very
soon iOS that can interoperate with WebRTC

35
00:03:20,760 --> 00:03:24,330
in the browser.
[ Applause ]

36
00:03:24,330 --> 00:03:30,900
>>Justin Uberti: So here are just a handful
of the companies that see the opportunity

37
00:03:30,900 --> 00:03:37,790
in WebRTC and are building their business
around it.

38
00:03:37,790 --> 00:03:43,739
So that's the vision for WebRTC. Now let's
dig into the APIs.

39
00:03:43,739 --> 00:03:48,230
There are three main categories of API that
exist in WebRTC.

40
00:03:48,230 --> 00:03:55,230
First, getting access to input devices. Accessing
the microphone, accessing the webcam, getting

41
00:03:55,590 --> 00:04:01,190
a stream of media from either of them.
Secondly, being able to connect to another

42
00:04:01,190 --> 00:04:05,769
WebRTC endpoint across the Internet and to
send this audio and video in real time.

43
00:04:05,769 --> 00:04:11,299
And third, the ability to do this not just
for audio and video but for arbitrary application

44
00:04:11,299 --> 00:04:16,470
data. And we think this one is especially
interesting.

45
00:04:16,470 --> 00:04:20,889
So because of three categories, we have three
objects, three primary objects in WebRTC to

46
00:04:20,889 --> 00:04:24,520
access this stuff.
The first one, MediaStream, is for getting

47
00:04:24,520 --> 00:04:29,969
access to media, then RTCPeerConnection and
RTCDataChannel. And we'll get into each one

48
00:04:29,969 --> 00:04:34,050
of these individually.
Sam, why don't you tell us about MediaStream.

49
00:04:34,050 --> 00:04:41,050
>>Sam Dutton: Yeah. Sure. So MediaStream.
It represents like a single source of synchronized

50
00:04:41,289 --> 00:04:48,289
audio or video or both. Each media stream
contains one or more MediaStream tracks. You

51
00:04:48,969 --> 00:04:55,199
know, for example, on your -- like your laptop,
you've got a webcam and a microphone providing

52
00:04:55,199 --> 00:05:02,199
video and audio streams, and they're synchronized.
We get access to these local devices using

53
00:05:02,830 --> 00:05:08,930
the getUserMedia method of navigator, so we'll
just look at the code for that, just highlight

54
00:05:08,930 --> 00:05:11,430
that.
And you can see the getUserMedia there. It

55
00:05:11,430 --> 00:05:17,349
takes three parameters, three arguments there,
and the first one, if we look at the constraints

56
00:05:17,349 --> 00:05:22,300
argument I've got, you can see I'm just specifying
I want video. That's all I'm saying. "Just

57
00:05:22,300 --> 00:05:27,319
give me video, nothing else."
And then in the success callback, we're setting

58
00:05:27,319 --> 00:05:33,879
the source of the video using the stream that's
returned by getUserMedia.

59
00:05:33,879 --> 00:05:38,710
Now, let's see that in action.
A really simple example here.

60
00:05:38,710 --> 00:05:45,710
And you can see when we fire the getUserMedia
method, we get the allow permissions bar at

61
00:05:46,550 --> 00:05:49,559
the top there.
Now, this means that users have to explicitly

62
00:05:49,559 --> 00:05:52,729
opt in to allowing access to their microphone
and camera.

63
00:05:52,729 --> 00:05:58,219
And yeah, there we have it. Using that code,
we've got video displayed in a video element.

64
00:05:58,219 --> 00:06:01,889
Great.
What really excites me about these APIs, you

65
00:06:01,889 --> 00:06:06,819
know, is when they kind of come up against
each other. Like in this example, what's happening

66
00:06:06,819 --> 00:06:12,710
is that we've got getUserMedia being piped
into a Canvas element, and then the Canvas

67
00:06:12,710 --> 00:06:19,429
element being analyzed and then producing
ASCII, just like that. That might be a good

68
00:06:19,429 --> 00:06:20,089
codec.
>>Justin Uberti: Yeah. That would be a good

69
00:06:20,089 --> 00:06:23,270
codec. You can just compress it. You can use
gzip.

70
00:06:23,270 --> 00:06:25,619
>>Sam Dutton: Yeah. Smaller font sizes, higher
resolution.

71
00:06:25,619 --> 00:06:29,399
>>Justin Uberti: Yeah.
>>Sam Dutton: Also another example is from

72
00:06:29,399 --> 00:06:35,729
Facekat. Now what's happening here is that
it's using the head tracker JavaScript library

73
00:06:35,729 --> 00:06:42,020
to track the position of my head. And when
I move around, you can see I'm moving through

74
00:06:42,020 --> 00:06:49,020
the game and trying to stay alive, which is
quite difficult. This is painful. Anyway...

75
00:06:51,069 --> 00:06:56,289
[ Laughter ]
>>Sam Dutton: Whoa! Okay. I think I've flipped

76
00:06:56,289 --> 00:07:00,619
into hyperspace there.
And an old favorite you may well have seen,

77
00:07:00,619 --> 00:07:07,580
Webcam Toy which gives us access to the camera,
kind of a photo booth app, uses WebGL to create

78
00:07:07,580 --> 00:07:13,770
a bunch of slightly psychedelic effects there,
quite like this old movie one. So I'll take

79
00:07:13,770 --> 00:07:20,770
that and, yeah, get a snapshot. And I can
share that with my friends. So beautiful work

80
00:07:21,379 --> 00:07:27,189
from Paul (saying name) there.
Now, you might remember I said that we could

81
00:07:27,189 --> 00:07:30,659
use the constraints object. The simple example
there was just saying, you know, "Use the

82
00:07:30,659 --> 00:07:33,580
video, nothing else."
Well, we can do more interesting things with

83
00:07:33,580 --> 00:07:38,899
constraints than that. We can do stuff like
specify the resolution or the frame rate,

84
00:07:38,899 --> 00:07:42,550
a whole stack of things that we want from
our local devices.

85
00:07:42,550 --> 00:07:48,909
A little example from that. If we go over
here, now this is with the code, actually.

86
00:07:48,909 --> 00:07:54,059
If we go to the DevTools there, you can see
that I've got three different constraints

87
00:07:54,059 --> 00:08:00,409
objects, one for each resolution.
So when I press the buttons, I use the QVGA

88
00:08:00,409 --> 00:08:05,879
constraints, getUserMedia, and then with the
VGA one, I'm getting high-resolution, and

89
00:08:05,879 --> 00:08:12,879
for HD, I'm getting the full 1280 by 720.
We can also use getUserMedia now for input

90
00:08:16,069 --> 00:08:20,379
from our microphone.
In other words, we can use getUserMedia to

91
00:08:20,379 --> 00:08:26,149
provide a source node for Web Audio, and there's
a huge amount of interesting stuff we can

92
00:08:26,149 --> 00:08:30,439
do without processing audio using web audio
from the mic or wherever.

93
00:08:30,439 --> 00:08:36,180
A little example of that here. I'll just allow
access to the mic and you can see I'm getting

94
00:08:36,180 --> 00:08:41,579
a nice little visualization there in the Canvas
element, and I can start to record this, blah,

95
00:08:41,579 --> 00:08:45,360
blah, blah, blah, blah, blah, and then --
>>> To record this, blah, blah, blah, blah,

96
00:08:45,360 --> 00:08:47,649
blah.
>>Sam Dutton: And, yeah, you can see that's

97
00:08:47,649 --> 00:08:52,970
-- you record a JS to save that locally to
disk.

98
00:08:52,970 --> 00:08:58,660
GetUserMedia also now -- this is kind of experimental,
but we can use getUserMedia to get a screen

99
00:08:58,660 --> 00:09:05,420
capture. In other words, data coming directly
from what we see on screen, not from the audio/video

100
00:09:05,420 --> 00:09:09,769
from the mic and the camera.
Probably the simplest if I show an example

101
00:09:09,769 --> 00:09:13,740
of this.
So, yeah, a little application here and when

102
00:09:13,740 --> 00:09:20,740
I click to make the call, allow, and you can
see there that I get this kind of crazy hall

103
00:09:22,350 --> 00:09:27,519
of mirrors effect because, you know, I'm capturing
the screen that I'm capturing and so on and

104
00:09:27,519 --> 00:09:32,300
so on.
Now, that's quite nice, but it would be really

105
00:09:32,300 --> 00:09:39,300
useful if we could take that screen capture
and then, you know, transmit that to another

106
00:09:39,370 --> 00:09:45,470
computer. And for that, we have RTCPeerConnection.
>>Justin Uberti: Thanks, Sam.

107
00:09:45,470 --> 00:09:50,970
So as the name implies, RTCPeerConnection
is all about making a connection to another

108
00:09:50,970 --> 00:09:56,329
peer. And over this peer connection, we can
actually then go and send audio and video.

109
00:09:56,329 --> 00:10:00,779
And the way we do this is we take the media
streams that we've gotten from getUserMedia

110
00:10:00,779 --> 00:10:06,060
and we plug them into the peer connection
and send them off to the other side.

111
00:10:06,060 --> 00:10:10,120
When the other side receives them, they'll
pop out as a new media stream on their peer

112
00:10:10,120 --> 00:10:13,689
connection and they can then plug that into
a video element to display on the page. And

113
00:10:13,689 --> 00:10:18,350
so both sides have a peer connection, they
both get streams from getUserMedia, they plug

114
00:10:18,350 --> 00:10:22,670
them in and then those media streams pop out
magically encoded and decoded on the other

115
00:10:22,670 --> 00:10:26,720
side.
Now, under the hood, peer connection is doing

116
00:10:26,720 --> 00:10:32,639
a ton of stuff. Signal processing to remove
noise from audio and video. Codec selection

117
00:10:32,639 --> 00:10:37,220
and compression and decompression of the actual
audio and video. Finding the actually peer-to-peer

118
00:10:37,220 --> 00:10:42,519
route, through firewalls, through NATs, through
relays. Encrypting the data so that the user's

119
00:10:42,519 --> 00:10:46,779
data is fully protected at all times. And
then actually managing the bandwidth so that

120
00:10:46,779 --> 00:10:52,029
if you have 2 megabits, we use it, if you
have 200 kilobits, that's all we use. But

121
00:10:52,029 --> 00:10:59,029
we do everything we can to hide this complexity
from that web developer. And so the main thing

122
00:10:59,360 --> 00:11:04,439
is that you get your media streams, you plug
them in via add stream to peer connection,

123
00:11:04,439 --> 00:11:08,269
and off you go. And here's a real example
of that this.

124
00:11:08,269 --> 00:11:14,779
>>Sam Dutton: Yeah. So you can see here that
we've created a new RTCPeerConnection, and

125
00:11:14,779 --> 00:11:21,779
then when the stream is received, the callback
for that in GotRemoteStream there attaches

126
00:11:22,120 --> 00:11:25,730
the media we're getting from a video element
to the stream.

127
00:11:25,730 --> 00:11:30,800
Now, at the same time, we're also creating
what's called an offer, giving information

128
00:11:30,800 --> 00:11:35,949
about media and we're setting that as the
local description, and then sending that to

129
00:11:35,949 --> 00:11:40,339
the callee, so that they can set the remote
description. You can see that in the got answer

130
00:11:40,339 --> 00:11:44,120
function there.
Let's have a little look at RTCPeerConnection

131
00:11:44,120 --> 00:11:51,120
on one page. A very simple example here.
So what we've got here is getUserMedia here.

132
00:11:52,439 --> 00:11:59,129
Just start that up. So it's getting video
from the local camera here, displaying it

133
00:11:59,129 --> 00:12:05,180
on the left there.
Now, when I press "call," it's using RTCPeerConnection

134
00:12:05,180 --> 00:12:11,149
to communicate that video to the other -- yeah,
the other video element on the page there.

135
00:12:11,149 --> 00:12:14,740
This is a great place to start to get your
head around RTCPeerConnection.

136
00:12:14,740 --> 00:12:19,839
And if we look in the code there, you can
see that, you know, it's really simple. There's

137
00:12:19,839 --> 00:12:25,230
not a lot of code there to do that, to transmit
video from one peer to another.

138
00:12:25,230 --> 00:12:30,499
>>Justin Uberti: So that's very cool stuff.
You know, a full video chat client in a single

139
00:12:30,499 --> 00:12:34,449
Web page in just about 15 lines of JavaScript.
And we talked a little bit quickly through

140
00:12:34,449 --> 00:12:38,170
the whole thing around how we set up the parameters
for the call, the offers and answers, but

141
00:12:38,170 --> 00:12:42,120
I'll come back to that later.
The next thing I want to talk about is RTCDataChannel,

142
00:12:42,120 --> 00:12:46,579
and this says "If we have a peer connection
which already creates our peer-to-peer link

143
00:12:46,579 --> 00:12:49,639
for us, can we send arbitrary application
data over it?"

144
00:12:49,639 --> 00:12:51,769
And this is the mechanism that we use to do
so.

145
00:12:51,769 --> 00:12:55,589
Now, one example where we would do this would
be in a game. Like take this game. I think

146
00:12:55,589 --> 00:13:01,180
it's called Jank Wars or something. And we
have all these ships floating around the screen.

147
00:13:01,180 --> 00:13:05,069
Now, when a ship moves, we want to make sure
that's communicated to the other player as

148
00:13:05,069 --> 00:13:11,189
quickly as possible, and so we have this little
json object that kind of contains the parameters

149
00:13:11,189 --> 00:13:16,389
and the position and the velocity of the ships,
and we can just take that object and stuff

150
00:13:16,389 --> 00:13:20,029
it into the send method and it will shoot
it across to the other side where it pops

151
00:13:20,029 --> 00:13:23,610
out as on message.
And the other side can do the same thing.

152
00:13:23,610 --> 00:13:28,310
It can call send on its DataChannel and it
works pretty much like a WebSocket. That's

153
00:13:28,310 --> 00:13:32,839
not an accident. You know, we tried to design
it that way so that people familiar with using

154
00:13:32,839 --> 00:13:38,920
WebSockets could also use a similar API for
RTCDataChannel. And the benefit is that here

155
00:13:38,920 --> 00:13:44,600
we have a peer-to-peer connection for the
lowest possible latency for doing this communication.

156
00:13:44,600 --> 00:13:50,029
In addition, RTCDataChannel can be either
unreliable or reliable, and we can think about

157
00:13:50,029 --> 00:13:54,970
this kind of like UDP versus TCP.
If you're doing a game, it's more important

158
00:13:54,970 --> 00:13:58,749
that your packets get there quickly, than
they're guaranteed to get there. Whereas if

159
00:13:58,749 --> 00:14:04,069
you're doing a file transfer, the files are
only any good if the entire file is delivered.

160
00:14:04,069 --> 00:14:08,610
So you can choose this, as the app developer,
which method you want to use, either unreliable

161
00:14:08,610 --> 00:14:13,240
or reliable.
And lastly, everything is fully secure. We

162
00:14:13,240 --> 00:14:18,379
use standard DTLS encryption to make sure
that the packages you sent across the DataChannel

163
00:14:18,379 --> 00:14:23,339
are fully encrypted on their way to the destination.
And you can do this either with audio and

164
00:14:23,339 --> 00:14:28,149
video, or if you want to make a peer connection
for just data, you can do that as well.

165
00:14:28,149 --> 00:14:31,149
So Sam's going to show us how this actually
works.

166
00:14:31,149 --> 00:14:35,910
>>Sam Dutton: Yeah. So again, another really
simple example. We're creating a peer connection

167
00:14:35,910 --> 00:14:42,399
here, and once the DataChannel is received,
in the callback to that we're setting the

168
00:14:42,399 --> 00:14:49,399
receive channel using the event.channel object.
Now, when the receive channel gets a message,

169
00:14:49,480 --> 00:14:56,480
kind of like WebSocket, really, we're just
putting some text in a local div there using

170
00:14:56,649 --> 00:15:00,759
event.data.
Now, the send channel is created with create

171
00:15:00,759 --> 00:15:06,120
DataChannel, and then we've got a send button
and when that's clicked, we get the data from

172
00:15:06,120 --> 00:15:10,319
a text area, and we use the send channel to
send that to the other peer.

173
00:15:10,319 --> 00:15:15,220
Again, let's see this in action.
This is, again, a good place to start, a one-page

174
00:15:15,220 --> 00:15:22,220
demo with all the code for RTCDataChannel,
so type in some text, and we hit "send," and

175
00:15:22,529 --> 00:15:27,470
it's transmitting it to the other text area.
A great place to start if you're looking at

176
00:15:27,470 --> 00:15:32,860
RTCDataChannel.
Something a little more useful here, a great

177
00:15:32,860 --> 00:15:39,699
app from sort of Sharefest. Now, Sharefest
is using RTCDataChannel to enable us to do

178
00:15:39,699 --> 00:15:46,339
file sharing. I'm going to select a nice photo
here I've got of some cherries, and it's popeye

179
00:15:46,339 --> 00:15:50,529
is the URL.
Now, Justin is going to try and get that up

180
00:15:50,529 --> 00:15:54,110
on the screen on his side, just to check that
that's gone through.

181
00:15:54,110 --> 00:16:00,100
So like I say, this is doing file sharing
using RTCDataChannel and there's a huge amount

182
00:16:00,100 --> 00:16:02,629
of potential. There we go. There's the --
>>Justin Uberti: I love cherries.

183
00:16:02,629 --> 00:16:06,449
>>Sam Dutton: Good. These are beautiful Mountain
View cherries, actually. They're really, really

184
00:16:06,449 --> 00:16:09,110
nice.
>>Justin Uberti: All this data is being sent

185
00:16:09,110 --> 00:16:13,480
peer to peer, and anybody else who connects
to the same URL will download that data peer

186
00:16:13,480 --> 00:16:18,709
to peer from Sam's machine. And so none of
this has to touch Sharefest servers. And I

187
00:16:18,709 --> 00:16:25,709
think that's pretty interesting, if you think
about things like file transfer, bulk video

188
00:16:27,769 --> 00:16:33,309
distribution.
Okay. So we talked a lot about how we can

189
00:16:33,309 --> 00:16:39,869
do really clever peer-to-peer stuff with RTCPeerConnection,
but it turns out we need servers to kind of

190
00:16:39,869 --> 00:16:43,670
get the process kicked off.
And the first part of it is actually making

191
00:16:43,670 --> 00:16:49,019
sure that both sides can agree to actually
conduct the session. And this is the process

192
00:16:49,019 --> 00:16:54,610
that we call "signaling."
The signaling in WebRTC is abstract, which

193
00:16:54,610 --> 00:17:00,199
means that there's no fully defined protocol
exactly how you do it. The key part is that

194
00:17:00,199 --> 00:17:03,949
you just have to exchange session description
objects, and if you think about this kind

195
00:17:03,949 --> 00:17:08,250
of like a telephone call, when you make a
call to someone, the telephone network sends

196
00:17:08,250 --> 00:17:12,169
a message to the person you're calling telling
them there's an incoming call and the phone

197
00:17:12,169 --> 00:17:15,120
should ring.
Then when they answer the call, they send

198
00:17:15,120 --> 00:17:20,779
a message back that says "The call is now
active." Now, these messages also contain

199
00:17:20,779 --> 00:17:24,880
parameters around like what media format to
use, where the person is on the network. And

200
00:17:24,880 --> 00:17:28,470
the same is true for WebRTC.
And these things, these session description

201
00:17:28,470 --> 00:17:34,679
objects, contain parameters like what codecs
to use, what security keys to use, the network

202
00:17:34,679 --> 00:17:38,570
information for setting up the peer-to-peer
route. And the only important thing is that

203
00:17:38,570 --> 00:17:42,880
you just send it from your side to the other
side and vice versa. You can use any mechanism

204
00:17:42,880 --> 00:17:49,399
you want. WebSockets, Google cloud messaging,
XHR. You can use any protocol. You can just

205
00:17:49,399 --> 00:17:54,830
send it as json or you can use a standard
protocol like SIP or XMTP. And here's a picture

206
00:17:54,830 --> 00:17:59,440
of how this all works.
The app gets a session description from the

207
00:17:59,440 --> 00:18:04,250
browser and sends it across through the cloud
to the other side. Once it gets the message

208
00:18:04,250 --> 00:18:09,059
back from the other side with the other side's
session description, and both session descriptions

209
00:18:09,059 --> 00:18:14,649
are passed down to WebRTC in the browser,
WebRTC can then set up and conduct the media

210
00:18:14,649 --> 00:18:20,720
link peer to peer.
So we do a lot to try to hide the details

211
00:18:20,720 --> 00:18:24,659
of what's inside the RTC session description
because this includes a whole bunch of parameters.

212
00:18:24,659 --> 00:18:29,270
As I said, codecs, network information, all
sorts of stuff. This is just a snippet of

213
00:18:29,270 --> 00:18:32,340
what's contained inside a session description
right now.

214
00:18:32,340 --> 00:18:36,409
Really advanced apps can do complex behaviors
by modifying this, but we've designed the

215
00:18:36,409 --> 00:18:43,409
API so that regular apps just don't have to
think about it.

216
00:18:43,809 --> 00:18:48,529
The other thing that we need service for is
to actually get the peer-to-peer session fully

217
00:18:48,529 --> 00:18:55,529
routed, and in the old days, this wouldn't
be a problem. A long time ago, each side had

218
00:18:57,600 --> 00:19:01,720
a public IP address, they send each other's
IP address to each other through the cloud,

219
00:19:01,720 --> 00:19:05,340
and we make the link directly between the
peers.

220
00:19:05,340 --> 00:19:12,100
Well, in the age of NAT, things are more complicated.
NATs hand out what's called a private IP address,

221
00:19:12,100 --> 00:19:16,299
and these IP addresses are not useful for
communication. There's no way we can make

222
00:19:16,299 --> 00:19:21,440
the link actually peer-to-peer unless we have
a public address.

223
00:19:21,440 --> 00:19:27,110
So this is where we bring a technology called
STUN. The STUN server, we contact from WebRTC,

224
00:19:27,110 --> 00:19:31,630
and we say, "What's my public IP address?"
And basically the request comes in to the

225
00:19:31,630 --> 00:19:36,000
STUN server, it sees the address that that
request came from, puts that address into

226
00:19:36,000 --> 00:19:41,149
the packet and sends it back, so now WebRTC
knows its public IP address and the STUN server

227
00:19:41,149 --> 00:19:44,750
doesn't have to be in the party anymore. It
doesn't have to have media flowing through

228
00:19:44,750 --> 00:19:48,740
it.
So here if we look at this example, each side

229
00:19:48,740 --> 00:19:53,309
is contacted its STUN server to find out what
its public I.P. address is and then sent the

230
00:19:53,309 --> 00:19:59,419
traffic to the other I.P. address through
its NAT and the data still flows peer-to-peer.

231
00:19:59,419 --> 00:20:03,480
So this is kind of magic stuff, and it usually
works.

232
00:20:03,480 --> 00:20:07,610
Usually, we can make sure that, you know,
the data all flows properly peer-to-peer but

233
00:20:07,610 --> 00:20:11,240
not in every case.
And for that, we have a technology called

234
00:20:11,240 --> 00:20:18,240
TURN built into RTC. This turns things around
and provides a cloud fallback when a peer-to-peer

235
00:20:19,210 --> 00:20:24,200
link is impossible. You basically ask for
a relay in the cloud saying, Give me a public

236
00:20:24,200 --> 00:20:28,409
address and because this public address is
in the cloud, anybody can contact it which

237
00:20:28,409 --> 00:20:32,730
means the call always sets up, even if you
are behind a restrictive NAT or even behind

238
00:20:32,730 --> 00:20:36,510
a proxy.
The downside is since the data is actually

239
00:20:36,510 --> 00:20:40,429
being relayed through the server, there is
an operational cost to it, but it does mean

240
00:20:40,429 --> 00:20:45,750
the call works in almost all environments.
Now, on one hand, we have STUN which is super

241
00:20:45,750 --> 00:20:51,309
cheap but doesn't always work. And we have
TURN which always works but has some costs

242
00:20:51,309 --> 00:20:53,610
to it. How do we make sure we get the best
of both worlds?

243
00:20:53,610 --> 00:21:00,610
Here is TURN in action where we tried to use
STUN and STUN didn't work. We couldn't get

244
00:21:01,830 --> 00:21:05,380
the things to actually penetrate the NATs.
So instead we fell back and only then did

245
00:21:05,380 --> 00:21:10,940
we use TURN and sent the media from one peer
through the NAT, through the TURN server and

246
00:21:10,940 --> 00:21:17,940
to the other side. This is all done by technology
called ICE. ICE knows about STUN and TURN

247
00:21:18,370 --> 00:21:23,010
and tries all the things in parallel to figure
out the best path for the call. If it can

248
00:21:23,010 --> 00:21:28,230
do STUN, it does STUN. If it can do TURN,
well, then it will fall to TURN but it will

249
00:21:28,230 --> 00:21:31,340
do so quickly.
We have stats from a deploy WebRTC application

250
00:21:31,340 --> 00:21:37,870
which says 86% of the time we can make things
work with just STUN. So only one out of seven

251
00:21:37,870 --> 00:21:41,799
calls actually have to run through a TURN
server.

252
00:21:41,799 --> 00:21:48,600
How do you deploy TURN for your application?
We have some testing servers, testing STUN

253
00:21:48,600 --> 00:21:52,549
server that you can use. Plus, we make source
code available for our own STUN and TURN servers

254
00:21:52,549 --> 00:21:56,620
as part of the WebRTC code package.
The ting I would really recommend is the long

255
00:21:56,620 --> 00:22:03,620
name but really good product rfc5766-turn-server,
which has Amazon VM images that you can just

256
00:22:03,799 --> 00:22:08,010
take, download, and deploy into the cloud
and you have got your TURN server provision

257
00:22:08,010 --> 00:22:12,760
for all your users right there. You can also
recommend restund, another TURN server that

258
00:22:12,760 --> 00:22:19,760
we have used with excellent results.
One question that comes up around WebRTC is:

259
00:22:19,789 --> 00:22:25,750
How is security handled? And the great thing
is that security has been built into WebRTC

260
00:22:25,750 --> 00:22:29,279
from the very beginning. And so this means
several different things.

261
00:22:29,279 --> 00:22:34,919
It means we have mandatory encryption for
both media and data. So all the data that's

262
00:22:34,919 --> 00:22:40,360
being sent by WebRTC is being encrypted using
standard AES encryption.

263
00:22:40,360 --> 00:22:44,169
We also have secure UI. I mean, the user's
camera and microphone can only be accessed

264
00:22:44,169 --> 00:22:47,360
if they have explicitly opt in to making that
functionality available.

265
00:22:47,360 --> 00:22:53,399
And, last, WebRTC runs inside the Chrome Sandbox.
So even if somebody tries to attack WebRTC

266
00:22:53,399 --> 00:22:59,029
inside of Chrome, the browser and the user
will be fully protected.

267
00:22:59,029 --> 00:23:02,970
So here's what you need to do to take advantage
of the security in WebRTC, is really simple.

268
00:23:02,970 --> 00:23:08,019
Your app just needs to use HTTPS for actually
doing the signaling. As long as the signaling

269
00:23:08,019 --> 00:23:12,350
goes over a security conduit, the data will
be fully secured as well using the standard

270
00:23:12,350 --> 00:23:19,350
protocols of SRTP for media or datagram TLS
for the DataChannel.

271
00:23:21,500 --> 00:23:25,950
One more question that comes up is around
making a multi-party call, a conference call.

272
00:23:25,950 --> 00:23:30,580
How should I architect my application?
In the simple two-party case, it is really

273
00:23:30,580 --> 00:23:34,580
simple -- it is easy. We just have a peer-to-peer
link. But as we start adding more peers into

274
00:23:34,580 --> 00:23:39,799
the mix, things get a bit more complicated
and one approach that people use is a mesh

275
00:23:39,799 --> 00:23:44,850
where basically every peer connects to every
other peer. And this is really simple because

276
00:23:44,850 --> 00:23:50,549
there's no servers or anything involved other
than the signaling stuff. But every peer has

277
00:23:50,549 --> 00:23:55,480
to send a copy of its data to every other
peer. So this has a corresponding CPU and

278
00:23:55,480 --> 00:23:59,760
bandwidth cost. So depending on the media
you are trying to send, for audio, it can

279
00:23:59,760 --> 00:24:03,039
be kind of higher, for video, it is going
to be less, the number of peers you can support

280
00:24:03,039 --> 00:24:07,250
in this topology, you know, is fairly limited
especially if one of the peers is on a mobile

281
00:24:07,250 --> 00:24:11,330
device.
To deal with that, another architecture that

282
00:24:11,330 --> 00:24:16,590
can be used is the star architecture. And
here you can pick the most capable device

283
00:24:16,590 --> 00:24:20,450
to be what we call the focus for the call.
And the focus is the part that's actually

284
00:24:20,450 --> 00:24:26,110
responsible for taking the data and sending
a copy to each of the other end points.

285
00:24:26,110 --> 00:24:30,950
But as we get to handing multiple HD video
streams, the job for a focus becomes pretty

286
00:24:30,950 --> 00:24:37,950
difficult. And so for the most robust conferencing
architecture, we recommend an MCU, our multi-point

287
00:24:38,639 --> 00:24:44,340
control unit. And this is a server that's
custom made for relaying large amounts of

288
00:24:44,340 --> 00:24:49,330
audio and video. It can do various things.
It can do -- select a stream forwarding. It

289
00:24:49,330 --> 00:24:53,260
can actually mix the audio or video data.
It can also do things like recording.

290
00:24:53,260 --> 00:24:57,880
And so if one peer drops out, it doesn't interrupt
the whole conference because the MCU is taking

291
00:24:57,880 --> 00:25:04,880
care of everything.
So WebRTC is made with standards in mind.

292
00:25:05,500 --> 00:25:10,010
So you connect with things that aren't even
WebRTC devices. And one thing that people

293
00:25:10,010 --> 00:25:14,830
might want to talk to you from WebRTC is phones.
There is a bunch of easy things you can drop

294
00:25:14,830 --> 00:25:19,980
into your Web page to make this happen. There
is sipML5, which is a way to talk to various

295
00:25:19,980 --> 00:25:25,789
standard SIP devices, Photo, and what we are
going to show you now, a widget from Zingaya

296
00:25:25,789 --> 00:25:32,789
to make a phone call.
>>Sam Dutton: Okay. So we got a special guest

297
00:25:34,539 --> 00:25:39,620
joining us a little bit later in the presentation.
I just want to give him a call to see if he's

298
00:25:39,620 --> 00:25:45,970
available. So let's use the Zingaya WebRTC
phone app now and you can see it's asking

299
00:25:45,970 --> 00:25:51,399
for access to my microphone. Calling someone.
I hope it is the person I want. We'll see

300
00:25:51,399 --> 00:25:53,190
if he's there.
>>> Hello?

301
00:25:53,190 --> 00:26:00,190
>>Sam Dutton: Hey. Is that you, Chris?
>>> Hey, Sam, how's it going? It is.

302
00:26:02,889 --> 00:26:09,130
>>Sam Dutton: Fantastic. I just wanted to
check if you are ready for your gig later

303
00:26:09,130 --> 00:26:11,840
on?
>>> I'm ready whenever you are.

304
00:26:11,840 --> 00:26:15,630
>>Sam Dutton: That's fantastic. Okay. Speak
to you soon, Chris. Thanks. Bye-bye.

305
00:26:15,630 --> 00:26:18,639
>>> Talk to you soon. Bye.
>>Sam Dutton: Cheers.

306
00:26:18,639 --> 00:26:25,639
>>Justin Uberti: No plug-ins, realtime communication.
>>Sam Dutton: Yeah, well, that situation,

307
00:26:29,010 --> 00:26:31,950
we had a guy with a telephone.
You know, something we were thinking about

308
00:26:31,950 --> 00:26:38,950
is situations where there is no telephone
network. Now, Voxio demonstrated this with

309
00:26:39,460 --> 00:26:46,230
something called Tethr, which is disaster
communications in a box. Uses the open VGS

310
00:26:46,230 --> 00:26:51,549
cell framework. You can see it is that little
box there to enable calls between feature

311
00:26:51,549 --> 00:26:57,710
phones via the open PTS cell through WebRTC
to computers.

312
00:26:57,710 --> 00:27:01,970
You can imagine this is kind of fun to get
a license for this in downtown San Francisco,

313
00:27:01,970 --> 00:27:08,970
but this is incredibly useful in situations
where there is no infrastructure. Yeah, this

314
00:27:10,139 --> 00:27:13,549
is like telephony without a carrier. Just
amazing.

315
00:27:13,549 --> 00:27:18,510
>>Justin Uberti: So we have a codelab this
afternoon that I hope you can come to where

316
00:27:18,510 --> 00:27:22,659
we will really go into the details of exactly
how to build a WebRTC application.

317
00:27:22,659 --> 00:27:27,429
But now we are going to talk about some resources
that I think are really useful.

318
00:27:27,429 --> 00:27:33,720
The first one is something called WebRTC internals.
And this is a page you can open up just by

319
00:27:33,720 --> 00:27:37,220
going to this URL while you are in a WebRTC
call. And it will show all sorts of great

320
00:27:37,220 --> 00:27:42,950
statistics about what's actually happening
inside your call. You know, this will be things

321
00:27:42,950 --> 00:27:49,950
like packet loss, bandwidth, video resolution,
and sizes and there is also a full log of

322
00:27:50,389 --> 00:27:54,649
all the calls made to the WebRTC API that
you can download and export.

323
00:27:54,649 --> 00:27:58,889
So if a customer is reporting problems with
their call, you can easily get this debugging

324
00:27:58,889 --> 00:28:04,480
information from them.
Another thing is the WebRTC spec has been

325
00:28:04,480 --> 00:28:09,620
updating fairly rapidly. And so in a given
browser, the API might not always match the

326
00:28:09,620 --> 00:28:14,840
latest spec. Adapter.js is something that's
there to instantly update the Web developer

327
00:28:14,840 --> 00:28:18,690
with the differences between browsers and
the differences between versions.

328
00:28:18,690 --> 00:28:24,659
So we make sure that adapter.js always implements
the latest spec and it thunks down to whatever

329
00:28:24,659 --> 00:28:30,559
the version supports. So as new APIs are added,
we polyfill them to make sure you don't have

330
00:28:30,559 --> 00:28:34,080
to write custom version code or custom browser
code for each browser. And we use this in

331
00:28:34,080 --> 00:28:39,679
our own applications.
>>Sam Dutton: Okay. If all this is too much

332
00:28:39,679 --> 00:28:45,679
for you, good news is we got some fantastic
JavaScript frameworks come up in the last

333
00:28:45,679 --> 00:28:50,769
few months, really great abstraction libraries
to make it really, really simple to build

334
00:28:50,769 --> 00:28:56,100
WebRTC apps, just with a few lines of code.
Example here from SimpleWebRTC, a little bit

335
00:28:56,100 --> 00:29:01,950
of JavaScript there to specify a video element
that represents local video and one that represents

336
00:29:01,950 --> 00:29:06,980
the remote video stream coming in. And then
join a room just by calling the join room

337
00:29:06,980 --> 00:29:13,980
method with a room name. Really, really simple.
Peerjs does something similar for RTC DataChannel.

338
00:29:14,090 --> 00:29:18,929
Create a peer and then on connection, you
can send messages and receive messages. So

339
00:29:18,929 --> 00:29:24,059
really, really easy to use.
>>Justin Uberti: So, JavaScript frameworks

340
00:29:24,059 --> 00:29:28,809
go a long way. They don't cover the production
aspects of the service, the signaling, the

341
00:29:28,809 --> 00:29:35,809
STUN and TURN servers we talked about. Fortunately,
we have things from Opentok and vline which

342
00:29:36,130 --> 00:29:42,870
are turn-key RTC services and they handle
all this stuff for you. You get an API key,

343
00:29:42,870 --> 00:29:46,460
and then you can make calls using their production
infrastructure, which is spread throughout

344
00:29:46,460 --> 00:29:50,519
the entire globe.
They also make UI widgets that can be easily

345
00:29:50,519 --> 00:29:57,330
dropped into your RTC app. So you can get
up and running with WebRTC super fast.

346
00:29:57,330 --> 00:30:03,399
Now we have got a special treat for you today.
Chris Wilson, a colleague of ours a developer

347
00:30:03,399 --> 00:30:08,139
on the original Mosaic browser and an occasional
musician as well, is going to be joining us

348
00:30:08,139 --> 00:30:14,779
courtesy of WebRTC to show off the HD video
quality and full-band audio quality that we

349
00:30:14,779 --> 00:30:17,529
are now able to offer in the latest version
of Chrome.

350
00:30:17,529 --> 00:30:20,000
Take it away, Chris.
>>> Hey, guys.

351
00:30:20,000 --> 00:30:24,370
>>Sam Dutton: Hey, Chris, how's it going?
>>> I'm good. How are you?

352
00:30:24,370 --> 00:30:28,600
>>Sam Dutton: Yeah, good, good. Have you got
some kind of musical instrument with you?

353
00:30:28,600 --> 00:30:33,019
>>> I do. Originally, you asked me for a face-melting
guitar solo.

354
00:30:33,019 --> 00:30:36,250
>>Sam Dutton: Yeah, yeah.
>>> I'm a little more relaxed now. I/O is

355
00:30:36,250 --> 00:30:39,889
starting to wind down. You can tell I have
already got my Hawaiian shirt on and I'm ready

356
00:30:39,889 --> 00:30:45,009
for some vacation. So I figured I would bring
my ukulele and hook it up through a nice microphone

357
00:30:45,009 --> 00:30:52,009
here so we can listen to how it sounds.
>>Sam Dutton: Take it away. Melt my face,

358
00:30:54,470 --> 00:30:55,679
Chris.
[ Laughter ]

359
00:30:55,679 --> 00:30:58,830
[ Music ]
>>Sam Dutton: He's pretty good.

360
00:30:58,830 --> 00:31:05,830
>>Justin Uberti: He is pretty good.
>>Sam Dutton: All right. That was beautiful.

361
00:31:11,659 --> 00:31:13,220
Thank you, Chris.
[ Applause ]

362
00:31:13,220 --> 00:31:17,399
>>Justin Uberti: Chris Wilson, everybody.
>>Sam Dutton: The audience is going crazy,

363
00:31:17,399 --> 00:31:20,700
Chris. Thank you very much.
So we have had a fraction of 30 minutes to

364
00:31:20,700 --> 00:31:24,539
cover a really big topic.
There is a great, a lot more information out

365
00:31:24,539 --> 00:31:30,070
there online. Some good stuff on html5rocks
and a really good e-book, too, if you want

366
00:31:30,070 --> 00:31:36,259
to take a look at that. There are several
ways to contact us. There is a great Google

367
00:31:36,259 --> 00:31:42,730
group, discuss WebRTC. You can post your technical
questions. All the kind of new news for WebRTC

368
00:31:42,730 --> 00:31:48,370
comes through on Google+ and Twitter stream.
And, you know, we're really grateful to all

369
00:31:48,370 --> 00:31:54,789
the people, all of you, who have submitted
feature requests and bugs and please keep

370
00:31:54,789 --> 00:32:00,210
them coming. And the URL for that is crbug.com/new.
So thank you for that.

371
00:32:00,210 --> 00:32:07,210
[ Applause ]
>>Justin Uberti: So -- and so we've built

372
00:32:07,419 --> 00:32:12,789
this stuff into the Web platform to make realtime
communication accessible to everyone. And

373
00:32:12,789 --> 00:32:18,019
we're super excited because we can't wait
to see what you all are going to build. So

374
00:32:18,019 --> 00:32:23,090
thank you for coming.
Once again, the slide link. And now if you

375
00:32:23,090 --> 00:32:26,029
have any questions, we will be happy to try
to answer them. Thank you very much.

376
00:32:26,029 --> 00:32:28,460
>>Sam Dutton: Thank you.
[ Applause ]

377
00:32:28,460 --> 00:32:35,460
>>> Hi, my name is Mark.
>>Justin Uberti: Hi, Mark.

378
00:32:36,780 --> 00:32:43,780
>>> I would like to know, because I'm using
Linux and I want to, how, finally, can I get

379
00:32:44,799 --> 00:32:48,129
rid of the talk plug-in for using Hangouts
in Google+?

380
00:32:48,129 --> 00:32:52,830
>>Justin Uberti: So the question was when
can we get rid of the Hangouts plug-in? And

381
00:32:52,830 --> 00:32:57,460
so, unfortunately, like, we can only talk
about WebRTC matters today. That's handled

382
00:32:57,460 --> 00:32:59,990
by another team.
But let's say there are many of us who have

383
00:32:59,990 --> 00:33:03,769
the same feeling.
>>> Okay.

384
00:33:03,769 --> 00:33:07,850
[ Laughter ]
>>> Can you make any comments on Microsoft's

385
00:33:07,850 --> 00:33:14,850
competing standard? Considering they kind
of hold the cards with Skype and how maybe

386
00:33:15,669 --> 00:33:21,009
we can go forward supporting both or maybe
converge the two or just your thoughts on

387
00:33:21,009 --> 00:33:24,629
that.
>>Justin Uberti: So Microsoft has actually

388
00:33:24,629 --> 00:33:29,519
been very -- a great participant in standards.
They have several people they send from their

389
00:33:29,519 --> 00:33:34,370
team. And, you know, although they don't see
things exactly the same way that we do, I

390
00:33:34,370 --> 00:33:38,940
think that the API differences are sort of
theirs is a little more low level, geared

391
00:33:38,940 --> 00:33:43,789
for expert developers. Ours are a little more
high-level, geared for Web developers. I think

392
00:33:43,789 --> 00:33:48,190
really what you can do is you can implement
the high-level one on top of the low-level

393
00:33:48,190 --> 00:33:53,169
one, maybe even vice versa.
So, like, Microsoft, you know, is a little

394
00:33:53,169 --> 00:33:57,759
more secretive about what they do. So we don't
know exactly what their time frame is relative

395
00:33:57,759 --> 00:34:02,659
to IE. They are fully participating, and,
obviously, they are interested in with Skype.

396
00:34:02,659 --> 00:34:07,570
I'm optimistic that we will see a version
of IE that supports this technology in the

397
00:34:07,570 --> 00:34:12,940
not-too-distant future.
>>> Very good to hear. Thank you.

398
00:34:12,940 --> 00:34:17,300
>>> My question would be I think you mentioned
it quickly in the beginning, so if I wanted

399
00:34:17,300 --> 00:34:21,840
to communicate with WebRTC but, one, I'm using
a different environment than the browser,

400
00:34:21,840 --> 00:34:26,260
let's say I want the Web application to speak
to a native Android app, so what would be

401
00:34:26,260 --> 00:34:30,380
the approach to integrate that with WebRTC?
>>Justin Uberti: So as I mentioned earlier,

402
00:34:30,380 --> 00:34:37,200
we have a fully supported, official native
version of PeerConnection, peerconnection.java,

403
00:34:37,200 --> 00:34:40,830
which is open source. And you can download
and you can build that into your native application

404
00:34:40,830 --> 00:34:43,710
and it interoperates.
We have a demo app that interoperates with

405
00:34:43,710 --> 00:34:50,090
our AppRTC demo app.
So, you know, I think that sort of using Chrome

406
00:34:50,090 --> 00:34:53,700
for Android in a Web view is one thing you
can think about. If that doesn't work for

407
00:34:53,700 --> 00:34:56,100
you, we have the native version that works
great.

408
00:34:56,100 --> 00:35:01,960
>>> Okay, thank you.
>>> Hi. My question would be: Are there any

409
00:35:01,960 --> 00:35:07,630
things to be taken care between cross-browser
compatibility for this Firefox, Chrome? Anything

410
00:35:07,630 --> 00:35:14,450
specific that needs to be taken care of?
>>Justin Uberti: There's some minor differences.

411
00:35:14,450 --> 00:35:18,940
I mentioned adapter.js covers some of the
sort of things where the API isn't quite in

412
00:35:18,940 --> 00:35:25,380
sync in both places. One specific thing is
that Firefox only supports, like, the Opus

413
00:35:25,380 --> 00:35:32,160
codec and they only support DTLS Encryption.
They don't support something called SDES that

414
00:35:32,160 --> 00:35:35,860
we also support.
So for right now, you have to set, like, one

415
00:35:35,860 --> 00:35:42,430
parameter in the API. And you can see that
in the -- in our AppRTC source code to make

416
00:35:42,430 --> 00:35:46,490
sure that communication actually uses this
compatible protocol.

417
00:35:46,490 --> 00:35:49,810
We actually have a document, though, on our
Web page. The document is exactly what you

418
00:35:49,810 --> 00:35:53,810
have to do, which is really setting a single
constraint parameter when you are creating

419
00:35:53,810 --> 00:35:57,450
your PeerConnection object.
>>Sam Dutton: If you go to webrtc.org/interop.

420
00:35:57,450 --> 00:36:03,350
>>Justin Uberti: Webrtc.org/interop.
>>> Okay, thank you.

421
00:36:03,350 --> 00:36:10,110
>>> When a PeerConnection is made and it falls
back to TURN, does the TURN server, is it

422
00:36:10,110 --> 00:36:14,480
capable of unencrypting the messages that
go between the two end points?

423
00:36:14,480 --> 00:36:19,310
>>Justin Uberti: No, The TURN server is just
a packet relay. So the stuff is fully encrypted.

424
00:36:19,310 --> 00:36:22,950
It doesn't have the keying information to
do anything to it. So the TURN server just

425
00:36:22,950 --> 00:36:28,400
takes a byte, sends a byte, takes a packet,
sends a packet.

426
00:36:28,400 --> 00:36:33,950
>>> So for keeping data in sync with low latency
between, say, an Android application and the

427
00:36:33,950 --> 00:36:40,950
server, how would the -- both the native and
the HTML5 -- or the Android Chrome implementations

428
00:36:42,890 --> 00:36:47,720
of WebRTC fare in terms of battery life?
>>Justin Uberti: I don't really have a good

429
00:36:47,720 --> 00:36:53,320
answer for that. I wouldn't think there would
be much difference. I mean, the key things

430
00:36:53,320 --> 00:36:56,920
that are going to be driving battery consumption
in this case -- are you talking about data

431
00:36:56,920 --> 00:36:58,890
or are you talking about audio and video?
>>> Data.

432
00:36:58,890 --> 00:37:02,480
>>Justin Uberti: For data, the key drivers
for your power consumption are going to be

433
00:37:02,480 --> 00:37:07,990
the screen and the network. So I think those
should be comparable between Chrome for Android

434
00:37:07,990 --> 00:37:14,610
and in the native application.
>>> Okay, cool. Thanks.

435
00:37:14,610 --> 00:37:20,750
>>> With two computers running Chrome, what
-- have you seen glass-to-glass latency?

436
00:37:20,750 --> 00:37:25,230
>>Justin Uberti: Repeat?
>>> Glass-to-glass latency, so from the camera

437
00:37:25,230 --> 00:37:30,580
to the opacity.
>>Justin Uberti: Oh, yeah. So it depends on

438
00:37:30,580 --> 00:37:36,190
the platform because, like, the camera can
have a large delay built into it itself. And,

439
00:37:36,190 --> 00:37:41,680
also, some things -- some of the audio things
have, you know, higher latencies than others.

440
00:37:41,680 --> 00:37:46,690
But the overall target is 150 milliseconds
end to end. And we've seen lower than 100

441
00:37:46,690 --> 00:37:51,150
milliseconds in best-case solutions for glass-to-glass-type
latency.

442
00:37:51,150 --> 00:37:57,270
>>> Okay. And how are you ensuring priority
of your data across the network?

443
00:37:57,270 --> 00:38:03,920
>>Justin Uberti: Okay. So that's a complex
question with a long answer. But the basic

444
00:38:03,920 --> 00:38:08,350
thing -- are you saying how do we compete
with, like, cat videos?

445
00:38:08,350 --> 00:38:13,400
>>> No. Within the WebRTC, are you -- how
are you tagging your packets?

446
00:38:13,400 --> 00:38:15,200
>>Justin Uberti: Right.
>>> Or otherwise.

447
00:38:15,200 --> 00:38:20,350
>>Justin Uberti: There is something called
DSCP where we can mark QUOS bits. And this

448
00:38:20,350 --> 00:38:24,380
isn't yet implemented in WebRTC, but it is
on the roadmap to be able to tag things like

449
00:38:24,380 --> 00:38:29,120
audio as higher priority than, say, video
and that has a higher priority than cat videos.

450
00:38:29,120 --> 00:38:31,760
>>> So it is not done today, but it will be
done?

451
00:38:31,760 --> 00:38:36,030
>>Justin Uberti: It will be done. We also
have things for doing, like, FEC-type mechanisms

452
00:38:36,030 --> 00:38:40,870
to, you know, protect things at the application
layer. But the expectation is that, you know,

453
00:38:40,870 --> 00:38:46,890
as WebRTC becomes more pervasive, carriers
will support DSCP at least on the bit from

454
00:38:46,890 --> 00:38:49,640
coming off the computer and going onto their
network.

455
00:38:49,640 --> 00:38:54,010
We have seen the DSCP does help going through
WiFi access points because WiFi access points

456
00:38:54,010 --> 00:39:00,520
do give priority to DSCP-marked traffic.
>>> Thank you.

457
00:39:00,520 --> 00:39:05,910
>>> So in Chrome for iOS being limited to
UI Web view and with other restrictions, how

458
00:39:05,910 --> 00:39:11,400
much of WebRTC will you be able to implement?
>>Justin Uberti: So that's a really interesting

459
00:39:11,400 --> 00:39:16,700
question. You know, they haven't made it easy
for us, but the Chrome for iOS team has already

460
00:39:16,700 --> 00:39:20,890
done some amazing things to deliver the Chrome
experience that exists there now. And so we're

461
00:39:20,890 --> 00:39:25,290
pretty optimistic that one way or another,
we can find some way to make that work. No

462
00:39:25,290 --> 00:39:32,290
commitment to time frame, though.
>>> What are the mechanisms for saving video

463
00:39:32,340 --> 00:39:39,340
and audio that's broadcast with WebRTC? Like
record -- like, making video recordings from

464
00:39:39,410 --> 00:39:42,490
it?
>>Justin Uberti: So if you have the media

465
00:39:42,490 --> 00:39:46,320
stream, you can then take the media stream
and plug it into things like the Web audio

466
00:39:46,320 --> 00:39:50,530
API where you can actually get the raw samples
and then make a wav file and then save that

467
00:39:50,530 --> 00:39:53,360
out.
On the video side, you can go into a canvas

468
00:39:53,360 --> 00:39:57,100
and extract the frames from a canvas and you
can save it. There isn't really way to sort

469
00:39:57,100 --> 00:40:04,100
of save it as, like, a .mp4, .webm file yet.
But if you want to make a thing that just

470
00:40:05,800 --> 00:40:10,070
captures audio from the computer and it stores
on the server, you can basically make a custom

471
00:40:10,070 --> 00:40:12,850
server that can do that recording. That's
one option.

472
00:40:12,850 --> 00:40:17,110
>>> So the TURN server is open -- but you
said the TURN server doesn't capture, act

473
00:40:17,110 --> 00:40:21,490
as an end point. Do you have server technology
that acts as an end point?

474
00:40:21,490 --> 00:40:28,490
>>Justin Uberti: There are people building
this sort of stuff. Vline might be one particular

475
00:40:28,610 --> 00:40:31,940
vendor who does this. But there is something
where you can basically have an MCU and the

476
00:40:31,940 --> 00:40:36,290
MCU that receives the media can do things
like compositing or recording of that media.

477
00:40:36,290 --> 00:40:41,240
>>> Presumably, the libraries for Java or
Objective C can be user to create a server

478
00:40:41,240 --> 00:40:42,560
implementation?
>>Justin Uberti: Exactly. That's what they're

479
00:40:42,560 --> 00:40:46,890
doing.
>>> Kind of a two-part question that has to

480
00:40:46,890 --> 00:40:52,900
do around codec specifically on the video
side. Currently VP8, webm. Is there plans

481
00:40:52,900 --> 00:40:58,420
for h.264 and also what's the time line?
>>Justin Uberti: Our plans around the VP family

482
00:40:58,420 --> 00:41:03,460
of codec. So we support VP8 and VP9.
You may have heard that is sort of trying

483
00:41:03,460 --> 00:41:07,990
to finalize the bit stream right now. So we
are very much looking forward to taking advantage

484
00:41:07,990 --> 00:41:12,010
of VP9 with all of its sort of new coding
techniques once it is both finished and also

485
00:41:12,010 --> 00:41:16,200
optimized for realtime average.
>>> And h.264 not really on the plane?

486
00:41:16,200 --> 00:41:21,360
>>Justin Uberti: We think VP9 provides much
better sort of compression and overall performance

487
00:41:21,360 --> 00:41:25,950
than h.264 so we have no plans for h.264 at
this time.

488
00:41:25,950 --> 00:41:32,950
>>> I'm running WebRTC on Chrome for tablets.
How does it compare with native performance

489
00:41:35,600 --> 00:41:39,330
like Hangouts on Android?
>>Justin Uberti: We think that we provide

490
00:41:39,330 --> 00:41:44,100
a comparable performance to any native application
right now. We're always trying to make things

491
00:41:44,100 --> 00:41:49,290
better. We still have Chrome for Android.
The WebRTC is behind a flag because we still

492
00:41:49,290 --> 00:41:53,770
have work to do around improving audio, improving
some of the performance. We think we can deliver

493
00:41:53,770 --> 00:41:57,050
equivalent performance in the Web browser.
And we are also working on taking advantage

494
00:41:57,050 --> 00:42:02,560
of acceleration in cases where there is hardware
encoders like there is on Nexus 10 and making

495
00:42:02,560 --> 00:42:09,560
that so we can get the same sort of down-to-the-metal
performance that you can get from a native

496
00:42:09,770 --> 00:42:10,820
app.
>>> Thanks.

497
00:42:10,820 --> 00:42:17,820
>>> So the Google Talk plug-in is using not
just h.264 but h.264 svc optimized for the

498
00:42:18,710 --> 00:42:24,290
needs of video conferencing. Is VP8 and VP9
going to be similarly optimized, specifically

499
00:42:24,290 --> 00:42:30,840
in an svc-like fashion for videoconferencing
versus just the version used for file encoding?

500
00:42:30,840 --> 00:42:37,840
>>Justin Uberti: So VP8 already supports temporal
scalability and the S part of SVC. VP9 supports

501
00:42:39,500 --> 00:42:43,290
additional scalability modes as well.
So we are very excited about the new coding

502
00:42:43,290 --> 00:42:50,290
techniques that are coming in VP9.
>>> So we want to use WebRTC to do live streaming

503
00:42:50,670 --> 00:42:57,670
from, let's say, cameras, hardware cameras
and one of the things, you know, we should

504
00:42:59,450 --> 00:43:05,230
take care for such kind of an application.
And when you mentioned VP8 and VP9 support,

505
00:43:05,230 --> 00:43:09,980
h.264 is not supported, assuming hardware
support is only h.264, (inaudible) can be

506
00:43:09,980 --> 00:43:14,360
used with Chrome in that case?
>>Justin Uberti: We are building up support

507
00:43:14,360 --> 00:43:19,090
for hardware VP8 and later VP9 encoders.
>>> Okay.

508
00:43:19,090 --> 00:43:24,910
>>Justin Uberti: So, you know, you can make
a media streaming application like you described,

509
00:43:24,910 --> 00:43:31,060
but we're expecting that all the major SSE
vendors are now shipping hardware with built-in

510
00:43:31,060 --> 00:43:35,190
VP8 encoders and decoders.
So as this stuff gets into market, you are

511
00:43:35,190 --> 00:43:40,000
going to see this stuff become, you know,
the most efficient way to record and compress

512
00:43:40,000 --> 00:43:43,540
data.
>>> Okay. So the only way is to support VP8

513
00:43:43,540 --> 00:43:48,290
in hardware right now for us, right?
>>Justin Uberti: If you want hardware -- if

514
00:43:48,290 --> 00:43:53,060
you basically -- if you want hardware compression,
the only -- the only things we support right

515
00:43:53,060 --> 00:43:57,230
now will be VP8 encoders.
>>> That's on the device side, you know, the

516
00:43:57,230 --> 00:44:00,130
camera, which is, you know, is on the remote
computer.

517
00:44:00,130 --> 00:44:03,090
>>Justin Uberti: If you are having encoding
from a device that you want to be decoded

518
00:44:03,090 --> 00:44:07,530
within the browser, I would advise you to
do it in VP8.

519
00:44:07,530 --> 00:44:12,050
>>> Thank you.
>>Justin Uberti: Thank you all for coming.

520
00:44:12,050 --> 00:44:13,040
>>Sam Dutton: Yeah, thank you.
[ Applause ]

