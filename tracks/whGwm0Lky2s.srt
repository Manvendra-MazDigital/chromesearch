1
00:00:00,000 --> 00:00:09,720

2
00:00:09,720 --> 00:00:10,800
Hey everybody.

3
00:00:10,800 --> 00:00:12,770
Welcome to another
fantastic episode

4
00:00:12,770 --> 00:00:16,017
of Google Developers Live,
where we here at Google

5
00:00:16,017 --> 00:00:18,350
get a chance to talk to you
about all the amazing things

6
00:00:18,350 --> 00:00:21,270
going on on the web, in the
tech, and all the platforms

7
00:00:21,270 --> 00:00:23,130
and the fantastic things
you can do with it.

8
00:00:23,130 --> 00:00:24,610
My name is Colt McAnlis.

9
00:00:24,610 --> 00:00:27,270
I'm a developer advocate
here on the Chrome team,

10
00:00:27,270 --> 00:00:30,710
and I mostly work with web
performance issues as well as

11
00:00:30,710 --> 00:00:31,860
web gaming.

12
00:00:31,860 --> 00:00:33,647
Now, what I'd like
to talk to you today

13
00:00:33,647 --> 00:00:35,480
about is something
that's very near and dear

14
00:00:35,480 --> 00:00:38,580
and close to my heart,
and that's compression.

15
00:00:38,580 --> 00:00:41,510
Today, we're going to be talking
about GZIP on the web platform

16
00:00:41,510 --> 00:00:45,360
and how you can modify it
and address it and munge

17
00:00:45,360 --> 00:00:48,740
it to try to minimize your
website memory footprint

18
00:00:48,740 --> 00:00:50,360
and actually get
faster loading data.

19
00:00:50,360 --> 00:00:52,940
Now, before we get into
this talk too much,

20
00:00:52,940 --> 00:00:55,900
I'd actually like to point
out the #perfmatters hashtag.

21
00:00:55,900 --> 00:00:57,400
This is a fantastic
hashtag that's

22
00:00:57,400 --> 00:00:59,440
being used all
around the intertubes

23
00:00:59,440 --> 00:01:02,370
on various social networks
for people in web performance

24
00:01:02,370 --> 00:01:04,940
who are trying to
find the same problems

25
00:01:04,940 --> 00:01:07,380
and address difficult issues
and have conversations

26
00:01:07,380 --> 00:01:10,230
about performance in
interesting and insightful ways.

27
00:01:10,230 --> 00:01:11,570
So use this hashtag.

28
00:01:11,570 --> 00:01:14,864
If anything during this talk
is interesting or inspiring,

29
00:01:14,864 --> 00:01:17,530
feel free to go to your favorite
social media network of choice.

30
00:01:17,530 --> 00:01:20,550
It's a fantastic outlet, and
you need to be following it.

31
00:01:20,550 --> 00:01:23,120
So with that, let's
get into things.

32
00:01:23,120 --> 00:01:24,690
So let's start at the beginning.

33
00:01:24,690 --> 00:01:26,940
I can't really talk about
compression on the web

34
00:01:26,940 --> 00:01:29,675
without actually taking a look
at the state of the web today.

35
00:01:29,675 --> 00:01:31,640
Now, besides being
filled with cats,

36
00:01:31,640 --> 00:01:33,710
there's actually some
interesting data out there

37
00:01:33,710 --> 00:01:35,070
for us to look at.

38
00:01:35,070 --> 00:01:37,627
There's a great site
called httparchive.org,

39
00:01:37,627 --> 00:01:39,210
and what this site
will do is actually

40
00:01:39,210 --> 00:01:42,640
run large amounts of website
through processing filters

41
00:01:42,640 --> 00:01:43,512
during the day.

42
00:01:43,512 --> 00:01:44,970
So what will happen
is they'll take

43
00:01:44,970 --> 00:01:47,989
300,000, 400,000
very common websites,

44
00:01:47,989 --> 00:01:49,530
run them through a
processing system,

45
00:01:49,530 --> 00:01:51,470
and then scrape
information about them

46
00:01:51,470 --> 00:01:52,950
to present on their page.

47
00:01:52,950 --> 00:01:55,450
Now, what you can see
here in this graph

48
00:01:55,450 --> 00:01:57,310
is actually one of the
latest studies that

49
00:01:57,310 --> 00:02:00,910
shows the average bytes of
a page per content type.

50
00:02:00,910 --> 00:02:03,140
So what this graph
shows us is that images,

51
00:02:03,140 --> 00:02:06,350
for an average site,
actually take up

52
00:02:06,350 --> 00:02:10,870
the lion's share of the content
that's being streamed to users.

53
00:02:10,870 --> 00:02:12,390
In fact, it's more than 50%.

54
00:02:12,390 --> 00:02:16,222
It's actually a massive amount
of the content being delivered,

55
00:02:16,222 --> 00:02:18,680
while scripts actually remain
the second largest, ending up

56
00:02:18,680 --> 00:02:21,119
at around 260k or so.

57
00:02:21,119 --> 00:02:22,660
Now, the interesting
thing about this

58
00:02:22,660 --> 00:02:25,990
is when you look at the
individual response sizes,

59
00:02:25,990 --> 00:02:28,150
you see a similar
trend, of course,

60
00:02:28,150 --> 00:02:33,243
Flash being one of the largest
sizes per number of responses.

61
00:02:33,243 --> 00:02:34,993
But of course, that
comes down to the fact

62
00:02:34,993 --> 00:02:39,020
that you generally only have one
or two flashes per page where

63
00:02:39,020 --> 00:02:43,090
they tend to be pretty large,
whereas JavaScript or PNGs

64
00:02:43,090 --> 00:02:45,410
or JPEGs tend to be lots
of requests for a page.

65
00:02:45,410 --> 00:02:47,490
That actually drives
that number down.

66
00:02:47,490 --> 00:02:49,390
So what we're looking
at here is basically

67
00:02:49,390 --> 00:02:53,020
a vision of the web that shows
that we're dominated by images

68
00:02:53,020 --> 00:02:55,070
and we're dominated
by image data.

69
00:02:55,070 --> 00:02:56,870
But I'm not
necessarily sure that

70
00:02:56,870 --> 00:03:00,260
means that we can give up on
trying to minimize and reduce

71
00:03:00,260 --> 00:03:03,632
the size of the text data
that actually drives our web.

72
00:03:03,632 --> 00:03:04,840
So let's take a look at this.

73
00:03:04,840 --> 00:03:07,490
This is another great set
of charts from HTTP Archive.

74
00:03:07,490 --> 00:03:09,323
And effectively, what
you're looking at here

75
00:03:09,323 --> 00:03:11,730
is three graphs showing
the transfer size

76
00:03:11,730 --> 00:03:14,380
against the number of requests.

77
00:03:14,380 --> 00:03:16,840
And so what you're
seeing is over time,

78
00:03:16,840 --> 00:03:20,390
our number of requests have
remained pretty constant

79
00:03:20,390 --> 00:03:23,130
over the past couple
of years, while you

80
00:03:23,130 --> 00:03:26,840
see a general increasing
trend in the overall size

81
00:03:26,840 --> 00:03:27,960
per request.

82
00:03:27,960 --> 00:03:29,750
Now, this is
specifically three graphs

83
00:03:29,750 --> 00:03:32,760
for HTML, JavaScript, and CSS.

84
00:03:32,760 --> 00:03:35,170
What this is telling us is
that while images may make up

85
00:03:35,170 --> 00:03:38,140
the lion's share of
the internet right now,

86
00:03:38,140 --> 00:03:41,580
text information, the backbone
of these three formats,

87
00:03:41,580 --> 00:03:44,560
isn't going away
anytime soon, nor is it

88
00:03:44,560 --> 00:03:46,742
slated to get any smaller.

89
00:03:46,742 --> 00:03:48,450
So for most people,
they would say, well,

90
00:03:48,450 --> 00:03:52,370
if the web is mostly images,
why should we care about text?

91
00:03:52,370 --> 00:03:54,100
Well, here's why.

92
00:03:54,100 --> 00:03:57,000
You see, images may
be fantastic and they

93
00:03:57,000 --> 00:04:00,120
may be the dominant
form of content for us,

94
00:04:00,120 --> 00:04:03,170
but it's apparent and been
shown that they're not

95
00:04:03,170 --> 00:04:04,581
as small as they could be.

96
00:04:04,581 --> 00:04:06,580
So for example, let's
take this fantastic image,

97
00:04:06,580 --> 00:04:08,380
which was captured on a camera.

98
00:04:08,380 --> 00:04:11,130
This is about one
megapixel of data.

99
00:04:11,130 --> 00:04:12,840
Now, if we say
this is a PNG file,

100
00:04:12,840 --> 00:04:16,540
it actually comes out at
about 2.3 megabytes of data.

101
00:04:16,540 --> 00:04:18,500
Now, for those of you
playing the home game,

102
00:04:18,500 --> 00:04:21,050
you'll note that
HTTP Archive averages

103
00:04:21,050 --> 00:04:23,070
that the average
website out there

104
00:04:23,070 --> 00:04:26,850
is about 1.2 to
1.1 megs of data.

105
00:04:26,850 --> 00:04:29,350
So this single PNG image,
if hosted on your website,

106
00:04:29,350 --> 00:04:31,400
is already larger
than the average size

107
00:04:31,400 --> 00:04:32,890
of a website on the internet.

108
00:04:32,890 --> 00:04:34,115
Now, PNG's fantastic.

109
00:04:34,115 --> 00:04:36,170
It allows you to
get transparency,

110
00:04:36,170 --> 00:04:38,620
and it does support GZIP
compression internally

111
00:04:38,620 --> 00:04:41,902
as a format, but it really only
supports a lossless encoding,

112
00:04:41,902 --> 00:04:43,610
which means you're
not really getting rid

113
00:04:43,610 --> 00:04:46,240
of any visual data
that may be redundant.

114
00:04:46,240 --> 00:04:48,740
JPEG, on the other
hand, as a format

115
00:04:48,740 --> 00:04:51,560
is built both a lossy
and lossless encoder.

116
00:04:51,560 --> 00:04:55,100
This allows the JPEG encoding
system to actually remove

117
00:04:55,100 --> 00:04:57,620
visually redundant
information from the image

118
00:04:57,620 --> 00:05:00,135
so that you don't
really notice it's gone.

119
00:05:00,135 --> 00:05:03,180
The human eye is pretty
complex, but still,

120
00:05:03,180 --> 00:05:05,570
at 32 bits per pixel,
there's a lot of information

121
00:05:05,570 --> 00:05:07,250
that we can't discern.

122
00:05:07,250 --> 00:05:09,065
So if you JPEG this
image, you lose

123
00:05:09,065 --> 00:05:10,940
the ability to get
transparency, however, you

124
00:05:10,940 --> 00:05:15,880
do cut the size of the
image down to 297k.

125
00:05:15,880 --> 00:05:20,150
Now, these have been the
two mostly dominant image

126
00:05:20,150 --> 00:05:23,100
formats, besides animated
cat GIFs on the internet,

127
00:05:23,100 --> 00:05:27,710
but earlier this year,
internet technology evangelists

128
00:05:27,710 --> 00:05:30,930
and other enthusiasts
decided that maybe we

129
00:05:30,930 --> 00:05:32,160
weren't done here yet.

130
00:05:32,160 --> 00:05:34,480
Maybe the compression
system of JPEG

131
00:05:34,480 --> 00:05:36,990
wasn't good enough for
the rest of the web,

132
00:05:36,990 --> 00:05:39,970
and thus we had the
WebP format being born.

133
00:05:39,970 --> 00:05:41,854
Now, the WebP format is new.

134
00:05:41,854 --> 00:05:43,770
It's not necessarily
supported by all browsers

135
00:05:43,770 --> 00:05:44,870
as of this date.

136
00:05:44,870 --> 00:05:47,010
But it's actually got
some exciting properties

137
00:05:47,010 --> 00:05:48,710
that are really making
people on the web

138
00:05:48,710 --> 00:05:49,950
stand up and take notice.

139
00:05:49,950 --> 00:05:52,515
So first off, if you compress
this image with WebP,

140
00:05:52,515 --> 00:05:55,140
it actually drops to about 198k.

141
00:05:55,140 --> 00:05:57,350
That's 100 kilobytes
of data difference

142
00:05:57,350 --> 00:05:58,565
for this single image.

143
00:05:58,565 --> 00:06:00,190
In addition to that,
WebP will actually

144
00:06:00,190 --> 00:06:02,580
allow you to get
transparency and even

145
00:06:02,580 --> 00:06:06,220
support some forms of animation,
which means this single image

146
00:06:06,220 --> 00:06:08,850
format gets you the
compression sizes of JPEG,

147
00:06:08,850 --> 00:06:11,170
the transparency
properties of PNG,

148
00:06:11,170 --> 00:06:14,580
as well as the animation
properties of GIFs.

149
00:06:14,580 --> 00:06:17,050
It's an exciting,
one stop solution

150
00:06:17,050 --> 00:06:19,140
for a lot of your image needs.

151
00:06:19,140 --> 00:06:22,320
Now, this is the important
thing to note about this image

152
00:06:22,320 --> 00:06:24,240
size, and about
this image format,

153
00:06:24,240 --> 00:06:27,250
is that most mobile phones
right now are actually

154
00:06:27,250 --> 00:06:29,700
about five megapixels
for their cameras,

155
00:06:29,700 --> 00:06:32,440
which means people are taking
snapshots of the food they eat

156
00:06:32,440 --> 00:06:34,120
or signs on the
street or their kids,

157
00:06:34,120 --> 00:06:36,370
uploading them to their
favorite social media network,

158
00:06:36,370 --> 00:06:39,580
and that's actually coming in
at five megapixels uncompressed.

159
00:06:39,580 --> 00:06:42,330
Now, if we look at PNG and
JPEG and the compression ratios

160
00:06:42,330 --> 00:06:44,030
they got with a
single megabyte image,

161
00:06:44,030 --> 00:06:47,280
you can see that over time,
as the number of images that

162
00:06:47,280 --> 00:06:49,290
fill the internet
increases, we're

163
00:06:49,290 --> 00:06:51,960
going to quickly run into
compression and data size

164
00:06:51,960 --> 00:06:52,650
issues.

165
00:06:52,650 --> 00:06:54,770
That's why WebP shows
up, and it actually

166
00:06:54,770 --> 00:06:56,930
solves a huge problem
for us and allows

167
00:06:56,930 --> 00:06:58,230
us to address a lot of issues.

168
00:06:58,230 --> 00:07:01,870
Now, there's a whole separate
series of GDLs done on WebP.

169
00:07:01,870 --> 00:07:03,620
I encourage you to go
to the GDL website,

170
00:07:03,620 --> 00:07:04,880
take a look at
some of these talks

171
00:07:04,880 --> 00:07:06,760
for more information on how
to get started with that.

172
00:07:06,760 --> 00:07:08,593
Now, that's all I'm
going to say about image

173
00:07:08,593 --> 00:07:11,090
compression for now.

174
00:07:11,090 --> 00:07:13,070
But in the meantime,
let's talk about text.

175
00:07:13,070 --> 00:07:17,680
So I said before that text
data-- CSS, HTML, JavaScript--

176
00:07:17,680 --> 00:07:20,830
actually drive the rendering
performance and initial page

177
00:07:20,830 --> 00:07:24,220
load of your page
more than images do.

178
00:07:24,220 --> 00:07:25,694
This is for a very
specific reason.

179
00:07:25,694 --> 00:07:27,110
So if you look at
this graph here,

180
00:07:27,110 --> 00:07:30,380
we actually can't start
rendering anything on your page

181
00:07:30,380 --> 00:07:34,520
until the HTML has been parsed
and subsequent JavaScript

182
00:07:34,520 --> 00:07:39,010
and CSS may be loaded to create
both the DOM and the CSS DOM

183
00:07:39,010 --> 00:07:40,890
so that we can actually
parse the render tree

184
00:07:40,890 --> 00:07:43,070
and actually start getting
pixels on the screen.

185
00:07:43,070 --> 00:07:46,395
So if we look at this average
flow of events from HTML

186
00:07:46,395 --> 00:07:48,710
to CSS to actually
getting pixels there,

187
00:07:48,710 --> 00:07:50,650
we can actually see an
interesting example.

188
00:07:50,650 --> 00:07:53,780
So we have a standard HTML,
which actually links to a CSS.

189
00:07:53,780 --> 00:07:56,180
That CSS is updating
some of the information

190
00:07:56,180 --> 00:07:57,660
on the page, the text data.

191
00:07:57,660 --> 00:07:59,340
We also have an image
that's referenced

192
00:07:59,340 --> 00:08:01,780
and a script that's
at the bottom of it.

193
00:08:01,780 --> 00:08:04,690
Now you can see on the
mobile image on the side

194
00:08:04,690 --> 00:08:07,200
there that nothing
has been shown yet,

195
00:08:07,200 --> 00:08:09,830
even though this
HTML has been parsed.

196
00:08:09,830 --> 00:08:12,120
So we've got our little HTML
box that's mostly orange.

197
00:08:12,120 --> 00:08:14,170
You can see that the DOM
has mostly been parsed.

198
00:08:14,170 --> 00:08:15,200
It's partially orange.

199
00:08:15,200 --> 00:08:17,920
And that the CSS
file-- example.css--

200
00:08:17,920 --> 00:08:21,620
has been discovered, but it
hasn't really been loaded yet.

201
00:08:21,620 --> 00:08:23,760
So this means that we
actually cannot display text

202
00:08:23,760 --> 00:08:26,510
on the screen because we don't
have the styling properties

203
00:08:26,510 --> 00:08:29,080
that define that
text on the screen.

204
00:08:29,080 --> 00:08:31,130
It's not until the
CSS data actually

205
00:08:31,130 --> 00:08:34,460
gets loaded that we can build
the CSS version of the DOM

206
00:08:34,460 --> 00:08:36,650
and actually begin
constructing the render tree.

207
00:08:36,650 --> 00:08:39,510
Now notice that the DOM still
hasn't finished loading yet.

208
00:08:39,510 --> 00:08:40,730
That's OK.

209
00:08:40,730 --> 00:08:43,510
If we can actually partially
load and partially display

210
00:08:43,510 --> 00:08:46,680
the top part of the DOM, or
what people typically say

211
00:08:46,680 --> 00:08:49,440
is above the fold, that allows
us to actually start rendering

212
00:08:49,440 --> 00:08:51,980
pixels on the screen while
the bottom part of the page

213
00:08:51,980 --> 00:08:52,904
is still loading.

214
00:08:52,904 --> 00:08:54,320
So now that the
CSS has loaded, we

215
00:08:54,320 --> 00:08:56,340
can actually get some
text on the screen.

216
00:08:56,340 --> 00:08:58,670
Now notice that layout
and paint are halfway

217
00:08:58,670 --> 00:09:01,140
through their process because
they don't have all the data.

218
00:09:01,140 --> 00:09:03,530
Loading up next should be
WebP, and then of course,

219
00:09:03,530 --> 00:09:06,050
last.js is defined
there as well.

220
00:09:06,050 --> 00:09:08,825
Now, because of the
file sizes, last.js

221
00:09:08,825 --> 00:09:11,430
is smaller than the image,
and so it can actually

222
00:09:11,430 --> 00:09:13,880
get loaded and parsed
before the image

223
00:09:13,880 --> 00:09:15,720
data actually gets
to the screen.

224
00:09:15,720 --> 00:09:18,767
Now, if last.js kicks
off some style chains

225
00:09:18,767 --> 00:09:20,850
or some other information
that needs to be loaded,

226
00:09:20,850 --> 00:09:22,580
it can go back
and change the DOM

227
00:09:22,580 --> 00:09:25,600
and change the CSS properties,
forcing page reflows,

228
00:09:25,600 --> 00:09:28,330
forcing page repaints, and other
sorts of chaos in your loading

229
00:09:28,330 --> 00:09:28,830
time.

230
00:09:28,830 --> 00:09:31,550
But notice the image still
hasn't been loaded yet

231
00:09:31,550 --> 00:09:33,380
and hasn't been
displayed on screen.

232
00:09:33,380 --> 00:09:35,680
It's not until the final
complete bits come off

233
00:09:35,680 --> 00:09:38,140
the wire, the CSS
DOM can be finished,

234
00:09:38,140 --> 00:09:39,900
the render tree can
actually be completed,

235
00:09:39,900 --> 00:09:41,650
and finally, we can
actually get the image

236
00:09:41,650 --> 00:09:43,487
on the screen showing
you what we're

237
00:09:43,487 --> 00:09:45,070
trying to show you
in the first place.

238
00:09:45,070 --> 00:09:46,611
Now, what you should
gather from this

239
00:09:46,611 --> 00:09:49,800
is that while images make up the
bulk of the content on the web,

240
00:09:49,800 --> 00:09:52,350
it's really the
textual base data that

241
00:09:52,350 --> 00:09:56,204
drives how and when
pixels get to your screen.

242
00:09:56,204 --> 00:09:57,620
And when you're
trying to optimize

243
00:09:57,620 --> 00:10:01,637
for fast path and critical path
rendering on mobile devices,

244
00:10:01,637 --> 00:10:03,053
it's really the
text data you need

245
00:10:03,053 --> 00:10:06,550
to get off the wire
as fast as possible.

246
00:10:06,550 --> 00:10:08,055
Now, a lot of you
kids out there are

247
00:10:08,055 --> 00:10:09,680
talking about some
really cool internet

248
00:10:09,680 --> 00:10:11,080
technology called Emscripten.

249
00:10:11,080 --> 00:10:14,430
Now, this is a library has
been developed open source that

250
00:10:14,430 --> 00:10:18,130
allows developers to take
existing C and C++ code

251
00:10:18,130 --> 00:10:21,150
and trans-compile that
to JavaScript data.

252
00:10:21,150 --> 00:10:23,010
Probably one of the
most impressive examples

253
00:10:23,010 --> 00:10:25,840
of this technology was debuted
a little bit earlier this year,

254
00:10:25,840 --> 00:10:28,380
sometime in March at the game
developer conference, where

255
00:10:28,380 --> 00:10:30,820
Nvidia and Mozilla and a
bunch of the open source

256
00:10:30,820 --> 00:10:34,850
community at Emscripten actually
debuted the Unreal 3 graphics

257
00:10:34,850 --> 00:10:38,540
engine running on
top of Emscripten.

258
00:10:38,540 --> 00:10:40,880
Now, this means that you
get this rich, full graphics

259
00:10:40,880 --> 00:10:42,360
engine running
inside of a browser.

260
00:10:42,360 --> 00:10:43,901
It runs inside of
JavaScript, so it's

261
00:10:43,901 --> 00:10:45,780
spec compliant and all
this other fun stuff.

262
00:10:45,780 --> 00:10:47,695
Now, I'm from the
games industry,

263
00:10:47,695 --> 00:10:50,070
and I've been working with
Unreal 3 for a number of years

264
00:10:50,070 --> 00:10:50,410
now.

265
00:10:50,410 --> 00:10:52,090
And one of the things
Unreal has never

266
00:10:52,090 --> 00:10:55,600
been popular for is
being small in size.

267
00:10:55,600 --> 00:10:57,910
You can actually see that
when we move the source

268
00:10:57,910 --> 00:11:00,320
code and the compiled
data to the web,

269
00:11:00,320 --> 00:11:02,300
this problem still consists.

270
00:11:02,300 --> 00:11:06,480
So if you look at the loading
time of this application,

271
00:11:06,480 --> 00:11:08,360
you can actually see
that the core JavaScript

272
00:11:08,360 --> 00:11:11,610
file for this Emscripten-based
port of Unreal

273
00:11:11,610 --> 00:11:14,120
is about 50 megabytes of data.

274
00:11:14,120 --> 00:11:16,810
50 megabytes to bring
down a JavaScript

275
00:11:16,810 --> 00:11:18,980
file to actually load this game.

276
00:11:18,980 --> 00:11:21,080
Now to be fair, it
is served compressed,

277
00:11:21,080 --> 00:11:22,496
which means that
you actually only

278
00:11:22,496 --> 00:11:24,350
have to pull down
five megabytes of data

279
00:11:24,350 --> 00:11:27,780
across the wire for this
single JavaScript file.

280
00:11:27,780 --> 00:11:29,040
But still, five megabytes.

281
00:11:29,040 --> 00:11:31,360
That's five times larger than
any of our other websites

282
00:11:31,360 --> 00:11:31,860
out there.

283
00:11:31,860 --> 00:11:34,410
And by the way, this isn't
counting the 18 megabytes

284
00:11:34,410 --> 00:11:37,540
worth of data that has to
be pulled down as well.

285
00:11:37,540 --> 00:11:39,670
So when you look at these
trends-- and the more

286
00:11:39,670 --> 00:11:42,160
developers that are trying to
move towards high performance

287
00:11:42,160 --> 00:11:45,000
JavaScript execution using
things like Emscripten

288
00:11:45,000 --> 00:11:48,170
and asm.js-- what you
start seeing is a trend.

289
00:11:48,170 --> 00:11:51,370
The more web applications
that produce source code

290
00:11:51,370 --> 00:11:53,540
in JavaScript that come
from other languages,

291
00:11:53,540 --> 00:11:57,041
we'll start seeing bloated
and larger and larger

292
00:11:57,041 --> 00:11:59,540
JavaScript files that need to
be downloaded by their clients

293
00:11:59,540 --> 00:12:01,510
before pixels can get
on the screen and a game

294
00:12:01,510 --> 00:12:02,270
can be played.

295
00:12:02,270 --> 00:12:05,400
This trend is not going
to diminish over time.

296
00:12:05,400 --> 00:12:07,640
This means that our
text data is going

297
00:12:07,640 --> 00:12:09,870
to continue growing larger.

298
00:12:09,870 --> 00:12:12,230
As such, as a
developer, it's your job

299
00:12:12,230 --> 00:12:14,660
to figure out how to
minimize, compress, and reduce

300
00:12:14,660 --> 00:12:17,390
the number of bits on the wire.

301
00:12:17,390 --> 00:12:19,410
And so with that,
let's take a look

302
00:12:19,410 --> 00:12:21,876
at the good old
boy known as GZIP,

303
00:12:21,876 --> 00:12:24,000
sort of the backbone of
compression on the internet

304
00:12:24,000 --> 00:12:24,940
today.

305
00:12:24,940 --> 00:12:29,570
Now, I want to do a quick side
to note that compression is not

306
00:12:29,570 --> 00:12:32,830
the same as minimization
or minification,

307
00:12:32,830 --> 00:12:34,880
depending on how purist
you are on the term.

308
00:12:34,880 --> 00:12:36,960
So let's take a look at
the two really quick.

309
00:12:36,960 --> 00:12:40,010
Minification in the
web platform is the art

310
00:12:40,010 --> 00:12:43,880
of removing redundant
information from textual data

311
00:12:43,880 --> 00:12:48,510
such that we can still
symbolically parse and process

312
00:12:48,510 --> 00:12:51,241
it when we pass it off
to the underlying system.

313
00:12:51,241 --> 00:12:52,740
So if you look at
this example here,

314
00:12:52,740 --> 00:12:54,780
we've got a function
that adds two numbers,

315
00:12:54,780 --> 00:12:57,610
but there seems like
there's a lot of information

316
00:12:57,610 --> 00:12:58,460
that you don't see.

317
00:12:58,460 --> 00:13:01,930
There's line returns, there's
extra spacing information,

318
00:13:01,930 --> 00:13:04,610
perhaps the variable names
are actually too long.

319
00:13:04,610 --> 00:13:07,710
The process of minification
actually reduces all of this

320
00:13:07,710 --> 00:13:12,610
to give you the least number of
completely processing and valid

321
00:13:12,610 --> 00:13:15,040
bytes for this file
representation.

322
00:13:15,040 --> 00:13:17,290
Again here, we can
process this ahead of time

323
00:13:17,290 --> 00:13:19,850
and actually pass this right
to the underlying systems

324
00:13:19,850 --> 00:13:22,781
to be processed and actually
get stuff on your screen.

325
00:13:22,781 --> 00:13:24,280
Now, compression,
on the other hand,

326
00:13:24,280 --> 00:13:26,060
offers something
completely different.

327
00:13:26,060 --> 00:13:29,240
Compression is the act of
modifying your data so that it

328
00:13:29,240 --> 00:13:32,770
has to be reconstructed before
being passed to the underlying

329
00:13:32,770 --> 00:13:33,600
processing system.

330
00:13:33,600 --> 00:13:35,570
So again, if we take
the minimized form

331
00:13:35,570 --> 00:13:37,620
of that some
function, compression

332
00:13:37,620 --> 00:13:40,900
will actually turn it into
a bit stream of information

333
00:13:40,900 --> 00:13:44,360
that then has to be deflated,
or actually decompressed

334
00:13:44,360 --> 00:13:46,280
into the original form
before it can actually

335
00:13:46,280 --> 00:13:48,260
be passed off to the
underlying systems.

336
00:13:48,260 --> 00:13:51,480
These two technologies
work as a one-two punch.

337
00:13:51,480 --> 00:13:54,600
So you actually have to add
minification to your technology

338
00:13:54,600 --> 00:13:57,020
before you allow GZIP-ing
to occur in order

339
00:13:57,020 --> 00:13:58,851
to get the fewest
bytes on the wire.

340
00:13:58,851 --> 00:14:00,350
Now for the rest
of this talk, we're

341
00:14:00,350 --> 00:14:03,410
actually going to be addressing
some of the issues and pros

342
00:14:03,410 --> 00:14:05,050
and cons of GZIP,
but before we do

343
00:14:05,050 --> 00:14:07,174
that, I need to make sure
that we understand what's

344
00:14:07,174 --> 00:14:09,060
going on under the
hood of this algorithm.

345
00:14:09,060 --> 00:14:11,450
You see, GZIP is a
compression format

346
00:14:11,450 --> 00:14:13,880
that actually uses two
interplaying technologies

347
00:14:13,880 --> 00:14:14,760
together.

348
00:14:14,760 --> 00:14:17,646
The first one is a
technology known as LZ77.

349
00:14:17,646 --> 00:14:20,830
Now, this is a
dictionary-based transform

350
00:14:20,830 --> 00:14:23,130
that will actually take
a given data stream

351
00:14:23,130 --> 00:14:26,170
and convert it into
a sequence of tuples.

352
00:14:26,170 --> 00:14:30,350
Now, in each tuple, we actually
have a position and a length

353
00:14:30,350 --> 00:14:30,850
value.

354
00:14:30,850 --> 00:14:32,330
So I know that's confusing.

355
00:14:32,330 --> 00:14:34,220
Let's take a look at
a little example here.

356
00:14:34,220 --> 00:14:35,761
So we've got this
string up top which

357
00:14:35,761 --> 00:14:37,570
is comprised of
various A, B's, and C's

358
00:14:37,570 --> 00:14:39,080
in some sort of random order.

359
00:14:39,080 --> 00:14:41,420
What happens is if we
start parsing this string,

360
00:14:41,420 --> 00:14:43,870
we're going to look at
each symbol individually

361
00:14:43,870 --> 00:14:47,220
and we're going to do a
scan backwards to find out

362
00:14:47,220 --> 00:14:51,000
when previously we've
seen this symbol before,

363
00:14:51,000 --> 00:14:52,930
because then that allows
us to actually encode

364
00:14:52,930 --> 00:14:56,470
this symbol rather as a
single piece of information,

365
00:14:56,470 --> 00:14:59,180
but actually as a relative
piece of information.

366
00:14:59,180 --> 00:15:01,620
The goal here in
LZ77 is actually

367
00:15:01,620 --> 00:15:04,464
to create a lot of
redundant types of tuples

368
00:15:04,464 --> 00:15:06,380
that we can then compress
a little bit better.

369
00:15:06,380 --> 00:15:08,560
So let's walk through
this just a little bit

370
00:15:08,560 --> 00:15:10,170
so you can see what's going on.

371
00:15:10,170 --> 00:15:12,330
So you see if we start
with the first A here

372
00:15:12,330 --> 00:15:13,830
and we actually
parse it, well, it's

373
00:15:13,830 --> 00:15:15,871
the beginning of the
string, so we haven't really

374
00:15:15,871 --> 00:15:17,219
seen anything before yet.

375
00:15:17,219 --> 00:15:18,760
Therefore, we actually
have to output

376
00:15:18,760 --> 00:15:20,540
a position and length of 0, 0.

377
00:15:20,540 --> 00:15:22,150
It represents that
we're not actually

378
00:15:22,150 --> 00:15:23,690
seeing any other characters.

379
00:15:23,690 --> 00:15:25,730
We're just looking
at the A itself.

380
00:15:25,730 --> 00:15:29,380
Now, when we get to the second
A, we start our backwards scan

381
00:15:29,380 --> 00:15:31,670
and we find that the
first A we encounter

382
00:15:31,670 --> 00:15:35,290
was actually one symbol ago
at a length of one symbol.

383
00:15:35,290 --> 00:15:37,880
So this means we can actually
output the tuple, 1, 1.

384
00:15:37,880 --> 00:15:39,620
Now B, we haven't
seen any B's before,

385
00:15:39,620 --> 00:15:41,150
so we have to output 0, 0.

386
00:15:41,150 --> 00:15:43,240
And C, same thing, 0, 0.

387
00:15:43,240 --> 00:15:45,126
But now we've reached
another B. So it's

388
00:15:45,126 --> 00:15:46,500
going to start
scanning backwards

389
00:15:46,500 --> 00:15:49,430
and actually finds that the last
B we encountered in the stream

390
00:15:49,430 --> 00:15:51,000
was two symbols ago.

391
00:15:51,000 --> 00:15:52,585
And again, we only
want length of one.

392
00:15:52,585 --> 00:15:54,960
Now, the important and probably
the more interesting part

393
00:15:54,960 --> 00:15:56,590
of this particular
example is when

394
00:15:56,590 --> 00:16:00,010
we get to the end of the string
where we see A, B, C. Now,

395
00:16:00,010 --> 00:16:01,900
when we scan backwards
from A, B, C,

396
00:16:01,900 --> 00:16:05,040
we can actually find that we
found this exact three symbol

397
00:16:05,040 --> 00:16:08,490
value previously in
our stream, and exactly

398
00:16:08,490 --> 00:16:11,280
about five character
positions back.

399
00:16:11,280 --> 00:16:14,140
So this means we're going
to output the tuple of 5, 3.

400
00:16:14,140 --> 00:16:16,450
So we output a tuple rather
than the three characters

401
00:16:16,450 --> 00:16:17,010
themselves.

402
00:16:17,010 --> 00:16:21,590
Now again, the point of
LZ77 is to actually create

403
00:16:21,590 --> 00:16:24,930
duplicate and high
redundancy tuples.

404
00:16:24,930 --> 00:16:26,750
These tuples and the
redundancy that we

405
00:16:26,750 --> 00:16:29,300
create with them are actually
very important to pass off

406
00:16:29,300 --> 00:16:31,630
to the next step of
the GZIP algorithm

407
00:16:31,630 --> 00:16:33,520
known as Huffman compression.

408
00:16:33,520 --> 00:16:36,020
Now, for those of you who don't
remember Huffman compression

409
00:16:36,020 --> 00:16:38,510
or probably have blocked
it out of your mind back

410
00:16:38,510 --> 00:16:41,310
from the Computer Science
101 days at your university,

411
00:16:41,310 --> 00:16:46,710
Huffman works by assigning
variable length bit codes

412
00:16:46,710 --> 00:16:50,530
to symbols in your stream
based on probability.

413
00:16:50,530 --> 00:16:53,540
The idea here is that the more
probable and more frequent

414
00:16:53,540 --> 00:16:56,300
a symbol is in your
stream, the least bits

415
00:16:56,300 --> 00:16:57,820
you should use to represent it.

416
00:16:57,820 --> 00:17:01,710
A perfect real world example of
Huffman encoding is Morse code.

417
00:17:01,710 --> 00:17:03,590
In the American
language, the letter E

418
00:17:03,590 --> 00:17:05,490
is the most dominant
of all our words.

419
00:17:05,490 --> 00:17:07,980
Therefore, it's
assigned a single beep

420
00:17:07,980 --> 00:17:09,880
to represent its value.

421
00:17:09,880 --> 00:17:11,818
Huffman compression
works in a similar way.

422
00:17:11,818 --> 00:17:14,109
Now, we're going to spare
you the knowledge of building

423
00:17:14,109 --> 00:17:15,567
a Huffman tree and
everything else.

424
00:17:15,567 --> 00:17:17,520
This is in tons of
data structures books.

425
00:17:17,520 --> 00:17:19,050
Instead, we'll just
show you that we

426
00:17:19,050 --> 00:17:23,230
can see after we parse our
newly tokenized tuple string,

427
00:17:23,230 --> 00:17:26,558
you can see that 0, 0 is
the most dominant tuple set.

428
00:17:26,558 --> 00:17:28,099
And of course, we
can assign that one

429
00:17:28,099 --> 00:17:30,210
bit, being a single zero.

430
00:17:30,210 --> 00:17:32,350
Therefore, the next
set is actually 1, 1.

431
00:17:32,350 --> 00:17:33,517
We have two symbols of that.

432
00:17:33,517 --> 00:17:35,474
And because of the way
our tree is constructed,

433
00:17:35,474 --> 00:17:37,890
we actually have to provide
that with two bit symbols.

434
00:17:37,890 --> 00:17:40,600
We continue on and continue
on and effectively keep

435
00:17:40,600 --> 00:17:44,240
assigning variable bit
code words to symbols,

436
00:17:44,240 --> 00:17:46,240
creating a compressed
version of our data.

437
00:17:46,240 --> 00:17:49,150
Now, these two characters
or these two algorithms

438
00:17:49,150 --> 00:17:51,300
operating back and
forth with each other

439
00:17:51,300 --> 00:17:53,770
actually have a very
beautiful ballet

440
00:17:53,770 --> 00:17:55,350
in the way that
content is created

441
00:17:55,350 --> 00:17:57,750
and how it's compressed with
the statistical encoding.

442
00:17:57,750 --> 00:17:59,390
This is the cornerstone
of everything

443
00:17:59,390 --> 00:18:02,660
we're going to talk about for
the rest of this conversation.

444
00:18:02,660 --> 00:18:04,630
Now, it's worth pointing
out that GZIP is not

445
00:18:04,630 --> 00:18:06,160
the only kid on the block.

446
00:18:06,160 --> 00:18:10,648
In fact, GZIP is about 20
something years old now.

447
00:18:10,648 --> 00:18:12,022
If it were a real
human being, it

448
00:18:12,022 --> 00:18:13,700
would be able to
drink and drive and do

449
00:18:13,700 --> 00:18:16,100
all other sorts of fun
stuff on the internet.

450
00:18:16,100 --> 00:18:18,190
But other compression
algorithms which

451
00:18:18,190 --> 00:18:20,690
may be a little bit newer
and a little bit different

452
00:18:20,690 --> 00:18:22,773
can actually give you some
interesting trade-offs.

453
00:18:22,773 --> 00:18:25,140
So here are four of the more
popular encoders out there.

454
00:18:25,140 --> 00:18:27,220
The first is one known as LZMA.

455
00:18:27,220 --> 00:18:28,930
Some of you may know
this more popularly

456
00:18:28,930 --> 00:18:32,000
as 7-Zip, a very
popular compression

457
00:18:32,000 --> 00:18:33,540
archive tool out there.

458
00:18:33,540 --> 00:18:35,860
You can see that LZMA
actually gives you

459
00:18:35,860 --> 00:18:38,040
smaller compression than GZIP.

460
00:18:38,040 --> 00:18:40,960
This is actually due to
a lot of higher order

461
00:18:40,960 --> 00:18:44,290
searching and finding algorithms
built around that LZ77

462
00:18:44,290 --> 00:18:45,230
algorithm.

463
00:18:45,230 --> 00:18:48,310
In fact, LZMA can actually be
considered a distant cousin

464
00:18:48,310 --> 00:18:50,950
to GZIP because it's actually
very similar in the way

465
00:18:50,950 --> 00:18:52,410
that it compresses its data.

466
00:18:52,410 --> 00:18:54,680
However, because it uses
more modern heuristics

467
00:18:54,680 --> 00:18:57,150
and algorithms, it can
actually get better results.

468
00:18:57,150 --> 00:18:59,960
Now, below LZMA, you see LPAQ.

469
00:18:59,960 --> 00:19:03,210
This is a context
mixing based encoder.

470
00:19:03,210 --> 00:19:05,630
It's effectively
mostly a neural net

471
00:19:05,630 --> 00:19:08,320
in practice of how
it matches symbols.

472
00:19:08,320 --> 00:19:09,900
Now notice LPAQ
actually gives us

473
00:19:09,900 --> 00:19:12,760
the smallest file size
compared to everything else

474
00:19:12,760 --> 00:19:15,940
at 0.35 megabytes.

475
00:19:15,940 --> 00:19:18,209
I'll get into that a little
bit more in a second.

476
00:19:18,209 --> 00:19:20,500
Now, the final compressor we
look at is actually BZIP2.

477
00:19:20,500 --> 00:19:25,160
Now, BZIP is a modern variant
of the Burrows-Wheeler transform

478
00:19:25,160 --> 00:19:27,810
assigned also with a
move to front transform

479
00:19:27,810 --> 00:19:30,270
and either a Huffman
or an arithmetic

480
00:19:30,270 --> 00:19:31,580
encoder on the back end.

481
00:19:31,580 --> 00:19:34,980
Now, BZIP2 fundamentally changes
the way that compression works.

482
00:19:34,980 --> 00:19:35,950
It's very different.

483
00:19:35,950 --> 00:19:37,530
Doesn't use LZ77 at all.

484
00:19:37,530 --> 00:19:40,430
Instead, it uses
a semi-block based

485
00:19:40,430 --> 00:19:42,640
sorting transform
to actually increase

486
00:19:42,640 --> 00:19:45,404
redundancy or adjacent
redundancy in data.

487
00:19:45,404 --> 00:19:47,570
This redundancy allows it
to get better compression.

488
00:19:47,570 --> 00:19:50,220
Again, you can see here
where GZIP actually gives us

489
00:19:50,220 --> 00:19:54,061
0.48 in terms of megabytes,
BZIP actually beats this.

490
00:19:54,061 --> 00:19:56,310
Now, what you're actually
seeing here in terms of data

491
00:19:56,310 --> 00:20:00,206
sizes is actually a scrape I
did of the amazon.com home page.

492
00:20:00,206 --> 00:20:01,580
So if I take all
of the text data

493
00:20:01,580 --> 00:20:05,000
from that-- HTML, CSS,
JSON, and JS data--

494
00:20:05,000 --> 00:20:07,315
it actually comes out
to about 1.64 megs.

495
00:20:07,315 --> 00:20:09,430
So you can see how
these compressors relate

496
00:20:09,430 --> 00:20:12,770
to that specific
memory footprint.

497
00:20:12,770 --> 00:20:14,680
Now, when you're
comparing compressors,

498
00:20:14,680 --> 00:20:17,030
the size of the data is
not the only heuristic.

499
00:20:17,030 --> 00:20:19,030
You have two other
heuristics that you typically

500
00:20:19,030 --> 00:20:20,030
throw into the mix.

501
00:20:20,030 --> 00:20:22,350
The first one is encoding
time and the second one

502
00:20:22,350 --> 00:20:25,740
is decoding time, which often is
the more important of the two.

503
00:20:25,740 --> 00:20:27,530
So if we look at the
encoding time column,

504
00:20:27,530 --> 00:20:31,100
you can see that GZIP does
probably the fastest in terms

505
00:20:31,100 --> 00:20:34,550
of encoding speed
at 0.79 seconds.

506
00:20:34,550 --> 00:20:38,460
LZMA, being a distant cousin
with more search heuristics,

507
00:20:38,460 --> 00:20:40,750
actually comes in
pretty close at 1.26.

508
00:20:40,750 --> 00:20:44,860
You can see that the wins
it gets in compression size

509
00:20:44,860 --> 00:20:47,810
actually come from
more preprocessing

510
00:20:47,810 --> 00:20:51,140
at the front of
the encoding step.

511
00:20:51,140 --> 00:20:53,320
Now, LPAQ gives you
amazing compression,

512
00:20:53,320 --> 00:20:56,620
but notice it actually takes 11
seconds to compress your data.

513
00:20:56,620 --> 00:20:59,070
Now again, LPAQ is
a modern wonder.

514
00:20:59,070 --> 00:21:01,460
It actually uses
various forms of context

515
00:21:01,460 --> 00:21:04,490
mixing in different
sort of combinations,

516
00:21:04,490 --> 00:21:07,240
again, resulting in pretty
much an artificial intelligence

517
00:21:07,240 --> 00:21:09,780
neural net algorithm to
find compression wins.

518
00:21:09,780 --> 00:21:11,410
So it makes sense
that this actually

519
00:21:11,410 --> 00:21:13,927
oscillates at about 11 seconds
worth of processing time

520
00:21:13,927 --> 00:21:16,260
because there's a lot of
things going on under the hood.

521
00:21:16,260 --> 00:21:19,620
Meanwhile, the Burrows-Wheeler
transform powered BZIP2

522
00:21:19,620 --> 00:21:21,750
doesn't stray too
far from the mark,

523
00:21:21,750 --> 00:21:24,235
but gives us about 2.1
seconds in encoding time.

524
00:21:24,235 --> 00:21:25,610
And there's a
whole set of papers

525
00:21:25,610 --> 00:21:28,630
that talk about how that
clustering transform actually

526
00:21:28,630 --> 00:21:30,560
affects memory and
processing and whatnot.

527
00:21:30,560 --> 00:21:33,280
Now, decoding time is probably
the most important part here.

528
00:21:33,280 --> 00:21:35,530
You can actually see
that GZIP and LZMA are,

529
00:21:35,530 --> 00:21:38,520
for all practical
purposes, identical.

530
00:21:38,520 --> 00:21:41,230
So while LZMA takes a
little bit longer to encode,

531
00:21:41,230 --> 00:21:44,410
it seems to regularly produce
smaller files than GZIP,

532
00:21:44,410 --> 00:21:47,340
and the decoding time tends
to be almost identical.

533
00:21:47,340 --> 00:21:49,540
Now, LPAQ, again, because
it's running a neural net,

534
00:21:49,540 --> 00:21:51,690
the decode time actually
equals almost what

535
00:21:51,690 --> 00:21:54,000
the encode time was, so you
look at about 11 seconds

536
00:21:54,000 --> 00:21:56,490
there, while BZIP
actually is a faster

537
00:21:56,490 --> 00:21:57,970
decode than it is an encode.

538
00:21:57,970 --> 00:21:59,500
The whole point of this
slide-- the only thing

539
00:21:59,500 --> 00:22:00,958
I want you to take
away from here--

540
00:22:00,958 --> 00:22:03,630
is that GZIP is really
good at a couple things,

541
00:22:03,630 --> 00:22:05,480
specifically encoding time.

542
00:22:05,480 --> 00:22:07,920
It gets beat in a couple
places, specifically

543
00:22:07,920 --> 00:22:12,070
with the size of the compression
that it achieves, and then ties

544
00:22:12,070 --> 00:22:14,020
in a couple other
places in decoding time.

545
00:22:14,020 --> 00:22:16,470
So it's not really the
only algorithm on the block

546
00:22:16,470 --> 00:22:19,620
there in terms of
compression ratios.

547
00:22:19,620 --> 00:22:21,370
So you can see here
that we've highlighted

548
00:22:21,370 --> 00:22:23,120
a couple of these
specific instances.

549
00:22:23,120 --> 00:22:24,820
LPAQ wins in size.

550
00:22:24,820 --> 00:22:26,930
Encoding time is, of
course, dominated by GZIP,

551
00:22:26,930 --> 00:22:30,710
and decoding time is, of
course, dominated by LZMA.

552
00:22:30,710 --> 00:22:34,980
Now, this can lead a lot of
people on the web tech platform

553
00:22:34,980 --> 00:22:37,010
to actually come at me
with burning pitchforks

554
00:22:37,010 --> 00:22:39,530
and actually say, hey, GZIP
is actually pretty good.

555
00:22:39,530 --> 00:22:41,180
That data just showed
us that we really

556
00:22:41,180 --> 00:22:44,030
don't need any other compression
algorithms because it gives us

557
00:22:44,030 --> 00:22:47,120
decent size compression,
decent time for encoding,

558
00:22:47,120 --> 00:22:49,207
and decent decoding time.

559
00:22:49,207 --> 00:22:51,290
Well, I'm here to tell you
that's a good argument,

560
00:22:51,290 --> 00:22:53,860
but let's dig a
little bit deeper into

561
00:22:53,860 --> 00:22:55,920
whether or not GZIP should
be the only thing we

562
00:22:55,920 --> 00:22:57,239
allow on the web.

563
00:22:57,239 --> 00:22:59,530
So first off, you should
understand that there's really

564
00:22:59,530 --> 00:23:02,320
no silver bullet
for compression.

565
00:23:02,320 --> 00:23:04,840
Depending on what your
data is, how frequent

566
00:23:04,840 --> 00:23:08,500
it is, the relationships it
has with local information,

567
00:23:08,500 --> 00:23:11,520
all relate to how well it
can be compressed at the end,

568
00:23:11,520 --> 00:23:13,500
and different compressors
will handle this data

569
00:23:13,500 --> 00:23:14,470
in different ways.

570
00:23:14,470 --> 00:23:17,830
For example, if you actually
applied JPEG style compression

571
00:23:17,830 --> 00:23:19,456
to text data, you're
not going to be

572
00:23:19,456 --> 00:23:21,330
able to decompress your
text in the right way

573
00:23:21,330 --> 00:23:24,380
because it's lossy in terms
of removing information

574
00:23:24,380 --> 00:23:26,156
bits from the stream itself.

575
00:23:26,156 --> 00:23:28,030
So let's take a perfect
example that actually

576
00:23:28,030 --> 00:23:31,230
shows to beat GZIP without
really any modification.

577
00:23:31,230 --> 00:23:33,480
So let's say we have a
string of integers here.

578
00:23:33,480 --> 00:23:35,470
Now, this string of
integers was actually

579
00:23:35,470 --> 00:23:40,880
created as an index system into
some higher order database.

580
00:23:40,880 --> 00:23:44,010
So there's basically 10
things that are being indexed,

581
00:23:44,010 --> 00:23:45,270
and we list them here.

582
00:23:45,270 --> 00:23:46,980
Now, if we just GZIP
this, we actually

583
00:23:46,980 --> 00:23:48,406
don't get a lot of savings.

584
00:23:48,406 --> 00:23:49,780
This is because,
if you remember,

585
00:23:49,780 --> 00:23:53,100
the LZ77 algorithm
isn't going to find

586
00:23:53,100 --> 00:23:55,920
any duplicate indexes
in this array.

587
00:23:55,920 --> 00:23:59,220
Nine is only existing once,
and so are the other numbers.

588
00:23:59,220 --> 00:24:03,670
Meanwhile, that means that all
of the statistical probability

589
00:24:03,670 --> 00:24:05,710
for these symbols is
pretty much equal.

590
00:24:05,710 --> 00:24:07,430
There's no difference.

591
00:24:07,430 --> 00:24:09,190
Nothing is more frequent
or less frequent,

592
00:24:09,190 --> 00:24:11,180
which means the Huffman
algorithm is going

593
00:24:11,180 --> 00:24:13,350
to pretty much assign
equal bit lengths to all

594
00:24:13,350 --> 00:24:14,560
the symbols themselves.

595
00:24:14,560 --> 00:24:17,490
Because of this, we really don't
get any compression savings

596
00:24:17,490 --> 00:24:20,430
at all from the GZIP algorithm
for this particular data

597
00:24:20,430 --> 00:24:21,880
stream.

598
00:24:21,880 --> 00:24:25,780
This means that you have
to look at your data

599
00:24:25,780 --> 00:24:27,790
and go, what is
GZIP doing to it?

600
00:24:27,790 --> 00:24:30,710
If we, however, take a
little bit different swing

601
00:24:30,710 --> 00:24:32,710
at the problem itself,
we can actually

602
00:24:32,710 --> 00:24:35,560
beat what GZIP is doing with a
little knowledge of our data.

603
00:24:35,560 --> 00:24:38,100
So let's say we take
the original array,

604
00:24:38,100 --> 00:24:40,014
and instead of just
leaving it by itself,

605
00:24:40,014 --> 00:24:42,180
we actually notice the
property that we can actually

606
00:24:42,180 --> 00:24:44,990
sort this information because
we don't need the order

607
00:24:44,990 --> 00:24:46,517
to come out on the backside.

608
00:24:46,517 --> 00:24:48,100
So if we sort this,
we actually end up

609
00:24:48,100 --> 00:24:51,560
with a pretty incrementing
run of numbers-- zero to nine

610
00:24:51,560 --> 00:24:53,955
without any gaps in the middle.

611
00:24:53,955 --> 00:24:56,080
We can take this string
and apply a technique known

612
00:24:56,080 --> 00:25:00,680
as delta encoding to actually
create a different symbol set.

613
00:25:00,680 --> 00:25:03,660
Now, delta encoding
works by taking a symbol

614
00:25:03,660 --> 00:25:05,860
and finding the
difference in the symbol

615
00:25:05,860 --> 00:25:08,922
between the previous symbol,
and it encodes the difference

616
00:25:08,922 --> 00:25:10,130
rather than the actual value.

617
00:25:10,130 --> 00:25:12,160
So for our specific
example here,

618
00:25:12,160 --> 00:25:14,700
we start with the number
zero, and the number one

619
00:25:14,700 --> 00:25:16,920
is one more, so we
add one to that.

620
00:25:16,920 --> 00:25:19,600
Two is greater than
one by a single value,

621
00:25:19,600 --> 00:25:21,290
so we had one, one,
one, one, one, one.

622
00:25:21,290 --> 00:25:22,790
And you can see
with delta encoding,

623
00:25:22,790 --> 00:25:26,040
all we have to do is encode
one and then eight zeros

624
00:25:26,040 --> 00:25:29,280
to actually represent the
original string we have.

625
00:25:29,280 --> 00:25:32,160
Now, the delta encoded
version, notice

626
00:25:32,160 --> 00:25:34,210
that it does have a lot
of duplicate symbols,

627
00:25:34,210 --> 00:25:36,400
and there is a
particular symbol which

628
00:25:36,400 --> 00:25:38,890
is more dominant than
the other symbols.

629
00:25:38,890 --> 00:25:43,970
Now, once we've encoded
our data into this form,

630
00:25:43,970 --> 00:25:45,890
GZIP actually has a field day.

631
00:25:45,890 --> 00:25:48,940
The LZ77 algorithm finds lots
of matches in its window,

632
00:25:48,940 --> 00:25:50,690
and the arithmetic
compressor can actually

633
00:25:50,690 --> 00:25:53,710
come through and assign
smaller bit codes

634
00:25:53,710 --> 00:25:55,677
to more frequent streams.

635
00:25:55,677 --> 00:25:57,760
What we're showing you
here is that you can't just

636
00:25:57,760 --> 00:25:59,790
throw arbitrary data
at GZIP and expect

637
00:25:59,790 --> 00:26:03,330
to get the most perfect,
amazing form of compression.

638
00:26:03,330 --> 00:26:05,120
Instead, a little
bit of preprocessing

639
00:26:05,120 --> 00:26:07,520
can actually change the
way your compression works,

640
00:26:07,520 --> 00:26:09,830
most of the time for the better.

641
00:26:09,830 --> 00:26:12,012
Now, let's look at
another perfect example

642
00:26:12,012 --> 00:26:13,970
of where GZIP can actually
cause some problems.

643
00:26:13,970 --> 00:26:17,030
So if you look at my website, I
did a little bit of an analysis

644
00:26:17,030 --> 00:26:21,250
a while ago on various forms of
CSS minimization technologies.

645
00:26:21,250 --> 00:26:23,070
There was a
fantastic set of code

646
00:26:23,070 --> 00:26:25,820
out there by someone who
was using genetic algorithms

647
00:26:25,820 --> 00:26:29,754
to figure out optimal
ways to minify CSS data.

648
00:26:29,754 --> 00:26:31,420
So let's take a look
at this table here.

649
00:26:31,420 --> 00:26:35,000
So if we take two CSS files from
StackOverflow and Bootstrap,

650
00:26:35,000 --> 00:26:37,090
and we actually look at
their minified forms--

651
00:26:37,090 --> 00:26:42,390
so this has already been run
through Closure or YUI or Clean

652
00:26:42,390 --> 00:26:47,420
CSS, one of these things-- we
actually get about 90 and 97k,

653
00:26:47,420 --> 00:26:48,570
respectively.

654
00:26:48,570 --> 00:26:51,510
Now, using the genetic
algorithm version

655
00:26:51,510 --> 00:26:54,880
of the minifier actually
produced about 3% savings

656
00:26:54,880 --> 00:26:56,420
in both of these
files, which meant

657
00:26:56,420 --> 00:26:59,780
that we can use a different
variant of minification to get

658
00:26:59,780 --> 00:27:02,390
wins over what we're
typically already doing.

659
00:27:02,390 --> 00:27:03,880
So this is a good idea.

660
00:27:03,880 --> 00:27:07,290
If we find a better algorithm
that minifies our data smaller,

661
00:27:07,290 --> 00:27:09,030
we should be embracing that.

662
00:27:09,030 --> 00:27:11,550
This is where GZIP actually
causes some trouble.

663
00:27:11,550 --> 00:27:15,540
When we actually run these
genetically algorithmically

664
00:27:15,540 --> 00:27:19,340
minimized data through
the GZIP compressor,

665
00:27:19,340 --> 00:27:21,280
you can see the results
are actually negative.

666
00:27:21,280 --> 00:27:23,770
That means it's actually
growing the data.

667
00:27:23,770 --> 00:27:25,540
So we have our minified.GZIP.

668
00:27:25,540 --> 00:27:29,590
So if we just zip the data
that comes from Clean CSS

669
00:27:29,590 --> 00:27:31,920
or Closure, we get
about 18 and 15k.

670
00:27:31,920 --> 00:27:34,060
However, when we
zip the data that's

671
00:27:34,060 --> 00:27:36,850
been generated from
our genetic algorithm,

672
00:27:36,850 --> 00:27:38,780
you can see that we're
actually increasing

673
00:27:38,780 --> 00:27:40,580
the size of the file.

674
00:27:40,580 --> 00:27:43,000
Basically, what you're
seeing is that the data is so

675
00:27:43,000 --> 00:27:45,480
minimized coming from
the genetic algorithm

676
00:27:45,480 --> 00:27:47,620
that GZIP is actually
inflating the data.

677
00:27:47,620 --> 00:27:49,860
It's making your
file larger than it

678
00:27:49,860 --> 00:27:53,320
should be because it's only
using the LZ77 and Huffman

679
00:27:53,320 --> 00:27:54,500
encoding steps.

680
00:27:54,500 --> 00:27:56,450
Now, this should
scare a lot of people,

681
00:27:56,450 --> 00:27:59,550
hopefully, into thinking
how often this is actually

682
00:27:59,550 --> 00:28:02,030
occurring on the internet
for various size of files.

683
00:28:02,030 --> 00:28:04,180
Now, I can tell you
it's not that frequent,

684
00:28:04,180 --> 00:28:06,638
but it is something to keep an
eye on because it definitely

685
00:28:06,638 --> 00:28:08,720
shows that GZIP is
not a silver bullet

686
00:28:08,720 --> 00:28:10,970
and that it can actually
do harmful things in very

687
00:28:10,970 --> 00:28:12,842
certain circumstances.

688
00:28:12,842 --> 00:28:14,300
Now of course, a
lot of people here

689
00:28:14,300 --> 00:28:16,640
would then say, well,
this means we shouldn't

690
00:28:16,640 --> 00:28:19,190
be using this genetic
algorithm to minimize our CSS.

691
00:28:19,190 --> 00:28:20,579
Obviously, that's a flop.

692
00:28:20,579 --> 00:28:22,620
And I still think that
that's the wrong argument,

693
00:28:22,620 --> 00:28:24,530
but let's move on.

694
00:28:24,530 --> 00:28:26,320
Now, let's talk
about PNG for minute.

695
00:28:26,320 --> 00:28:28,960
Now, PNG is a format
that you can actually

696
00:28:28,960 --> 00:28:30,570
export in a compressed form.

697
00:28:30,570 --> 00:28:33,800
Now internal to PNG, the
compression algorithm it uses

698
00:28:33,800 --> 00:28:38,240
is called deflate, pretty much
the backbone of what GZIP uses.

699
00:28:38,240 --> 00:28:41,809
It's LZ77 coupled with a
Huffman compressor step.

700
00:28:41,809 --> 00:28:44,350
Now, what you're looking at on
the screen here is two images.

701
00:28:44,350 --> 00:28:47,970
Effectively, I took a 90
pixel by 90 pixel sprite,

702
00:28:47,970 --> 00:28:52,140
and I tiled it vertically,
creating a 90 by 270 image.

703
00:28:52,140 --> 00:28:55,490
Now, I resized that
image just a little bit

704
00:28:55,490 --> 00:28:59,380
and added two columns of
pixels, so we now have a 92

705
00:28:59,380 --> 00:29:02,490
by 270 image instead
of a 90 by 270.

706
00:29:02,490 --> 00:29:05,924
So we've barely altered
the size of the data.

707
00:29:05,924 --> 00:29:07,340
However, you can
see at the bottom

708
00:29:07,340 --> 00:29:13,340
there that the sizes post-GZIP
are drastically different.

709
00:29:13,340 --> 00:29:16,190
The 90 by 270
image is about 20k,

710
00:29:16,190 --> 00:29:19,790
while the 92 pixel image is 41k.

711
00:29:19,790 --> 00:29:23,650
That's twice as large for
only two columns of pixels

712
00:29:23,650 --> 00:29:24,720
being added to the image.

713
00:29:24,720 --> 00:29:26,730
And again, the
compression algorithm

714
00:29:26,730 --> 00:29:29,660
that's being used under the
hood here is effectively GZIP.

715
00:29:29,660 --> 00:29:31,560
So let's take a look at
why this is occurring

716
00:29:31,560 --> 00:29:33,320
and what's going on.

717
00:29:33,320 --> 00:29:35,550
So let's say we've got a
bunch of pixel colors here.

718
00:29:35,550 --> 00:29:37,014
Well remember,
the LZ77 algorithm

719
00:29:37,014 --> 00:29:38,430
is going to come
through, and it's

720
00:29:38,430 --> 00:29:39,638
going to try to find matches.

721
00:29:39,638 --> 00:29:42,480
So we've got a particular
green pixel here

722
00:29:42,480 --> 00:29:44,660
and another green
pixel that's identical

723
00:29:44,660 --> 00:29:47,220
to this somewhere previous
in our data stream.

724
00:29:47,220 --> 00:29:52,390
Now, how this works is that
LZ77 won't scan infinitely

725
00:29:52,390 --> 00:29:53,820
to try to find matches.

726
00:29:53,820 --> 00:29:56,050
Instead, it actually
operates in a window

727
00:29:56,050 --> 00:29:59,390
of 32k worth of information.

728
00:29:59,390 --> 00:30:02,050
Now, that means if I
encounter a symbol that's

729
00:30:02,050 --> 00:30:05,230
outside of that window, then
I'm not going to find a match.

730
00:30:05,230 --> 00:30:08,190
It only matches things
inside of a 32k window.

731
00:30:08,190 --> 00:30:10,850
So if we look at
our 90 by 90 images

732
00:30:10,850 --> 00:30:13,690
that we created for
our tile sheet there,

733
00:30:13,690 --> 00:30:17,810
90 times 90 times
4 bytes per pixel

734
00:30:17,810 --> 00:30:21,160
is roughly about 32k in size.

735
00:30:21,160 --> 00:30:23,590
Now, if we actually
look at 32 times 1,024,

736
00:30:23,590 --> 00:30:25,770
the real definition
of 32k, it's,

737
00:30:25,770 --> 00:30:29,950
again, about 300 bytes
larger than our 90 by 90.

738
00:30:29,950 --> 00:30:33,140
However, when we added
those two columns of pixels,

739
00:30:33,140 --> 00:30:36,690
we actually changed the size
and the stride of our images.

740
00:30:36,690 --> 00:30:41,674
90 by 92 by 4 is
actually 33k, not 32k.

741
00:30:41,674 --> 00:30:43,090
What this means
is that when we're

742
00:30:43,090 --> 00:30:46,110
trying to find duplicate
pixels in this image,

743
00:30:46,110 --> 00:30:50,590
we're just far enough away that
we can't find the exact pixel

744
00:30:50,590 --> 00:30:51,732
that we had seen before.

745
00:30:51,732 --> 00:30:53,190
Let's take a look
at this visually.

746
00:30:53,190 --> 00:30:56,670
So again, we have our two
images, the 20k and the 41k,

747
00:30:56,670 --> 00:30:58,540
and we can actually
create a heat map

748
00:30:58,540 --> 00:31:00,120
for this image representation.

749
00:31:00,120 --> 00:31:03,780
Now, this heat map shows
us in dark blue pixels

750
00:31:03,780 --> 00:31:05,580
being the ones that
are highly compressed.

751
00:31:05,580 --> 00:31:07,450
This is where matches
have been found.

752
00:31:07,450 --> 00:31:10,600
They're very small, encoded
pixels at this point.

753
00:31:10,600 --> 00:31:14,550
Meanwhile, reds and yellows mean
that we didn't find a match,

754
00:31:14,550 --> 00:31:18,030
and to encode this particular
pixel took a lot more bits.

755
00:31:18,030 --> 00:31:21,542
So you can see on the left hand
side, where we're 90 by 270,

756
00:31:21,542 --> 00:31:23,000
we actually get a
lot more matches.

757
00:31:23,000 --> 00:31:25,780
The dominant form of
the image is dark blue.

758
00:31:25,780 --> 00:31:29,280
You can see once we tiled that
image in the lower two regions,

759
00:31:29,280 --> 00:31:33,640
that we find an exact pixel
matching 32k pixels away.

760
00:31:33,640 --> 00:31:35,850
However, when we increase
the size by two columns,

761
00:31:35,850 --> 00:31:40,090
we've actually bumped out the
window of pixels to look at.

762
00:31:40,090 --> 00:31:42,620
Therefore, we're not actually
finding exact matches.

763
00:31:42,620 --> 00:31:44,680
In fact, we have to
look for near matches,

764
00:31:44,680 --> 00:31:47,500
and that's why the second
image has a lot more misses

765
00:31:47,500 --> 00:31:51,230
in the LZ77 cache,
resulting in worse encoding.

766
00:31:51,230 --> 00:31:55,460
Now, this basically shows
us that with small changes

767
00:31:55,460 --> 00:31:57,900
to our image data, the
GZIP algorithm actually

768
00:31:57,900 --> 00:31:59,960
falls completely on its face.

769
00:31:59,960 --> 00:32:02,516
Again, it's far from
being a silver bullet.

770
00:32:02,516 --> 00:32:04,890
Now, the truth is that at this
time you should be saying,

771
00:32:04,890 --> 00:32:05,830
well hey, it'd be
great if we could

772
00:32:05,830 --> 00:32:07,163
use one of the newer algorithms.

773
00:32:07,163 --> 00:32:10,640
But in reality,
we're stuck with GZIP

774
00:32:10,640 --> 00:32:12,570
for some indefinite future.

775
00:32:12,570 --> 00:32:13,334
Here's why.

776
00:32:13,334 --> 00:32:15,250
If you actually go to
the Chromium source code

777
00:32:15,250 --> 00:32:17,710
and look at the bug system
that's provided there,

778
00:32:17,710 --> 00:32:22,160
you'll find an interesting
tale of how Chrome actually

779
00:32:22,160 --> 00:32:25,232
adopted the BZIP2
compression format.

780
00:32:25,232 --> 00:32:26,940
Effectively, Chrome
added support for it,

781
00:32:26,940 --> 00:32:28,780
and a lot of servers--
Apache and whatnot--

782
00:32:28,780 --> 00:32:31,030
added support to send
out BZIP2 content.

783
00:32:31,030 --> 00:32:34,660
So effectively, a server creates
a BZIP2 file instead of GZIP

784
00:32:34,660 --> 00:32:36,720
and sends it off to servers.

785
00:32:36,720 --> 00:32:38,862
However, what started
happening in the wild

786
00:32:38,862 --> 00:32:40,320
was a little bit
interesting to us,

787
00:32:40,320 --> 00:32:41,861
and the results made
us actually have

788
00:32:41,861 --> 00:32:45,460
to remove the support
for BZIP2 from Chrome.

789
00:32:45,460 --> 00:32:48,120
You see, what was happening
was a lot of these middle boxes

790
00:32:48,120 --> 00:32:50,370
that are out there in
the wild didn't ever

791
00:32:50,370 --> 00:32:53,660
expect that we would do
anything besides GZIP,

792
00:32:53,660 --> 00:32:58,370
and they actually had hard coded
paths in there to look and say,

793
00:32:58,370 --> 00:33:01,030
if this is not GZIP
header compressed,

794
00:33:01,030 --> 00:33:02,880
compress it with GZIP.

795
00:33:02,880 --> 00:33:05,460
So effectively, what occurred
was some servers, when

796
00:33:05,460 --> 00:33:09,120
they received the BZIP2
archive or the BZIP2 header,

797
00:33:09,120 --> 00:33:11,120
and these middle boxes
would look at it and say,

798
00:33:11,120 --> 00:33:12,500
hey, this isn't GZIP.

799
00:33:12,500 --> 00:33:14,840
Strip the header, try
to GZIP the information,

800
00:33:14,840 --> 00:33:15,860
and then send it out.

801
00:33:15,860 --> 00:33:19,620
This means Chrome would
actually receive a BZIP2 data

802
00:33:19,620 --> 00:33:21,640
package whose header
had been stripped

803
00:33:21,640 --> 00:33:23,270
and then it had been re-gzipped.

804
00:33:23,270 --> 00:33:25,940
So we would have no idea what
the actual format of the data

805
00:33:25,940 --> 00:33:27,240
is that we're receiving.

806
00:33:27,240 --> 00:33:29,115
It could be binary data,
or it could actually

807
00:33:29,115 --> 00:33:30,100
be valid information.

808
00:33:30,100 --> 00:33:32,760
The problem was this was
so systemic out there

809
00:33:32,760 --> 00:33:35,342
in the wild for various types
of firmware and middle boxes

810
00:33:35,342 --> 00:33:37,050
that it was actually
too difficult for us

811
00:33:37,050 --> 00:33:40,030
to fix on the fly, which
means it was a smarter idea

812
00:33:40,030 --> 00:33:41,879
to actually just remove
this from Chrome.

813
00:33:41,879 --> 00:33:43,670
So when you start
talking about the ability

814
00:33:43,670 --> 00:33:46,960
to add other compression
algorithms to the base browser,

815
00:33:46,960 --> 00:33:48,760
you're going to run
into the same problem.

816
00:33:48,760 --> 00:33:50,290
There's a lot of
systems out there

817
00:33:50,290 --> 00:33:52,640
who just don't
understand and haven't

818
00:33:52,640 --> 00:33:55,520
been updated to expand to
different sorts of compression

819
00:33:55,520 --> 00:33:57,740
technologies.

820
00:33:57,740 --> 00:34:00,320
So this gets us to an
interesting idea of well,

821
00:34:00,320 --> 00:34:04,060
if we're stuck with GZIP, but
size of text data is a problem,

822
00:34:04,060 --> 00:34:06,320
how do we actually create
smaller GZIP files?

823
00:34:06,320 --> 00:34:07,695
Well lucky for
you, I've actually

824
00:34:07,695 --> 00:34:10,179
spent the past three or four
months focusing explicitly

825
00:34:10,179 --> 00:34:11,720
on this problem,
and you can actually

826
00:34:11,720 --> 00:34:14,520
see a lot of the results on my
blog at mainroach.blogspot.com.

827
00:34:14,520 --> 00:34:18,340
But first, let's dive into
a couple of these ideas,

828
00:34:18,340 --> 00:34:19,870
and then you can
go there and read

829
00:34:19,870 --> 00:34:21,880
about all the awesome things.

830
00:34:21,880 --> 00:34:26,050
So first off, we can actually
generate better GZIP files.

831
00:34:26,050 --> 00:34:27,920
A lot of people don't
understand this,

832
00:34:27,920 --> 00:34:31,130
but when your server creates
your GZIP file, a lot of them

833
00:34:31,130 --> 00:34:33,880
are only tuned to compress
it at a factor of six.

834
00:34:33,880 --> 00:34:40,230

835
00:34:40,230 --> 00:34:42,230
The command line parameter
that you pass to GZIP

836
00:34:42,230 --> 00:34:45,340
allows you to pass in
between zero and nine,

837
00:34:45,340 --> 00:34:47,511
where most of the servers
are default set to six.

838
00:34:47,511 --> 00:34:49,219
This seems to be an
interesting trade-off

839
00:34:49,219 --> 00:34:52,730
between compression speed
and compression size.

840
00:34:52,730 --> 00:34:55,230
Now, this is mostly
because web developers

841
00:34:55,230 --> 00:34:57,950
tend to just upload raw
assets to the server,

842
00:34:57,950 --> 00:34:59,590
not wanting to deal
with compression.

843
00:34:59,590 --> 00:35:01,673
The server actually is
responsible for compressing

844
00:35:01,673 --> 00:35:04,192
the content and then
sending it off to requests.

845
00:35:04,192 --> 00:35:05,650
Now, what's
interesting about this,

846
00:35:05,650 --> 00:35:08,080
though, is that you
can modify your files

847
00:35:08,080 --> 00:35:10,790
and actually GZIP them offline.

848
00:35:10,790 --> 00:35:12,570
So effectively,
part of your build

849
00:35:12,570 --> 00:35:15,040
step would be to take
your text information,

850
00:35:15,040 --> 00:35:18,262
GZIP it with some better
compressor to produce a smaller

851
00:35:18,262 --> 00:35:20,095
GZIP file, and then
actually tell the server

852
00:35:20,095 --> 00:35:22,180
to just pass that data through.

853
00:35:22,180 --> 00:35:25,100
There's two applications out
there that could actually

854
00:35:25,100 --> 00:35:27,510
produce this type of
smaller GZIP file.

855
00:35:27,510 --> 00:35:28,870
The first one is 7-Zip.

856
00:35:28,870 --> 00:35:32,190
This is actually a
very nice command line

857
00:35:32,190 --> 00:35:35,240
tool which can produce BZIP2
archives as well as GZIP

858
00:35:35,240 --> 00:35:38,160
archives, but it does
so taking advantage

859
00:35:38,160 --> 00:35:41,500
of the more modern,
more powerful searching

860
00:35:41,500 --> 00:35:44,330
and dictionary based
compression algorithms

861
00:35:44,330 --> 00:35:45,560
that it's going to be using.

862
00:35:45,560 --> 00:35:49,060
So 7-Zip can actually produce
smaller versions of GZIP files

863
00:35:49,060 --> 00:35:51,510
than the standard command
line GZIP archiver

864
00:35:51,510 --> 00:35:54,200
that ships with most
firmware in servers.

865
00:35:54,200 --> 00:35:56,582
Another fantastic tool out
there is one called Zopfli.

866
00:35:56,582 --> 00:35:58,540
Now, this was actually
created by the engineers

867
00:35:58,540 --> 00:36:01,060
here at Google to solve
this same problem.

868
00:36:01,060 --> 00:36:04,670
Now, Zopfli uses a lot more
memory and a lot more advanced

869
00:36:04,670 --> 00:36:08,800
algorithms than even 7-Zip to
find better matches in smaller

870
00:36:08,800 --> 00:36:12,524
spaces to produce smaller
GZIP files as well.

871
00:36:12,524 --> 00:36:13,690
It's an open source project.

872
00:36:13,690 --> 00:36:14,690
You can go check it out.

873
00:36:14,690 --> 00:36:16,720
I highly recommend it
if you're interested.

874
00:36:16,720 --> 00:36:19,250
So the cool thing is we can
use either 7-Zip or Zopfli

875
00:36:19,250 --> 00:36:21,940
as part of your build process to
actually generate smaller text

876
00:36:21,940 --> 00:36:24,770
files and have them sent
along on behalf of the user.

877
00:36:24,770 --> 00:36:26,400
And again, these are
GZIPs, so they're

878
00:36:26,400 --> 00:36:28,400
going to be accepted by
the middle boxes as well

879
00:36:28,400 --> 00:36:29,450
as the browsers.

880
00:36:29,450 --> 00:36:32,310
Now, if you're wondering
how these preprocessing

881
00:36:32,310 --> 00:36:35,030
systems actually fare
against the standard GZIP,

882
00:36:35,030 --> 00:36:36,695
here's a great graph to look at.

883
00:36:36,695 --> 00:36:38,320
So you can actually
see the blue column

884
00:36:38,320 --> 00:36:41,480
across the bunch of files is
actually the standard GZIP

885
00:36:41,480 --> 00:36:42,290
algorithm.

886
00:36:42,290 --> 00:36:45,430
The red one is actually
Zopfli, and the green one

887
00:36:45,430 --> 00:36:46,870
is actually 7-Zip.

888
00:36:46,870 --> 00:36:50,740
So you can see
across these 42 files

889
00:36:50,740 --> 00:36:54,690
that on average, in fact,
most of the time, both Zopfli

890
00:36:54,690 --> 00:36:59,990
and 7-Zip regularly beat GZIP
by somewhere between 2% to 10%.

891
00:36:59,990 --> 00:37:02,220
And if you actually
increase your parameters

892
00:37:02,220 --> 00:37:05,166
and allow Zopfli and 7-Zip to
spend more time doing matching

893
00:37:05,166 --> 00:37:06,790
and doing compression,
you can actually

894
00:37:06,790 --> 00:37:09,770
get it into the 15%
ratio, which is fantastic.

895
00:37:09,770 --> 00:37:12,390
Now, the cost of this,
though, is enormous.

896
00:37:12,390 --> 00:37:15,660
A lot of these algorithms
took probably 20 minutes

897
00:37:15,660 --> 00:37:19,460
to run-- on the light
side-- to find 1% to 5%

898
00:37:19,460 --> 00:37:20,607
worth of compression wins.

899
00:37:20,607 --> 00:37:22,190
Basically, what we
found is that we're

900
00:37:22,190 --> 00:37:25,230
at a local minima
for compression.

901
00:37:25,230 --> 00:37:28,550
The more time we spend trying
to compress the content

902
00:37:28,550 --> 00:37:31,330
yields less and less
and less actual savings.

903
00:37:31,330 --> 00:37:32,970
You could spend
six hours of cloud

904
00:37:32,970 --> 00:37:36,720
compute time to get 2% savings
in your compression algorithm.

905
00:37:36,720 --> 00:37:38,990
So at this point, you
have to look and say,

906
00:37:38,990 --> 00:37:40,630
well, if we're stuck
with GZIP, yet it

907
00:37:40,630 --> 00:37:43,880
takes additional hours of
time to compute smaller GZIP

908
00:37:43,880 --> 00:37:46,660
files, what the heck is
this entire talk about?

909
00:37:46,660 --> 00:37:48,500
Aren't we just stuck in this?

910
00:37:48,500 --> 00:37:49,680
Fret not, my good friends.

911
00:37:49,680 --> 00:37:51,430
You're actually not
stuck, and it actually

912
00:37:51,430 --> 00:37:53,460
comes down to you
owning your data.

913
00:37:53,460 --> 00:37:56,480
You see, you can create
smaller GZIP files

914
00:37:56,480 --> 00:37:58,680
by actually preprocessing
your information

915
00:37:58,680 --> 00:38:00,150
before handing it off to GZIP.

916
00:38:00,150 --> 00:38:01,990
Much like the example
I gave earlier

917
00:38:01,990 --> 00:38:04,705
where we took a set of numbers
and then delta compressed them

918
00:38:04,705 --> 00:38:07,080
to create something that was
highly repetitive and highly

919
00:38:07,080 --> 00:38:09,400
compressible, you can
apply these techniques

920
00:38:09,400 --> 00:38:11,640
to a lot of other portions
of your code base.

921
00:38:11,640 --> 00:38:13,750
So let's dive into some
interesting algorithms

922
00:38:13,750 --> 00:38:17,920
you can use in your
projects today.

923
00:38:17,920 --> 00:38:20,400
The first one happens to be
close and near to my heart.

924
00:38:20,400 --> 00:38:22,330
Now, many applications
and many websites

925
00:38:22,330 --> 00:38:25,390
out there use JSON as
a format to transfer

926
00:38:25,390 --> 00:38:27,190
data between client and server.

927
00:38:27,190 --> 00:38:29,237
Social media
information is typically

928
00:38:29,237 --> 00:38:30,570
sent around in this information.

929
00:38:30,570 --> 00:38:33,695
It's a very nice, very
adopted file format

930
00:38:33,695 --> 00:38:36,070
for sending information around,
particularly because it's

931
00:38:36,070 --> 00:38:38,220
built off the
JavaScript standard.

932
00:38:38,220 --> 00:38:41,110
Now, when you're sending
this data around,

933
00:38:41,110 --> 00:38:43,820
though, most of the time,
users and developers

934
00:38:43,820 --> 00:38:48,060
don't think about how to
modify the data being sent such

935
00:38:48,060 --> 00:38:49,370
that it compresses better.

936
00:38:49,370 --> 00:38:51,747
So a user asks for
some search query.

937
00:38:51,747 --> 00:38:53,580
The server goes and
computes the information

938
00:38:53,580 --> 00:38:56,110
and then returns
the JSON blob back,

939
00:38:56,110 --> 00:38:58,887
normally optimizing for
return round trip time.

940
00:38:58,887 --> 00:39:00,720
It's trying to get the
data back to the user

941
00:39:00,720 --> 00:39:01,707
as fast as possible.

942
00:39:01,707 --> 00:39:03,790
But what they don't
understand is if they actually

943
00:39:03,790 --> 00:39:06,150
have that GZIP flag
turned on, GZIP

944
00:39:06,150 --> 00:39:08,440
is going to stop the
operation and zip the content

945
00:39:08,440 --> 00:39:10,420
before sending it down
to the client anyway.

946
00:39:10,420 --> 00:39:11,940
Now, if you're the
type of developer

947
00:39:11,940 --> 00:39:14,920
that's dealing in third world
countries or other types

948
00:39:14,920 --> 00:39:18,620
of connectivity that may
be sparse or intermittent,

949
00:39:18,620 --> 00:39:22,240
this is actually a huge problem
because your client device--

950
00:39:22,240 --> 00:39:25,040
usually a mobile device
on some 1G or 2G network

951
00:39:25,040 --> 00:39:27,900
that may or may not
stay consistent--

952
00:39:27,900 --> 00:39:29,332
is sending off
requests, and then

953
00:39:29,332 --> 00:39:30,790
what's hurting them
is that they're

954
00:39:30,790 --> 00:39:32,720
getting a larger
payload coming down.

955
00:39:32,720 --> 00:39:35,250
So basically, what we're
talking about now is,

956
00:39:35,250 --> 00:39:38,212
how can you process
your JSON blobs that

957
00:39:38,212 --> 00:39:40,420
are being returned to these
individuals in such a way

958
00:39:40,420 --> 00:39:41,924
that GZIP can
compress it further?

959
00:39:41,924 --> 00:39:43,340
And it all starts
with the ability

960
00:39:43,340 --> 00:39:48,079
to transpose your structured
data in your JSON file.

961
00:39:48,079 --> 00:39:49,620
So let's take a look
at this example.

962
00:39:49,620 --> 00:39:51,500
Now, I've scraped
a lot of websites

963
00:39:51,500 --> 00:39:52,930
and a lot of JSON
responses, and I

964
00:39:52,930 --> 00:39:55,520
see this pattern occur
on a lot of websites.

965
00:39:55,520 --> 00:39:57,280
Effectively, what
you're looking at

966
00:39:57,280 --> 00:40:03,470
is a list of dictionary
structures of name value pairs.

967
00:40:03,470 --> 00:40:06,960
So if someone requests a
search for a particular item

968
00:40:06,960 --> 00:40:09,160
on a shopping website,
it's very common

969
00:40:09,160 --> 00:40:13,540
to actually return to them a
dictionary item that actually

970
00:40:13,540 --> 00:40:16,430
contains what the product
name is, what the price is,

971
00:40:16,430 --> 00:40:18,750
what thumbnail image to use,
et cetera, and then just

972
00:40:18,750 --> 00:40:21,860
list these linearly
inside of an array.

973
00:40:21,860 --> 00:40:24,830
So what this creates, though,
as an interesting problem,

974
00:40:24,830 --> 00:40:27,450
is that similar based
data is actually

975
00:40:27,450 --> 00:40:28,977
strided away from each other.

976
00:40:28,977 --> 00:40:30,060
It's actually interleaved.

977
00:40:30,060 --> 00:40:31,977
So you may have a
name and then a price,

978
00:40:31,977 --> 00:40:34,560
which could be a floating point
value, and then a description,

979
00:40:34,560 --> 00:40:36,410
which may be a long
form block of text,

980
00:40:36,410 --> 00:40:39,870
and then a URL, which has its
own specific characteristics.

981
00:40:39,870 --> 00:40:42,000
What we're proposing
in transposing our data

982
00:40:42,000 --> 00:40:44,495
is actually turning
this name value pair

983
00:40:44,495 --> 00:40:48,660
and actually de-interleaving it,
and instead actually grouping

984
00:40:48,660 --> 00:40:51,460
similar values for
property names together.

985
00:40:51,460 --> 00:40:54,200
So example here is we have these
two dictionary objects that

986
00:40:54,200 --> 00:40:56,370
both have name and
position values,

987
00:40:56,370 --> 00:40:58,490
and instead, we can
transpose that such

988
00:40:58,490 --> 00:41:01,060
that we have an
array of name answers

989
00:41:01,060 --> 00:41:03,050
and an array of pos answers.

990
00:41:03,050 --> 00:41:05,360
Now, I know this
looks weird at JSON,

991
00:41:05,360 --> 00:41:07,109
so let's take a
graphical look at things.

992
00:41:07,109 --> 00:41:08,650
So let's say on the
top here, we have

993
00:41:08,650 --> 00:41:10,660
an array where we have
a list of objects,

994
00:41:10,660 --> 00:41:12,250
and each one of
the colored blocks

995
00:41:12,250 --> 00:41:14,000
represents a property
on that object.

996
00:41:14,000 --> 00:41:17,320
So we've got green is a name,
and blue may be a price point,

997
00:41:17,320 --> 00:41:19,424
and again, red
might be some URL.

998
00:41:19,424 --> 00:41:21,090
What we can do, then,
is we can actually

999
00:41:21,090 --> 00:41:24,410
transpose this and align all of
the green blocks, blue blocks,

1000
00:41:24,410 --> 00:41:29,610
and red blocks together,
allowing homogeneous data

1001
00:41:29,610 --> 00:41:33,070
to reside in an array with
other homogeneous data.

1002
00:41:33,070 --> 00:41:35,232
Now, I'm going to explain
why this is actually

1003
00:41:35,232 --> 00:41:36,940
an improvement in a
second, but let's see

1004
00:41:36,940 --> 00:41:40,462
if we can take a look at whether
or not this saves us any data.

1005
00:41:40,462 --> 00:41:41,920
So if we take a
bunch of JSON files

1006
00:41:41,920 --> 00:41:43,580
that are returned from
very common websites--

1007
00:41:43,580 --> 00:41:45,790
so I took some responses
from Amazon and Pinterest

1008
00:41:45,790 --> 00:41:48,206
and whatnot-- you can see that
the first column represents

1009
00:41:48,206 --> 00:41:50,130
the source size in
bytes, the second column

1010
00:41:50,130 --> 00:41:53,420
represents the source
data actually gzipped.

1011
00:41:53,420 --> 00:41:56,070
The third is actually
the size of the data

1012
00:41:56,070 --> 00:41:57,670
after it's been transposed.

1013
00:41:57,670 --> 00:42:00,126
You can see that there's
some variability in numbers

1014
00:42:00,126 --> 00:42:02,000
there because we're
actually removing symbols

1015
00:42:02,000 --> 00:42:03,460
from the string at this point.

1016
00:42:03,460 --> 00:42:05,400
And you can see the
final column is actually

1017
00:42:05,400 --> 00:42:07,550
the gzipped version of
the transposed data.

1018
00:42:07,550 --> 00:42:09,890
Now, we're actually in an
interesting situation here,

1019
00:42:09,890 --> 00:42:14,990
is that for some JSON files,
the transposed gzipped data

1020
00:42:14,990 --> 00:42:17,770
is actually smaller than the
source gzipped data, which

1021
00:42:17,770 --> 00:42:20,690
means we actually get a win
by applying this process.

1022
00:42:20,690 --> 00:42:22,760
Thankfully, we're not
falling into that area

1023
00:42:22,760 --> 00:42:26,150
where genetic algorithm
minimized CSS is actually

1024
00:42:26,150 --> 00:42:27,617
causing GZIP to
inflate the data.

1025
00:42:27,617 --> 00:42:28,700
We don't want to be there.

1026
00:42:28,700 --> 00:42:30,533
So the transpose operation
actually gives us

1027
00:42:30,533 --> 00:42:32,420
smaller files,
which is fantastic.

1028
00:42:32,420 --> 00:42:34,580
Now, the reason that
this actually works

1029
00:42:34,580 --> 00:42:37,150
has to do with the
32k window that LZ77

1030
00:42:37,150 --> 00:42:39,150
uses for its matching.

1031
00:42:39,150 --> 00:42:41,175
So if we have our
values here that

1032
00:42:41,175 --> 00:42:43,550
are all interleaved-- red,
green, blue, red, green, blue,

1033
00:42:43,550 --> 00:42:48,020
red, green, blue-- again, we
have to go farther to search

1034
00:42:48,020 --> 00:42:50,390
for a piece of content
that may be an exact match.

1035
00:42:50,390 --> 00:42:54,070
However, when we transpose
our data, we de-interleave it

1036
00:42:54,070 --> 00:42:56,124
and group similar
types of data together.

1037
00:42:56,124 --> 00:42:57,540
So let's say we're
actually trying

1038
00:42:57,540 --> 00:43:00,360
to find a match for one
of these green values.

1039
00:43:00,360 --> 00:43:03,860
In the top array, you can see
that an entire listing of data,

1040
00:43:03,860 --> 00:43:06,850
the green may not fit
inside the 32k window.

1041
00:43:06,850 --> 00:43:09,440
Meanwhile, once we transpose
it and group homogeneous data

1042
00:43:09,440 --> 00:43:11,750
together, the entirety
of the green data

1043
00:43:11,750 --> 00:43:13,370
actually fits in
a single window.

1044
00:43:13,370 --> 00:43:16,530
This is going to allow you
to find better matches, which

1045
00:43:16,530 --> 00:43:20,500
is going to result in
smaller compression values.

1046
00:43:20,500 --> 00:43:23,120
Now this actually comes to
another step here, which is OK,

1047
00:43:23,120 --> 00:43:25,180
well, if we can
transpose our data,

1048
00:43:25,180 --> 00:43:27,310
how else might we
modify our content

1049
00:43:27,310 --> 00:43:28,310
so that we can get wins?

1050
00:43:28,310 --> 00:43:30,500
Now, this is a really,
really cool algorithm

1051
00:43:30,500 --> 00:43:33,230
that I found digging
through the archives of IEEE

1052
00:43:33,230 --> 00:43:34,655
known as compression boosting.

1053
00:43:34,655 --> 00:43:36,570
Now, it operates on
the same parameters.

1054
00:43:36,570 --> 00:43:39,775
How do we preprocess things
for better compression?

1055
00:43:39,775 --> 00:43:41,900
So the first one we're
going to take a look at here

1056
00:43:41,900 --> 00:43:43,760
is actually something
called Dense Codes.

1057
00:43:43,760 --> 00:43:46,680
Now, this is some great
research out of some academics

1058
00:43:46,680 --> 00:43:48,600
in Argentina, and
effectively, it

1059
00:43:48,600 --> 00:43:50,860
allows us to take a text
based file and preprocess

1060
00:43:50,860 --> 00:43:52,380
it and hand it off to GZIP.

1061
00:43:52,380 --> 00:43:53,990
Now, the preprocessing
is actually

1062
00:43:53,990 --> 00:43:54,990
the important part here.

1063
00:43:54,990 --> 00:43:56,430
We're not actually
transposing it,

1064
00:43:56,430 --> 00:43:59,660
but instead, were using a
modified dictionary index

1065
00:43:59,660 --> 00:44:00,770
lookup scheme.

1066
00:44:00,770 --> 00:44:03,900
So let's say we parse
our text and our seen,

1067
00:44:03,900 --> 00:44:06,740
and we create a
dictionary index.

1068
00:44:06,740 --> 00:44:09,600
So once a word is seen, we
provide it to the dictionary,

1069
00:44:09,600 --> 00:44:11,290
and then every
reference to that word

1070
00:44:11,290 --> 00:44:13,930
is replaced with an
index value to the array.

1071
00:44:13,930 --> 00:44:16,750
So let's say we have an array
here of, "How much would could

1072
00:44:16,750 --> 00:44:20,170
a woodchuck chuck," and we
have 400 symbols before that.

1073
00:44:20,170 --> 00:44:24,140
So we see that "how" is at
location 400, 401 is "much,"

1074
00:44:24,140 --> 00:44:26,720
402 is "wood," 403 is "could."

1075
00:44:26,720 --> 00:44:28,890
So when we're creating
a stream, we're

1076
00:44:28,890 --> 00:44:33,750
going to get values that point
into these element arrays.

1077
00:44:33,750 --> 00:44:37,240
So the problem with this is that
if we only have 256 symbols,

1078
00:44:37,240 --> 00:44:39,400
we can only use eight
bits per pointer.

1079
00:44:39,400 --> 00:44:41,210
However, if we go
above 256, we have

1080
00:44:41,210 --> 00:44:44,180
to start using 16 bits per
pointer, which is actually

1081
00:44:44,180 --> 00:44:48,210
a problem because if the
symbols are weighted such

1082
00:44:48,210 --> 00:44:51,440
that the most probable
and most visible

1083
00:44:51,440 --> 00:44:53,899
symbols are actually closer to
the front of the dictionary,

1084
00:44:53,899 --> 00:44:56,023
you're actually going to
be wasting a lot of space.

1085
00:44:56,023 --> 00:44:58,670
You're going to have a lot of
symbols and a lot of indexes

1086
00:44:58,670 --> 00:45:01,130
where the first upper
eight bits are actually

1087
00:45:01,130 --> 00:45:03,599
going to be zeros for the
entire dominant side of it.

1088
00:45:03,599 --> 00:45:05,140
So you're actually
inflating the size

1089
00:45:05,140 --> 00:45:06,973
of your stream at this
point because there's

1090
00:45:06,973 --> 00:45:09,060
a lot of bits that
aren't being used.

1091
00:45:09,060 --> 00:45:12,100
Now, the way that
Dense Codes work is it

1092
00:45:12,100 --> 00:45:16,730
actually allows you to modify
the way the token is used

1093
00:45:16,730 --> 00:45:18,660
to create the ability
to actually do

1094
00:45:18,660 --> 00:45:21,940
variable length coding
in string, which means

1095
00:45:21,940 --> 00:45:23,540
that for the first
three numbers,

1096
00:45:23,540 --> 00:45:26,490
we actually use 16 bits
to represent the indexes,

1097
00:45:26,490 --> 00:45:28,950
but the second three numbers,
because their indexes

1098
00:45:28,950 --> 00:45:31,170
are lower than 256,
we can actually

1099
00:45:31,170 --> 00:45:33,250
use eight bits instead of 16.

1100
00:45:33,250 --> 00:45:35,819
This is actually going to
create a smaller stream for us

1101
00:45:35,819 --> 00:45:37,485
to actually compress
a little bit later.

1102
00:45:37,485 --> 00:45:40,280

1103
00:45:40,280 --> 00:45:41,810
Now, the wins from
this are pretty

1104
00:45:41,810 --> 00:45:43,664
interesting to look
at because there's

1105
00:45:43,664 --> 00:45:45,330
a couple things in
flight here, and I've

1106
00:45:45,330 --> 00:45:49,749
got an entire 10-page writeup
on my blog about this algorithm

1107
00:45:49,749 --> 00:45:52,290
and the interesting things that
go back and forth between it,

1108
00:45:52,290 --> 00:45:54,248
some caveats, and some
things you need to know,

1109
00:45:54,248 --> 00:45:56,492
but it all boils
down to this table.

1110
00:45:56,492 --> 00:45:57,950
Now basically, what
this table says

1111
00:45:57,950 --> 00:46:01,230
is that we've tried the
source data being gzipped,

1112
00:46:01,230 --> 00:46:03,720
and then we've
tried the data being

1113
00:46:03,720 --> 00:46:05,220
run through our
dense codes-- that's

1114
00:46:05,220 --> 00:46:09,380
the ETDC column-- and then the
gzipping of that information.

1115
00:46:09,380 --> 00:46:11,940
Now, next to it, I actually
compare the other compressors

1116
00:46:11,940 --> 00:46:16,170
that we've mentioned today--
Zopfli, 7-Zip, and BZIP2.

1117
00:46:16,170 --> 00:46:17,800
So what I'm trying
to see here is

1118
00:46:17,800 --> 00:46:21,310
once I do this preprocessing,
what compression algorithm is

1119
00:46:21,310 --> 00:46:23,740
actually going to give
us the best results?

1120
00:46:23,740 --> 00:46:26,950
You can see that hands
down, GZIP really

1121
00:46:26,950 --> 00:46:30,027
doesn't produce savings with
this preprocessing method

1122
00:46:30,027 --> 00:46:31,610
for the majority of
the data that I've

1123
00:46:31,610 --> 00:46:33,402
shown here-- JSON files, CSSs.

1124
00:46:33,402 --> 00:46:34,860
And I've actually
run this probably

1125
00:46:34,860 --> 00:46:37,480
against 25,000,
30,000 files, and you

1126
00:46:37,480 --> 00:46:40,550
see this similar pattern,
that just doing standard GZIP

1127
00:46:40,550 --> 00:46:42,970
against this information
doesn't really give you wins.

1128
00:46:42,970 --> 00:46:45,150
However, you see when
you start using Zopfli,

1129
00:46:45,150 --> 00:46:46,710
you start getting some wins.

1130
00:46:46,710 --> 00:46:50,370
The advanced pattern matching
and more memory usage

1131
00:46:50,370 --> 00:46:52,970
characteristics that it
has inside of an encoder

1132
00:46:52,970 --> 00:46:54,860
allows you to get better
matches with this,

1133
00:46:54,860 --> 00:46:56,200
producing smaller files.

1134
00:46:56,200 --> 00:46:58,120
The clear winner here is 7-Zip.

1135
00:46:58,120 --> 00:47:00,620
Something inside of the way
it's using its algorithms

1136
00:47:00,620 --> 00:47:04,710
consistently produces smaller
dense code compressed files

1137
00:47:04,710 --> 00:47:06,350
as opposed to the
source data, which

1138
00:47:06,350 --> 00:47:08,156
means this is interesting,
that if you have

1139
00:47:08,156 --> 00:47:09,530
data that's above
a certain size.

1140
00:47:09,530 --> 00:47:12,860
Let's say you're returning a 20k
blob of data for some reason.

1141
00:47:12,860 --> 00:47:15,800
Preprocessing that
text with dense codes

1142
00:47:15,800 --> 00:47:22,980
and then using 7-Zip as
your compressor of choice

1143
00:47:22,980 --> 00:47:25,859
can actually produce smaller
GZIP files consistently.

1144
00:47:25,859 --> 00:47:27,400
Now, the downside
of this, of course,

1145
00:47:27,400 --> 00:47:29,875
is that you have to reconstruct
your dense code transform

1146
00:47:29,875 --> 00:47:32,082
at the client, but that's
not necessarily a big deal

1147
00:47:32,082 --> 00:47:33,540
depending on what
type of trade-off

1148
00:47:33,540 --> 00:47:35,760
you're willing to make.

1149
00:47:35,760 --> 00:47:38,150
Now, these are all
preprocessing schemes.

1150
00:47:38,150 --> 00:47:40,950
There's another form of
processing your data that

1151
00:47:40,950 --> 00:47:42,870
can actually get wins
that these can't touch.

1152
00:47:42,870 --> 00:47:44,370
I mean, the types
of wins that we're

1153
00:47:44,370 --> 00:47:46,630
going to talk about
for Delta.js blow

1154
00:47:46,630 --> 00:47:50,445
these preprocessing schemes out
of the water, but be warned.

1155
00:47:50,445 --> 00:47:53,560
Thar be dragons here, mostly
in the form of madness.

1156
00:47:53,560 --> 00:47:56,320
So let's dive into what
I'm talking about here.

1157
00:47:56,320 --> 00:47:59,680
So in 2012, the
Gmail team actually

1158
00:47:59,680 --> 00:48:02,410
did a fantastic
presentation for the W3C,

1159
00:48:02,410 --> 00:48:03,910
and actually put
up some slides that

1160
00:48:03,910 --> 00:48:07,460
are publicly available to other
information, that was proposing

1161
00:48:07,460 --> 00:48:09,100
a solution to a common problem.

1162
00:48:09,100 --> 00:48:13,000
You see, Gmail users
see about 61 years

1163
00:48:13,000 --> 00:48:16,730
of loading bar inside of
Gmail about every day.

1164
00:48:16,730 --> 00:48:20,130
This is a lot of time that
users are sitting around,

1165
00:48:20,130 --> 00:48:22,140
waiting for JavaScript
to be streamed out.

1166
00:48:22,140 --> 00:48:25,030
What they proposed was
a new form of ability

1167
00:48:25,030 --> 00:48:28,540
to, instead of transferring the
large files every single time,

1168
00:48:28,540 --> 00:48:30,910
they can actually start
transferring the difference

1169
00:48:30,910 --> 00:48:32,940
between the file
that the user has

1170
00:48:32,940 --> 00:48:35,470
and the new file
that the user needs.

1171
00:48:35,470 --> 00:48:36,720
Now, this isn't a new concept.

1172
00:48:36,720 --> 00:48:39,160
We've seen these sorts
of patching-based systems

1173
00:48:39,160 --> 00:48:42,147
everywhere in computing
since the late 1970s,

1174
00:48:42,147 --> 00:48:43,730
but it's never been
able to be applied

1175
00:48:43,730 --> 00:48:47,497
to the web due to some of
the architecture involved.

1176
00:48:47,497 --> 00:48:48,830
How the algorithm works is this.

1177
00:48:48,830 --> 00:48:52,900
So let's say we have a
file, file.CSS version zero,

1178
00:48:52,900 --> 00:48:54,510
and we've made an update to it.

1179
00:48:54,510 --> 00:48:57,000
This happens quite a
bit in large projects.

1180
00:48:57,000 --> 00:48:59,660
Now, when we make that update,
the majority of the file

1181
00:48:59,660 --> 00:49:01,180
is the same as it used to be.

1182
00:49:01,180 --> 00:49:02,800
There may be a
function that's added,

1183
00:49:02,800 --> 00:49:05,960
or some comments were placed
in, or some things were removed,

1184
00:49:05,960 --> 00:49:07,480
but for the majority
of the file,

1185
00:49:07,480 --> 00:49:08,910
it's almost
identical, which means

1186
00:49:08,910 --> 00:49:12,000
we can represent the
second file, the new file,

1187
00:49:12,000 --> 00:49:15,410
as a difference from the
file that we've already seen.

1188
00:49:15,410 --> 00:49:18,410
This is the concept,
again, of delta encoding.

1189
00:49:18,410 --> 00:49:20,350
Now, once we've delta
encoded this file,

1190
00:49:20,350 --> 00:49:23,400
the patch-- basically the
difference between the two--

1191
00:49:23,400 --> 00:49:27,790
is generally much, much smaller
than the updated version

1192
00:49:27,790 --> 00:49:30,180
of the file, which
means we can represent

1193
00:49:30,180 --> 00:49:36,110
version one as a patch
operation of version zero.

1194
00:49:36,110 --> 00:49:38,070
This means that if
the user already

1195
00:49:38,070 --> 00:49:41,530
has version one on their
machine, all we have to do

1196
00:49:41,530 --> 00:49:44,040
is send them the
patch, and it allows

1197
00:49:44,040 --> 00:49:47,817
them to reconstruct and create
the new version of the data.

1198
00:49:47,817 --> 00:49:49,900
Now, this is actually a
pretty interesting concept

1199
00:49:49,900 --> 00:49:51,566
because this means
we don't have to send

1200
00:49:51,566 --> 00:49:53,770
full files to the
clients all the time.

1201
00:49:53,770 --> 00:49:57,470
Instead, we can produce and send
down highly, highly minimized

1202
00:49:57,470 --> 00:49:58,890
content to these guys.

1203
00:49:58,890 --> 00:50:01,650
Now of course, this comes
with a bit of overhead

1204
00:50:01,650 --> 00:50:03,790
in terms of communication.

1205
00:50:03,790 --> 00:50:05,660
You see, in order
to get this working,

1206
00:50:05,660 --> 00:50:07,410
we have to have a
communication process

1207
00:50:07,410 --> 00:50:09,219
between the client
and the server.

1208
00:50:09,219 --> 00:50:11,010
So let's say we have
our mobile device here

1209
00:50:11,010 --> 00:50:13,330
and the user is going
to load a website.

1210
00:50:13,330 --> 00:50:15,520
Well, the client
actually needs to notify

1211
00:50:15,520 --> 00:50:19,260
the server on what version
of the file it has cached.

1212
00:50:19,260 --> 00:50:21,070
The server can then
take this information,

1213
00:50:21,070 --> 00:50:22,700
look up in its
array, and figure out

1214
00:50:22,700 --> 00:50:26,310
what patch file it needs
to send to the client

1215
00:50:26,310 --> 00:50:27,882
in order to get them up to date.

1216
00:50:27,882 --> 00:50:30,340
Once it's figured this out, it
passes it off to the client,

1217
00:50:30,340 --> 00:50:32,400
and then the client is
going to use this patch

1218
00:50:32,400 --> 00:50:35,390
to construct the new version
of the file and cache that

1219
00:50:35,390 --> 00:50:36,930
and use that appropriately.

1220
00:50:36,930 --> 00:50:40,410
Effectively, this technique
is trading network requests

1221
00:50:40,410 --> 00:50:42,530
for smaller file sizes.

1222
00:50:42,530 --> 00:50:44,530
Sending a couple byte
request to determine

1223
00:50:44,530 --> 00:50:46,210
what version of
the file you have

1224
00:50:46,210 --> 00:50:48,750
is going to be night and day
difference than actually doing

1225
00:50:48,750 --> 00:50:51,750
the entire file being sent
to the user multiple times.

1226
00:50:51,750 --> 00:50:54,900
Now, if this sounds like
craziness to you, just hold on.

1227
00:50:54,900 --> 00:50:57,230
You have to realize
this is a common problem

1228
00:50:57,230 --> 00:51:00,250
for many large, very
industrial websites

1229
00:51:00,250 --> 00:51:02,600
that serve millions
of users a day.

1230
00:51:02,600 --> 00:51:04,720
You see, when the
Gmail people actually

1231
00:51:04,720 --> 00:51:06,680
looked at the type
of savings they

1232
00:51:06,680 --> 00:51:08,350
can get from this
type of algorithm,

1233
00:51:08,350 --> 00:51:11,700
they saw a massive potential
for improvement here.

1234
00:51:11,700 --> 00:51:14,780
You see, when they
compared the number of revs

1235
00:51:14,780 --> 00:51:18,190
they do to a single file
over a month against the size

1236
00:51:18,190 --> 00:51:20,370
of the deltas if they
were using this scheme,

1237
00:51:20,370 --> 00:51:23,050
they actually saw that a
whole month's worth of changes

1238
00:51:23,050 --> 00:51:28,120
was about 9%-- lower than
9%-- of the size of the assets

1239
00:51:28,120 --> 00:51:30,160
altogether, which
means they would only

1240
00:51:30,160 --> 00:51:34,300
have to send 9% of the content
as opposed to the new full file

1241
00:51:34,300 --> 00:51:35,190
each time.

1242
00:51:35,190 --> 00:51:36,260
This is a huge win.

1243
00:51:36,260 --> 00:51:37,970
We're not talking
about 10% improvement.

1244
00:51:37,970 --> 00:51:39,636
We're not talking
about 50% improvement.

1245
00:51:39,636 --> 00:51:44,780
This is 90% size decrease by
using this delta algorithm.

1246
00:51:44,780 --> 00:51:48,724
So obviously, this was a
proposal to the W3C spec.

1247
00:51:48,724 --> 00:51:50,390
I've talked with some
of the Gmail guys.

1248
00:51:50,390 --> 00:51:52,764
It doesn't look like this is
actually live in the servers

1249
00:51:52,764 --> 00:51:53,323
right now.

1250
00:51:53,323 --> 00:51:55,406
If that's changed, I hope
to be wrong because this

1251
00:51:55,406 --> 00:51:57,430
is a fantastic
piece of technology

1252
00:51:57,430 --> 00:51:59,750
that I hope to see
rolling out in some form

1253
00:51:59,750 --> 00:52:02,660
to a lot of other
distributors on the internet.

1254
00:52:02,660 --> 00:52:04,697
Now, there's another
form of this compression

1255
00:52:04,697 --> 00:52:06,530
that I've actually been
playing around with.

1256
00:52:06,530 --> 00:52:08,863
It actually comes in the form
of horizontal compression.

1257
00:52:08,863 --> 00:52:11,200
Typically, when we think of
delta encoding for files,

1258
00:52:11,200 --> 00:52:12,990
we think in terms of patches.

1259
00:52:12,990 --> 00:52:14,659
I've got version
A and version B,

1260
00:52:14,659 --> 00:52:16,700
and I want to generate
the patch between the two.

1261
00:52:16,700 --> 00:52:18,920
This is especially common
in game development.

1262
00:52:18,920 --> 00:52:20,962
However, there exists a
form called horizontal.

1263
00:52:20,962 --> 00:52:22,420
How this works is
let's say we have

1264
00:52:22,420 --> 00:52:25,480
a cascade of files that may
be similar on a website.

1265
00:52:25,480 --> 00:52:29,540
So this particular website
uses three CSS files.

1266
00:52:29,540 --> 00:52:31,740
Let's say it's Bootstrap
or something like that.

1267
00:52:31,740 --> 00:52:35,277
Well, the interesting thing is
that these CSS files generally

1268
00:52:35,277 --> 00:52:36,860
aren't that different
from each other.

1269
00:52:36,860 --> 00:52:39,620
There's actually a lot a shared
syntax on a website for a given

1270
00:52:39,620 --> 00:52:41,600
CSS file, which
means that instead

1271
00:52:41,600 --> 00:52:45,190
of doing delta compression
between versions of the file,

1272
00:52:45,190 --> 00:52:48,857
we can actually do delta
compression between the files

1273
00:52:48,857 --> 00:52:50,440
that are going to
be sent to the user.

1274
00:52:50,440 --> 00:52:53,010
So we can actually do a
difference between file zero

1275
00:52:53,010 --> 00:52:56,290
and file one, and then
file one and file two.

1276
00:52:56,290 --> 00:52:58,200
This actually allows
us to create patches

1277
00:52:58,200 --> 00:52:59,780
for each one of
these, and when we

1278
00:52:59,780 --> 00:53:02,560
combine that with the
source file to your server,

1279
00:53:02,560 --> 00:53:05,690
the size of the assets that need
to be sent down to the client

1280
00:53:05,690 --> 00:53:07,710
is drastically reduced.

1281
00:53:07,710 --> 00:53:10,110
We can actually send them
the entire application

1282
00:53:10,110 --> 00:53:13,540
and all the content required
as deltas from base files.

1283
00:53:13,540 --> 00:53:15,630
This, again, is just
an extrapolation

1284
00:53:15,630 --> 00:53:17,590
of various forms
of delta algorithms

1285
00:53:17,590 --> 00:53:19,790
that are already
used out in the wild.

1286
00:53:19,790 --> 00:53:22,030
Now, how this works
on the client side

1287
00:53:22,030 --> 00:53:24,150
is that the server
will, of course, provide

1288
00:53:24,150 --> 00:53:27,760
to the client the base file
and the set of patches,

1289
00:53:27,760 --> 00:53:30,350
and then the client is
responsible for reconstructing

1290
00:53:30,350 --> 00:53:33,160
each one of those files and
then caching them locally.

1291
00:53:33,160 --> 00:53:35,040
This is a really
neat idea if you're

1292
00:53:35,040 --> 00:53:38,470
trying to optimize first
time load for users.

1293
00:53:38,470 --> 00:53:40,950
The Gmail proposed
specification requires

1294
00:53:40,950 --> 00:53:44,530
that the user has to download
version zero of the file, which

1295
00:53:44,530 --> 00:53:48,870
in some cases actually
could be 300k of CSS data.

1296
00:53:48,870 --> 00:53:50,530
Meanwhile, horizontal
compression

1297
00:53:50,530 --> 00:53:52,570
suggests that it may
not have to do that.

1298
00:53:52,570 --> 00:53:57,700
It may only have to send down
bites or chunks of 50k data

1299
00:53:57,700 --> 00:54:00,520
when you actually represent
that CSS as a delta.

1300
00:54:00,520 --> 00:54:03,420
Effectively, what you're doing
in this horizontal scheme is

1301
00:54:03,420 --> 00:54:07,551
you're trading client side
processing for smaller transfer

1302
00:54:07,551 --> 00:54:08,050
sizes.

1303
00:54:08,050 --> 00:54:10,780
This is because it actually
takes a lot of CPU cycles

1304
00:54:10,780 --> 00:54:14,050
on the client to reconstruct
these files before passing them

1305
00:54:14,050 --> 00:54:15,930
off to the processing
system to create

1306
00:54:15,930 --> 00:54:17,340
your DOM and everything else.

1307
00:54:17,340 --> 00:54:20,785
So there's this interesting
trade-off between Delta.js

1308
00:54:20,785 --> 00:54:24,340
or vertical delta encoding for
files and the horizontal delta

1309
00:54:24,340 --> 00:54:26,830
encoding for files as well.

1310
00:54:26,830 --> 00:54:29,099
Now, we can see when I've
applied this technique

1311
00:54:29,099 --> 00:54:30,640
to some various
sites on the internet

1312
00:54:30,640 --> 00:54:32,170
that we actually get some
interesting numbers out of it.

1313
00:54:32,170 --> 00:54:34,830
So I took all of the CSS
from a Gmail session,

1314
00:54:34,830 --> 00:54:36,960
all of the JavaScript
from a Gmail session,

1315
00:54:36,960 --> 00:54:39,760
and all of the CSS
from an Amazon session,

1316
00:54:39,760 --> 00:54:41,260
and I ran it through
this technique.

1317
00:54:41,260 --> 00:54:43,440
So you can see we've
got the source, the GZIP

1318
00:54:43,440 --> 00:54:45,960
source, and then
the size of the data

1319
00:54:45,960 --> 00:54:47,920
once it's been delta
encoded, and then

1320
00:54:47,920 --> 00:54:50,720
of course, the gzipping
of the delta encoded data

1321
00:54:50,720 --> 00:54:52,840
because we can't just stop
at the delta encoding.

1322
00:54:52,840 --> 00:54:54,480
So you can see for
Gmail, we actually

1323
00:54:54,480 --> 00:54:58,730
get some pretty amazing
savings, 31% and 12%,

1324
00:54:58,730 --> 00:55:02,334
by using this horizontal
delta compression encoding.

1325
00:55:02,334 --> 00:55:04,750
Meanwhile, something weird's
going on with the Amazon data

1326
00:55:04,750 --> 00:55:08,800
where we actually get an
increase of size by 13%.

1327
00:55:08,800 --> 00:55:10,540
Now, I'd like to
actually point out

1328
00:55:10,540 --> 00:55:15,332
that this data is highly, highly
minimized and highly redundant.

1329
00:55:15,332 --> 00:55:17,540
All of these files that are
creating multiple network

1330
00:55:17,540 --> 00:55:19,300
requests tend to
be self similar,

1331
00:55:19,300 --> 00:55:22,160
and that's actually due to the
minification processes that

1332
00:55:22,160 --> 00:55:24,550
are being applied to
these files on the web.

1333
00:55:24,550 --> 00:55:27,390
To give you an example of
how important minimization

1334
00:55:27,390 --> 00:55:29,760
is to horizontal
delta compression,

1335
00:55:29,760 --> 00:55:32,437
let's take a look at the game
library known as Impact.js.

1336
00:55:32,437 --> 00:55:33,270
I love this library.

1337
00:55:33,270 --> 00:55:34,079
It's fantastic.

1338
00:55:34,079 --> 00:55:35,620
If you're a game
developer and you're

1339
00:55:35,620 --> 00:55:37,470
looking to make HTML5
games, definitely

1340
00:55:37,470 --> 00:55:39,140
give Impact.js a look.

1341
00:55:39,140 --> 00:55:40,990
Effectively, I took
the source file

1342
00:55:40,990 --> 00:55:45,520
and left them as loose files.

1343
00:55:45,520 --> 00:55:47,520
I did not combine them
into a single large file.

1344
00:55:47,520 --> 00:55:50,510
I actually created the delta
between all these files

1345
00:55:50,510 --> 00:55:54,100
and then did the
gzipped version of that.

1346
00:55:54,100 --> 00:55:55,530
Now, you can see
that it actually

1347
00:55:55,530 --> 00:55:57,780
goes down from about 70k to 21k.

1348
00:55:57,780 --> 00:56:00,040
However, when I minimized
all of the files

1349
00:56:00,040 --> 00:56:02,470
before doing delta
compression, I actually

1350
00:56:02,470 --> 00:56:03,360
got a better savings.

1351
00:56:03,360 --> 00:56:05,100
I got down to about 14k.

1352
00:56:05,100 --> 00:56:06,900
This is because the
minimization techniques

1353
00:56:06,900 --> 00:56:08,441
we're using on the
web today actually

1354
00:56:08,441 --> 00:56:10,290
produce a lot of
duplicate symbols

1355
00:56:10,290 --> 00:56:12,510
that could be used
in various places.

1356
00:56:12,510 --> 00:56:14,730
Again, when you see
those original examples

1357
00:56:14,730 --> 00:56:18,070
of taking some and we
renamed the parameters that

1358
00:56:18,070 --> 00:56:20,690
are being passed
to it as ABC, we

1359
00:56:20,690 --> 00:56:23,086
tend to see that pattern
occur for all the JavaScript

1360
00:56:23,086 --> 00:56:24,710
for every function
that's been defined,

1361
00:56:24,710 --> 00:56:26,520
meaning we're going
to see more matches.

1362
00:56:26,520 --> 00:56:29,680
More matches result in better
statistical probability,

1363
00:56:29,680 --> 00:56:32,720
which results in
smaller file sizes.

1364
00:56:32,720 --> 00:56:35,540
So by minimizing our data before
we do our delta compression,

1365
00:56:35,540 --> 00:56:39,650
we actually get very,
very important wins.

1366
00:56:39,650 --> 00:56:42,290
So what we've talked
about today is

1367
00:56:42,290 --> 00:56:44,340
where GZIP sits on
the web platform.

1368
00:56:44,340 --> 00:56:46,770
We've got a lot of text data
that's about to blow up.

1369
00:56:46,770 --> 00:56:48,060
We've got mobile devices.

1370
00:56:48,060 --> 00:56:49,930
We've got fragmented hardware.

1371
00:56:49,930 --> 00:56:52,300
We've got different
connectivity all over the world,

1372
00:56:52,300 --> 00:56:56,040
but some things here allow you
to take advantage and actually

1373
00:56:56,040 --> 00:56:58,840
address these issues
today rather than waiting

1374
00:56:58,840 --> 00:57:00,090
for network times to increase.

1375
00:57:00,090 --> 00:57:01,631
So for example, we
looked at the fact

1376
00:57:01,631 --> 00:57:03,050
that GZIP is not
a silver bullet.

1377
00:57:03,050 --> 00:57:04,650
By preprocessing your
data, you can actually

1378
00:57:04,650 --> 00:57:05,790
get some pretty big wins.

1379
00:57:05,790 --> 00:57:08,027
We've seen where GZIP
actually falls on its face.

1380
00:57:08,027 --> 00:57:09,610
If you've done a lot
of preprocessing,

1381
00:57:09,610 --> 00:57:11,526
it may not compress your
data, and in fact, it

1382
00:57:11,526 --> 00:57:12,420
might inflate it.

1383
00:57:12,420 --> 00:57:16,120
Or in the case of PNGs,
modifying the window matching

1384
00:57:16,120 --> 00:57:17,840
by slight parameters
can actually

1385
00:57:17,840 --> 00:57:21,270
upset how well GZIP
attaches to your data.

1386
00:57:21,270 --> 00:57:23,630
We also looked at how to
preprocess your content using

1387
00:57:23,630 --> 00:57:27,930
various forms of other command
line algorithms, like Zopfli

1388
00:57:27,930 --> 00:57:30,880
or 7-Zip, and then how to
transpose your JSON data, which

1389
00:57:30,880 --> 00:57:33,190
is fantastic for a lot
of your shopping sites.

1390
00:57:33,190 --> 00:57:35,980
Also, when you combine that
with the dense code boosting,

1391
00:57:35,980 --> 00:57:39,440
which is more cutting edge
text preprocessing data,

1392
00:57:39,440 --> 00:57:41,970
you start getting a sense
that the web is not done yet.

1393
00:57:41,970 --> 00:57:44,252
You're not locked into
your form that you have.

1394
00:57:44,252 --> 00:57:46,710
Then you can start looking at
the delta compression methods

1395
00:57:46,710 --> 00:57:48,751
and start looking at the
ways to actually combine

1396
00:57:48,751 --> 00:57:53,410
your data in different
ways to reduce duplication

1397
00:57:53,410 --> 00:57:55,529
and complexity in the
content that you have.

1398
00:57:55,529 --> 00:57:57,070
And when you apply
all this together,

1399
00:57:57,070 --> 00:57:59,500
you start getting
a vision of the web

1400
00:57:59,500 --> 00:58:01,770
that we can actually
control more of our data

1401
00:58:01,770 --> 00:58:05,130
to reduce the sizes and actually
get around a lot of the hitches

1402
00:58:05,130 --> 00:58:07,380
and problems that
GZIP presents to us.

1403
00:58:07,380 --> 00:58:09,830
So with that, thank you
for your time today.

1404
00:58:09,830 --> 00:58:11,690
I really appreciate
you listening

1405
00:58:11,690 --> 00:58:14,050
to this ramble on some very
hard compression stuff.

1406
00:58:14,050 --> 00:58:15,930
If you're interested
in more, I highly

1407
00:58:15,930 --> 00:58:17,930
recommend you check
out html4rocks.com.

1408
00:58:17,930 --> 00:58:20,920
I recently put up
two articles there

1409
00:58:20,920 --> 00:58:22,800
on text compression
for web developers

1410
00:58:22,800 --> 00:58:25,100
as well as image compression
for web developers,

1411
00:58:25,100 --> 00:58:27,740
and these are generally
meant to be opener tutorials

1412
00:58:27,740 --> 00:58:30,200
to introduce you to different
terminology and different

1413
00:58:30,200 --> 00:58:31,800
algorithms and how
they're being used.

1414
00:58:31,800 --> 00:58:34,227
Once again, definitely check
out the #perfmatters hashtag.

1415
00:58:34,227 --> 00:58:35,560
A lot of smart people are there.

1416
00:58:35,560 --> 00:58:38,902
And join the Google+ Web
Performance community.

1417
00:58:38,902 --> 00:58:41,110
You can actually see the
short link there on the side

1418
00:58:41,110 --> 00:58:43,380
as goo.gl/webperf.

1419
00:58:43,380 --> 00:58:45,750
Again, a great place to talk
about performance problems

1420
00:58:45,750 --> 00:58:46,830
and find issues.

1421
00:58:46,830 --> 00:58:47,800
That's it for me.

1422
00:58:47,800 --> 00:58:48,962
My name is Colt McAnlis.

1423
00:58:48,962 --> 00:58:50,670
Here's how you get a
hold of me for email

1424
00:58:50,670 --> 00:58:52,380
and other various
social media times.

1425
00:58:52,380 --> 00:58:54,910
Thanks, once again, for tuning
in to this episode of Google

1426
00:58:54,910 --> 00:58:56,000
Developers Live.

1427
00:58:56,000 --> 00:58:57,410
I hope to see you again soon.

1428
00:58:57,410 --> 00:58:58,960
Thanks.

1429
00:58:58,960 --> 00:59:05,941

